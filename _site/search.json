[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Digital Humanities Projects",
    "section": "",
    "text": "debkeepr\n\n\nAnalysis of historical non-decimal currencies and value systems that use either tripartite or tetrapartite systems such as that of pounds, shillings, and pence.\n\n\n\n\n\n\n\n\n\n\n\nThe Estate of Jan della Faille de Oude, 1582–1617\n\n\nA Digital Humanities project on the account books of the estate of an early modern merchant using R\n\n\n\n\n\n\n\n\n\n\n\nThe Correspondence Network of Daniel van der Meulen, 1578–1591\n\n\nA digital humanities project of an early modern merchant’s correspondence network using GIS techniques with R\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/df-inheritance.html#footnotes",
    "href": "project/df-inheritance.html#footnotes",
    "title": "The Estate of Jan della Faille de Oude, 1582–1617",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor a complete discussion of Jan de Oude’s background and trade see Wilfrid Brulez, De Firma Della Faille en de internationale handel van Vlaamse firma’s in de 16e eeuw (Brussels: Paleis der Academièen, 1959).↩︎"
  },
  {
    "objectID": "project/dvdm-correspondence.html",
    "href": "project/dvdm-correspondence.html",
    "title": "The Correspondence Network of Daniel van der Meulen, 1578–1591",
    "section": "",
    "text": "Visualizations\n\nShiny version of the leaflet map that includes real-time filtering of dates\n\n\n\nBackground to Daniel van der Meulen\nLong-distance trade in the early modern period could not be conducted without correspondence networks and the exchange of letters. Merchants continued to travel in order to manage their affairs, but elite merchants rarely if ever travelled with their own goods. Instead of escorting goods from place to place, merchants organized and supervised the purchase, transportation, and sale of goods through correspondence with factors spread throughout the trade routes of Europe and beyond. This more sedentary system of exchange enabled the rapid growth of trade on an individual and European-wide basis, but it also necessitated that merchants create and maintain wide networks of competent and trustworthy correspondents willing and able to follow the directions of merchants who might be many hundreds of miles away.\nDaniel van der Meulen (1554–1600), the fifth child and youngest son of Jan van der Meulen and Elizabeth Zeghers, was born in Antwerp in 1554. The Van der Meulens participated in regional trade from Antwerp, mainly with the fairs in Frankfurt and Strassbourg. The prospects of the Van der Meulens improved over the course of Daniel’s childhood, especially after Daniel’s mother took over management of the family’s trading activities following the death of her husband around 1563. In the 1580s, Daniel and his older brother Andries both married into the mercantile elite of Antwerp, enabling the Van der Meulens to invest in long-distance trade. Closely associated with the rebel side of the Dutch Revolt, the Van der Meulens left Antwerp in 1585 after the city fell to Spanish forces. Daniel first spent six years in Bremen before relocating to the university city of Leiden in September 1591. By the time of Daniel’s death from the plague in 1600, his trading activities had expanded well beyond the European continent to involve the western coast of Africa and exploratory ventures to the Indies that predated the founding of the Dutch East Indies Company.\nThe large collection of approximately 6,600 letters that Daniel van der Meulen received between 1578 and 1600 — housed in the Daniel van der Meulen Archive at the municipal archive in Leiden — provides a rare opportunity to investigate the development of a mercantile correspondence network at an early stage in the maturation of the Atlantic economy. The collection can be divided into two periods: before and after Daniel’s move to Leiden in 1591. From 1578 — the date of the first letter addressed to Daniel extant in the archive — until the end of 1591 Daniel received 442 letters. After moving to Leiden, Daniel became more independent from his siblings and developed a much more robust and active correspondence network, receiving well over 6,000 letters in a period of just under nine years. The current state of the project incorporates the letters sent to Daniel up to the end of 1591, but data entry on the remaining letters is underway.\n\n\nMethodology\nThis project uses the R programming language to analyze and visualize Daniel’s correspondence network. The above map of the letters from 1578–1591 provides an example. In addition to static maps such as the one above, the project will also include interactive maps that enable the viewer to investigate the data on their own. See the links to some of the visualizations above. Further analysis and visualizations will be described on the blog. In addition, work is underway on building a website dedicated to the analysis of Daniel’s correspondence network.\nThe contents of this project are under active development. You can check out the GitHub page to see the progress. Feedback on the project is welcome. I can be reached via email, Twitter, or through an issue on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Rogue Historian",
    "section": "",
    "text": "debkeepr: An R package for the analysis of non-decimal currencies\n\n\ndebkeepr is now on CRAN\n\n\n\nrstats\n\n\ndebkeepr\n\n\n\ndebkeepr is an R package for the analysis of historical non-decimal currencies and double-entry bookkeeping. Version 0.1.1 is debkeepr’s first release on CRAN.\n\n\n\n\n\n23 March 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading Twitter data with rtweet\n\n\nGet your tweets, those you follow, followers, and Twitter lists\n\n\n\nrstats\n\n\ntwitter\n\n\n\nA short breakdown of how to quickly download Twitter data using the rtweet package developed by rOpenSci.\n\n\n\n\n\n18 November 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMoving to Quarto\n\n\nTrying to start to blog again\n\n\n\nquarto\n\n\n\nRevitalizing the blog and moving from Hugo to Quarto\n\n\n\n\n\n5 November 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing debkeepr\n\n\nAn R package for the analysis of non-decimal currencies\n\n\n\nrstats\n\n\ndebkeepr\n\n\n\nIntroduction to debkeepr, a package for analyzing historical non-decimal currencies such as pounds, shillings, and pence.\n\n\n\n\n\n18 September 2018\n\n\n\n\n\n\n\n\n\n\n\n\nOne Year Anniversary\n\n\nSome reflections\n\n\nA reflection on the first year writing this blog on Digital Humanities and R\n\n\n\n\n\n6 June 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGreat Circles with R\n\n\nThree methods with sp and sf\n\n\n\nrstats\n\n\nrspatial\n\n\n\nCreating great circles in R with the sp and sf packages\n\n\n\n\n\n28 March 2018\n\n\n\n\n\n\n\n\n\n\n\n\nAn Exploration of Simple Features for R\n\n\nBuilding sfg, sfc, and sf objects from the sf package\n\n\n\nrstats\n\n\nrspatial\n\n\n\nExploration of the implementation of simple features standard by the sf package for R\n\n\n\n\n\n12 March 2018\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to GIS with R\n\n\nSpatial data with the sp and sf packages\n\n\n\nrstats\n\n\nrspatial\n\n\n\nIntroduction to GIS with R through the sp and sf packages\n\n\n\n\n\n7 February 2018\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Network Analysis with R\n\n\nCreating static and interactive network graphs\n\n\n\nrstats\n\n\nnetwork-analysis\n\n\n\nAn introduction to network analysis with R for Digital Humanities using the network, igraph, tidygraph, and ggraph packages\n\n\n\n\n\n25 October 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGeocoding with R\n\n\nUsing ggmap to geocode and map historical data\n\n\n\nrstats\n\n\nrspatial\n\n\n\nTutorial on geocoding and mapping historical data with ggmap\n\n\n\n\n\n13 October 2017\n\n\n\n\n\n\n\n\n\n\n\n\nExcel vs R: A Brief Introduction to R\n\n\nWith examples using dplyr and ggplot2\n\n\n\nrstats\n\n\ndigitalhumanities\n\n\ntidyverse\n\n\n\nWhy use R, and specifically the tidyverse, in the place of Excel for data analysis and visualization\n\n\n\n\n\n2 October 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMy Approach to Digital Humanities\n\n\n\n\n\n\ndigitalhumanities\n\n\n\nMy approach to learning the tools of Digital Humanities while in the midst of this process in 2017\n\n\n\n\n\n18 August 2017\n\n\n\n\n\n\n\n\n\n\n\n\nNew kinds of Projects: DH 2.0 and Coding\n\n\n\n\n\n\ndigitalhumanities\n\n\n\nMoving from organizing research to creating Digital Humanities Projects, or why I decided to learn R\n\n\n\n\n\n16 August 2017\n\n\n\n\n\n\n\n\n\n\n\n\nThinking about Workflow: DH 1.0\n\n\n\n\n\n\ndigitalhumanities\n\n\n\nWhy I started to think about how I do research, or how I came to Digital Humanities\n\n\n\n\n\n22 June 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBy Way of Introduction\n\n\n\n\n\nAn introduction to the website on history and digital humanities\n\n\n\n\n\n6 June 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/excel-vs-r/index.html",
    "href": "post/excel-vs-r/index.html",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "",
    "text": "Quantitative research often begins with the humble process of counting. Historical documents are never as plentiful as a historian would wish, but counting words, material objects, court cases, etc. can lead to a better understanding of the sources and the subject under study. When beginning the process of counting, the first instinct is to open a spreadsheet. The end result might be the production of tables and charts created in the very same spreadsheet document. In this post, I want to show why this spreadsheet-centric workflow is problematic and recommend the use of a programming language such as R as an alternative for both analyzing and visualizing data. There is no doubt that the learning curve for R is much steeper than producing one or two charts in a spreadsheet. However, there are real long-term advantages to learning a dedicated data analysis tool like R. Such advice to learn a programming language can seem both daunting and vague, especially if you do not really understand what it means to code. For this reason, after discussing why it is preferable to analyze data with R instead of a spreadsheet program, this post provides a brief introduction to R, as well as an example of analysis and visualization of historical data with R.1\nThe draw of the spreadsheet is strong. As I first thought about ways to keep track of and analyze the thousands of letters in the Daniel van der Meulen Archive, I automatically opened up Numbers — the spreadsheet software I use most often — and started to think about what columns I would need to create to document information about the letters. Whether one uses Excel, Numbers, Google Sheets or any other spreadsheet program, the basic structure and capabilities are well known. They all provide more-or-less aesthetically pleasing ways to easily enter data, view subsets of the data, and rearrange the rows based on the values of the various columns. But, of course, spreadsheet programs are more powerful than this, because you can add in your own programatic logic into cells to combine them in seemingly endless ways and produce graphs and charts from the results. The spreadsheet, after all, was the first killer app.\nWith great power, there must also come great responsibility. Or, in the case of the spreadsheet, with great power there must also come great danger. The danger of the spreadsheet derives from its very structure. The mixture of data entry, analysis, and visualization makes it easy to confuse cells that contain raw data from those that are the product of analysis. The nature of defining programatic logic — such as which cells are to be added together — by mouse clicks means that a mistaken click or drag action can lead to errors or the overwriting of data. You only need to think about the dread of the moment when you go to close a spreadsheet and the program asks whether you would like to save changes. It makes you wonder. Do I want to save? What changes did I make? Because the logic in a spreadsheet is all done through mouse clicks, there is no way to effectively track what changes have been made either in one session or in the production of a chart. Excel mistakes can have wide-ranging consequences, as the controversy around the paper of Carmen Reinhart and Kenneth Rogoff on national debt made clear.2\nThere are certainly legitimate reasons why people default to using spreadsheets for data analysis instead of using a programming language like R. Spreadsheets are much more inviting and comforting than any programming language could ever be to a newcomer. Learning how to program is intimidating and not something that can be done either quickly or easily. Graphical user interface (GUI) applications are simply less daunting than a command-line interface. Secondly, spreadsheets are a good tool for data entry, and it is tempting to simply move on to data analysis, keeping everything in the same document. Finally, the interactive nature of spreadsheets and the ability to create charts that change based on inputs is very attractive, even if fully unlocking this potential involves quite complex knowledge about how the program works. The first advantage of spreadsheets over programming is not easily overcome, but the latter two are built on what I believe to be a problematic workflow. Instead of using a couple of monolithic applications — often a office suite of applications — to do everything, I think that it is better to split up the workflow among several applications that do one thing well.\nCreating a clear division between data entry and analysis is a major reason why analyzing data in a programming language is preferable to spreadsheet software. I still use spreadsheets, but their use is strictly limited to data entry.3 In a spreadsheet program the analysis directly manipulates the only copy of the raw data. In contrast, with R you import the data, creating an object that is a copy of the raw data.4 All manipulations to the data are done on this copy, and the original data are never altered in any way. This means that there is no way to mess up the raw data. Manipulating a copy of the data enables you to more freely experiment. All mistakes are basically inconsequential, even if they can be frustrating. A line of code that fails to produce the expected result can be tweaked and rerun — with the process repeated many times if necessary — until the expected result is returned.\nWorking on a copy of the raw data can even simplify the process of data entry. Analyzing tabular data in R results in the creation of multiple objects, which are referred to as data frames and can be thought of as equivalent to tables in a spreadsheet.5 The ability to split, subset, and transform the original data set into many different data frames has the benefit of drastically reducing the complexity of data entry. Instead of needing bespoke spreadsheets with multiple interrelated sheets and tables, every piece of data only needs to be entered once and all manipulations can be done in code. The different data frames that are created in the process of analysis do not even have to be saved, because they are so easily reproduced by the script of code.\nThe separation of data entry and data analysis severely reduces the potential for mistakes, but maybe even more significantly, the use of code for data analysis enables the creation of reproducible research that is simply not possible in spreadsheets. Reproducible research has been a hot button issue in the sciences, but there is no reason why research in the humanities should not strive to be reproducible where possible. With a programming language, the steps of the analysis can be clearly laid out in the code. The “truth” of the analysis is the code, not the objects or visuals that the code creates. Saving analysis in code has the immediate benefit that it can be easily rerun anytime that new data is added. Code can also be applied to a completely new data set in a much more transparent manner than with spreadsheets. The long-term benefit is that with code all analysis is documented instead of being hidden behind mouse clicks. This makes it easier for you to go over your own analyses long after you have finished with it, as well as for others to understand what you did and check for errors."
  },
  {
    "objectID": "post/excel-vs-r/index.html#a-brief-introduction-to-r",
    "href": "post/excel-vs-r/index.html#a-brief-introduction-to-r",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "A Brief Introduction to R",
    "text": "A Brief Introduction to R\nIf you have never read or written any code before, it is difficult to know what the theoretical differences between spreadsheets and coding mean in practice.6 Therefore, I want to provide an example of analyzing and visualizing data in R. I will use the example of letters sent to Daniel van der Meulen in 1585, which is a subset of the data from my project on the correspondence of Daniel van der Meulen. Before getting to the analysis, I will give a very brief introduction to R and define some key terms in order to make it more possible for anyone who has never used R or coded before to follow along. This is not meant to be a full-scale tutorial, so much as an introduction to what code in R looks like.7\nR is an open-source programming language, which can be freely downloaded. Opening the downloaded program will lead you to a command-line interface devoid of any clues for what a novice should do next. Thankfully, there is a freely available application, or IDE, called RStudio, which provides a number of features to help write code. It is very much recommended to write your R code in RStudio. RStudio also has a plethora of online tutorials on their website to help you get familiar with both R and the RStudio IDE.\nIn either R or RStudio, the command line or console is where all of the action takes place. It is here that you enter and run the code. The foundation of working in R is the process of creating and manipulating objects of various sorts. Objects are assigned names with the assignment operator &lt;-. Thus, if I want to assign the name x the value of 5, I write x &lt;- 5 into the console and press return.\n\nx &lt;- 5\n\nNothing is printed when this operation is run, but now, if I type x into the console and hit return, the value of x will be printed. Alternatively, I could have explicitly called for the object x to be printed with the command print(x). The result is the same with either method.8\n\nx\n#&gt; [1] 5\n\nThe power of R comes with the manipulation of objects through the use of functions. Functions take in objects, possibly with a list of arguments to specify how the function works, and return another object. The objects and instructions for the functions are included within parentheses following the function name, and all instructions are separated by a comma.9 You can think of objects as nouns and functions as verbs. A simple example with the sum() function demonstrates how functions work. sum() does exactly what you would think. It takes a series of objects and returns their sum. You can see the function’s documentation by entering ?sum() on the console.10 The result of the sum() function can either be immediately printed to the console, or it can be saved by assigning it a name. Below, I assign the result of x + 10 a name and then in a second command print out the value of the newly created object.\n\ny &lt;- sum(x, 10)\ny\n#&gt; [1] 15\n\nThe flexibility of coding enables objects to be redefined endlessly. If a mistake is made or the data changes, the command can be rerun with the new values. For instance, if the value of x changes from 5 to 15 in the data, I can simply change the code for creating the x object and reassign the value of x. Below, I do this and then rerun the same sum() command, but this time I print the result directly to the console instead of assigning it to the object y.\n\nx &lt;- 15\nsum(x, 10)\n#&gt; [1] 25\n\nOnce you get a command that produces the expected result, you can save the command to an R script. A script is a text file with a suffix of .R, which you can save. Thus, you could save the commands to create the objects x and y to a file called my_script.R. The script will then be available to you whenever you want to rerun the code in the console."
  },
  {
    "objectID": "post/excel-vs-r/index.html#r-packages-the-tidyverse",
    "href": "post/excel-vs-r/index.html#r-packages-the-tidyverse",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "R Packages: The tidyverse",
    "text": "R Packages: The tidyverse\nUpon download, R comes with a large number of functions, which together are referred to as base R. However, the capabilities of R can be greatly extended through the use of additional packages, which can be downloaded through The Comprehensive R Archive Network (CRAN). Packages both extend what is possible in R and provide alternative ways to do things possible in base R. This can be confusing, because it means there are often a plethora of ways to do a single operation, but the extension in capability is well worth it. Particularly significant are the packages bundled together in the tidyverse package that were built by Hadley Wickham and collaborators. The tidyverse provides a set of linked packages that all use a similar grammar to work with data. The tidyverse is the best place to start if you are new to R.\nThe examples below show the ability to analyze and visualize data using the tidyverse packages. The analysis will mainly be done with the dplyr package and the visualization is done with ggplot. The dplyr functions all have a similar structure. The main dplyr verbs or functions are filter(), select(), arrange(), mutate(), group_by(), and summarise(). All take a data frame as their first argument. The next set of arguments are the column names on which the function performs its actions. The result of the functions is a new data frame object. The dplyr functions can be linked together, so that it is possible to perform multiple manipulations on a data frame in one command. This is done with the use of the pipe, which in R is %&gt;%.11\nThe commands needed to produce a ggplot graph can be confusing at first, but the advantages of ggplot are that it is based on a grammar of graphics, which provides a systematic way to discuss graphs, and that ggplot comes with good defaults, enabling the creation of nice looking graphs with minimal code. The two main components of ggplot are geoms and the mapping of aesthetics. Geoms tell ggplot what kind of visual graphic to make, so there are separate geom functions for bar charts, points, lines, etc. Aesthetics tell ggplot which columns of the data to use for placement on the graph and for any other distinguishing aspects of these variables such as size, shape, and color. The graphs below are used to show some of the possibilities of ggplot while staying mainly within the defaults. Publication ready visualizations are possible with ggplot, but this would take more fiddling.12"
  },
  {
    "objectID": "post/excel-vs-r/index.html#an-example-analyzing-a-correspondence-network",
    "href": "post/excel-vs-r/index.html#an-example-analyzing-a-correspondence-network",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "An Example: Analyzing a correspondence network",
    "text": "An Example: Analyzing a correspondence network\nWith this basic overview of R out of the way, let’s move on to an example of an actual analysis of historical data to see what analysis and visualization in R looks like in practice. The data and an R script with all of the code can be found on my GitHub page, if you would like to run the code yourself.\nBefore doing anything with the data, it is necessary to set up our environment in R. This means loading the packages that we will be using with the library() command. To begin we only need to load the tidyverse package, which includes the individual packages that I will use for the analysis.\n\nlibrary(tidyverse)\n\nOnce this is done, it is possible to use the functions from the tidyverse packages, beginning with reading our data into R with the read_csv() function. The code below takes a csv file from a location on my computer and loads it into R as a data frame, while saving the object under the name letters.13 The code can be read as “create a data frame object called letters from the csv file named correspondence-data-1585.csv.”\n\nletters &lt;- read_csv(\"data/correspondence-data-1585.csv\")\n\nEven before anything has been done to manipulate the data, we are already in a better position than if we kept the data in Excel. Having loaded the data, all further manipulations will be done on the object letters, and no changes will be made to the correspondence-data-1585.csv file. In other words, there is no way to tamper with the database that may have taken hours (or more) to meticulously produce.\nLet’s start by taking a look at the data itself by printing out a subset of the data to the console, which can be done by typing the name of the object into the console.14\n\nletters\n#&gt; # A tibble: 114 × 4\n#&gt;    writer                  source  destination date      \n#&gt;    &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;       &lt;date&gt;    \n#&gt;  1 Meulen, Andries van der Antwerp Delft       1585-01-03\n#&gt;  2 Meulen, Andries van der Antwerp Haarlem     1585-01-09\n#&gt;  3 Meulen, Andries van der Antwerp Haarlem     1585-01-11\n#&gt;  4 Meulen, Andries van der Antwerp Delft       1585-01-12\n#&gt;  5 Meulen, Andries van der Antwerp Haarlem     1585-01-12\n#&gt;  6 Meulen, Andries van der Antwerp Delft       1585-01-17\n#&gt;  7 Meulen, Andries van der Antwerp Delft       1585-01-22\n#&gt;  8 Meulen, Andries van der Antwerp Delft       1585-01-23\n#&gt;  9 Della Faille, Marten    Antwerp Haarlem     1585-01-24\n#&gt; 10 Meulen, Andries van der Antwerp Delft       1585-01-28\n#&gt; # … with 104 more rows\n\nThis tells us that letters is a tibble, which is a special kind of data frame meant to work well in the tidyverse. Though not necessarily aesthetically pleasing, the basic shape of a table of data is clear.15 The command printed out the first ten rows, while informing us that the complete data set contains 114 rows. Each letter has four pieces of information or variables: the writer of the letter, the place from which it was sent, the place to which it was sent, and the date sent. Below the column headings of writer, source, destination, and date we are informed of the type of data for each variable. This shows that the first three columns consist of character strings (chr), meaning that the data is words, while the last column contains dates, which is discussed in greater detail below."
  },
  {
    "objectID": "post/excel-vs-r/index.html#creating-new-data-frames",
    "href": "post/excel-vs-r/index.html#creating-new-data-frames",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Creating new data frames",
    "text": "Creating new data frames\nNow that we have an idea of the basic structure of the data, it is possible to begin to analyze it. A simple question that is fairly difficult to answer in Excel — at least I do not know how to do it other than by hand — is how many different people wrote letters to Daniel during this period. The below gives a list of the writers of letters. The code is a good demonstration of basic structure of dplyr functions. The function distinct() takes a data frame, followed by the name of the column that I am interested in, returning a new data frame object with one column, which consists of the unique values from the original column.\n\ndistinct(letters, writer)\n#&gt; # A tibble: 12 × 1\n#&gt;    writer                        \n#&gt;    &lt;chr&gt;                         \n#&gt;  1 Meulen, Andries van der       \n#&gt;  2 Della Faille, Marten          \n#&gt;  3 Della Faille, Jacques         \n#&gt;  4 Staten van Brabant            \n#&gt;  5 Della Faille, Joris           \n#&gt;  6 Eeckeren, Robert van          \n#&gt;  7 Anraet, Thomas                \n#&gt;  8 Burgemeesters of Antwerp      \n#&gt;  9 Wale, Jan de                  \n#&gt; 10 Calvart, Jacques              \n#&gt; 11 Janssen van der Meulen, Peeter\n#&gt; 12 Noirot, Jacques\n\nBecause I did not assign the output a name, the results simply printed to the console. Saving the object requires the use of the assignment operator and choosing a memorable name. With this relatively simple operation I have created my first new data frame, which I can now refer to and further manipulate by calling to writers.\n\nwriters &lt;- distinct(letters, writer)\n\nPrinting out the object shows that there are 12 people who sent Daniel letters in 1585, but another way to get this information is to run the nrow() function, which returns the number of rows in a data frame. It is possible to run the function on either the writers data frame, or to use the command that created the writers data frame. Let’s do the latter and print the result to the console. If you happen to forget the number of correspondents, the command can be typed again.\n\nnrow(distinct(letters, writer))\n#&gt; [1] 12\n\nOnce we run the commands and see that they produce results that are what we would expect from the data, it is possible to both save the commands in a script and also to reuse their structure for other pieces of information. For example, we can learn the locations from which Daniel’s correspondents sent letters and the locations in which he received letters by reusing the distinct() function and changing the column name to be manipulated.\n\ndistinct(letters, source)\n#&gt; # A tibble: 9 × 1\n#&gt;   source   \n#&gt;   &lt;chr&gt;    \n#&gt; 1 Antwerp  \n#&gt; 2 Haarlem  \n#&gt; 3 Dordrecht\n#&gt; 4 Venice   \n#&gt; 5 Lisse    \n#&gt; 6 Het Vlie \n#&gt; 7 Hamburg  \n#&gt; 8 Emden    \n#&gt; 9 Amsterdam\ndistinct(letters, destination)\n#&gt; # A tibble: 5 × 1\n#&gt;   destination\n#&gt;   &lt;chr&gt;      \n#&gt; 1 Delft      \n#&gt; 2 Haarlem    \n#&gt; 3 The Hague  \n#&gt; 4 Middelburg \n#&gt; 5 Bremen\n\nWe can save the objects for later use by using the assignment operator and giving the data frames names.\n\nsources &lt;- distinct(letters, source)\ndestinations &lt;- distinct(letters, destination)"
  },
  {
    "objectID": "post/excel-vs-r/index.html#linking-dplyr-commands-together-with-the-pipe",
    "href": "post/excel-vs-r/index.html#linking-dplyr-commands-together-with-the-pipe",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Linking dplyr commands together with the pipe",
    "text": "Linking dplyr commands together with the pipe\nThus far, I have created data frames with one variable that show the unique values of the different variables, but this has not told us anything about the number of letters sent by each author or from each location. Doing this is more complicated, because it is necessary to chain together a series of functions using the pipe. This is the case for the question of how many letters each correspondent sent. Looking at the dplyr verbs listed above you might assume that the summarise() function will create a sum of the data. We can use the summarise() function to create a new column called count that is filled with the number of observations through the n() function. In other words, the code below tells R to summarize the letters data frame and place the number of observations in a column called count.\n\nper_correspondent &lt;- summarise(letters, count = n())\nper_correspondent\n#&gt; # A tibble: 1 × 1\n#&gt;   count\n#&gt;   &lt;int&gt;\n#&gt; 1   114\n\nRunning the command and printing the result shows a result that was not what we hoped. Instead of showing letters per correspondent, the function created a column called count with a single value equal to the amount of rows in the letters data frame. However, because the objects that are made in R are ephemeral, we can simply rerun the code after reworking it. This overwrites the old per_correspondent object with one that is more useful. In the first attempt there is no notion that the goal is to group the number of letters by writer. This is the task group_by() function. To create an object with correspondents in one column and the number of letters they sent in the second column we need to group the letters data frame by writer and then summarize the letters within each group, creating the count column while doing so. The last line of the code arranges the table in descending order of letters sent by the newly created count variable. Notice that the letters data frame is listed first before any function. This is because the letters data frame is piped into each of the functions with the use of the %&gt;% command, which can be read as “and then.”\n\nper_correspondent &lt;- letters %&gt;% \n  group_by(writer) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count))\n\nper_correspondent\n#&gt; # A tibble: 12 × 2\n#&gt;    writer                         count\n#&gt;    &lt;chr&gt;                          &lt;int&gt;\n#&gt;  1 Meulen, Andries van der           63\n#&gt;  2 Della Faille, Jacques             31\n#&gt;  3 Della Faille, Marten               6\n#&gt;  4 Staten van Brabant                 4\n#&gt;  5 Noirot, Jacques                    2\n#&gt;  6 Wale, Jan de                       2\n#&gt;  7 Anraet, Thomas                     1\n#&gt;  8 Burgemeesters of Antwerp           1\n#&gt;  9 Calvart, Jacques                   1\n#&gt; 10 Della Faille, Joris                1\n#&gt; 11 Eeckeren, Robert van               1\n#&gt; 12 Janssen van der Meulen, Peeter     1\n\nLooking at the result, we can see that the above changes produced the kind output we expected. The group_by() and summarise() functions worked to create a data frame in which each author is listed once. The count = n() within the summarise() function created a new variable called count that is filled with the number of letters each correspondent sent. A cursory look at the results shows that the vast majority of the letters sent to Daniel were written by his brother Andries and his brother-in-law Jacques della Faille. Andries lived in the besieged city of Antwerp for most of 1585, and Jacques lived in Haarlem, so it will hardly be surprising that if we look at the amount of letters sent from each location that Antwerp and Haarlem dominate. Having written the above code, it is possible to rework it to create a data frame called per_source, which is done by replacing the writer column in the group_by() function with the source variable.\n\nper_source &lt;- letters %&gt;% \n  group_by(source) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count))\n\nper_source\n#&gt; # A tibble: 9 × 2\n#&gt;   source    count\n#&gt;   &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 Antwerp      76\n#&gt; 2 Haarlem      30\n#&gt; 3 Venice        2\n#&gt; 4 Amsterdam     1\n#&gt; 5 Dordrecht     1\n#&gt; 6 Emden         1\n#&gt; 7 Hamburg       1\n#&gt; 8 Het Vlie      1\n#&gt; 9 Lisse         1"
  },
  {
    "objectID": "post/excel-vs-r/index.html#visualizing-the-data-with-ggplot2",
    "href": "post/excel-vs-r/index.html#visualizing-the-data-with-ggplot2",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Visualizing the data with ggplot2\n",
    "text": "Visualizing the data with ggplot2\n\nWhile it is nice to have the two above sets of information presented in tables, it is also possible to visualize the newly created data with ggplot. The structure of ggplot functions is a bit different than that of dplyr. Like dplyr the functions are linked together, but the linking is done with the + symbol instead of %&gt;%. With ggplot2 the three main functions are ggplot(), one or more geom_* function, which informs the type of graphical element to draw, and one or more aes() function that sets the aesthetic values of the graph. The designation of the data to use and the aes() functions can be placed within either the ggplot() function, indicating that they apply to all of the geoms, or they can be placed within individual geom_* functions in which case they will only be used for that geom. All this is to say that there are a variety of ways to produce the same visuals. The below code adds an extra function to label the axes with labs() and uses fct_reorder() from the forcats package to convert the source cities into a factor so that the bars are organized by height.\n\nggplot(data = per_source) +\n  geom_bar(aes(x = fct_reorder(source, count), y = count), stat = \"identity\") +\n  labs(x = NULL, y = \"Letters written\")\n\n\n\n\n\n\n\nOne part of the above code that might be a bit difficult to figure out is stat = \"identity\". This needs to be called, because the code to create the per_source data frame actually did more work than necessary. stat = \"identity\" tells geom_bar() to set the height of the bars to the exact number in the count column. However, with geom_bar() only the x-axis needs to be specified. The y value is then calculated automatically based on the x value. Therefore, the below code, which uses the original letters data frame, could produce the exact same graph. Because of this, it is little trouble to change the variable for the x-axis to writers and get a completely new graph. Notice the change in the data and the deletion of the y variable and stat. The data for the x-axis is again converted to a factor, but this time with fct_infreq() to automatically create and order the factor by the frequency of observations. One problem with the correspondents data is the length of the names. Therefore, this command makes changes to the theme() function. The arguments in the function are used to place the correspondent names at a ninety degree angle so that they do not overlap.\n\nggplot(data = letters) +\n  geom_bar(aes(x = fct_infreq(writer))) +\n  labs(x = NULL, y = \"Letters written\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))"
  },
  {
    "objectID": "post/excel-vs-r/index.html#working-with-dates-lubridate",
    "href": "post/excel-vs-r/index.html#working-with-dates-lubridate",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Working with dates: lubridate\n",
    "text": "Working with dates: lubridate\n\nNow, let’s investigate the dates column. Dealing with dates is often tricky. To facilitate this analysis we need to load the lubridate package. lubridate is another package created by Hadley Wickham, which fits into the tidyverse manner of dealing with data. It is not among the packages loaded with library(tidyverse), and so it is necessary to load it separately.\n\nlibrary(lubridate)\n\nSince the letters in this data set were all received in one year, it would be interesting to see how many letters Daniel received each month. This question is particularly interesting, because in 1585 Daniel’s home city of Antwerp was under siege by Spanish troops, and Daniel was serving as a representative for his city to the rebels in Holland. It is also an interesting issue of analysis, because while the letters data frame contains information about the month the letters were sent, there is no month column. This prevents us from using the group_by() and summarise() workflow that we have developed. The answer comes from the lubridate package, which includes a function called month(). This function extracts the month from a date object, which we know the date column is, because it is identified as such when we have printed the letters data frame. The goal is to create a new column called month. This is done through the dplyr function mutate(), which creates the column and then applies the month() function to each of the dates. The remaining code is similar to that used above, but now the column we want to group_by() and summarise() is the newly created month column.\n\nper_month &lt;- letters %&gt;% \n  mutate(month = month(date)) %&gt;% \n  group_by(month) %&gt;% \n  summarise(count = n())\nper_month\n#&gt; # A tibble: 12 × 2\n#&gt;    month count\n#&gt;    &lt;dbl&gt; &lt;int&gt;\n#&gt;  1     1    11\n#&gt;  2     2    14\n#&gt;  3     3    18\n#&gt;  4     4     9\n#&gt;  5     5    14\n#&gt;  6     6     9\n#&gt;  7     7     9\n#&gt;  8     8    14\n#&gt;  9     9    10\n#&gt; 10    10     2\n#&gt; 11    11     1\n#&gt; 12    12     3\n\nLooking at the result of this code, a problem is immediately apparent. The number of letters per month is as expected, but the months are returned as numbers, which is less than ideal. However, by looking at the documentation for lubridate by typing ?month() into the console, it is possible to see that what needs to be done is to change the label argument to TRUE from the default of false.\n\nper_month &lt;- letters %&gt;% \n  mutate(month = month(date, label = TRUE)) %&gt;% \n  group_by(month) %&gt;% \n  summarise(count = n())\nper_month\n#&gt; # A tibble: 12 × 2\n#&gt;    month count\n#&gt;    &lt;ord&gt; &lt;int&gt;\n#&gt;  1 Jan      11\n#&gt;  2 Feb      14\n#&gt;  3 Mar      18\n#&gt;  4 Apr       9\n#&gt;  5 May      14\n#&gt;  6 Jun       9\n#&gt;  7 Jul       9\n#&gt;  8 Aug      14\n#&gt;  9 Sep      10\n#&gt; 10 Oct       2\n#&gt; 11 Nov       1\n#&gt; 12 Dec       3\n\nNow that we have the data in a good form we can plot it by making another bar chart.\n\nggplot(data = per_month) +\n  geom_bar(aes(x = month, y = count), stat = \"identity\") +\n  labs(x = 1585, y = \"Letters sent\")\n\n\n\n\n\n\n\nThe graph shows a peak in March of 1585 when Antwerp was in an extremely vulnerable position and it was more important than ever that it receive military and monetary assistance from the rebels in Holland. Another peak is reached in August when Antwerp surrendered, forcing Daniel and his family members to go into exile. The letters declined beginning in October, when Daniel left Holland to live in exile with his family in Bremen.\nAs a fun aside, it is also possible to see what day of the week the letters were sent. Luckily, the data comes from 1585, three years after the creation of Gregorian Calendar, and the areas from which the letters were sent had already transitioned to the new calendar. This means that they were using the same calendar as we do now, as opposed to a place like England, which only adopted the Gregorian Calendar in 1752. Therefore, we do not have to worry about adding 10 days to move from the old calendar to the new calendar.\n\nper_wday &lt;- letters %&gt;% \n  mutate(wday = wday(date, label = TRUE)) %&gt;% \n  group_by(wday) %&gt;% \n  summarise(count = n())\nper_wday\n#&gt; # A tibble: 7 × 2\n#&gt;   wday  count\n#&gt;   &lt;ord&gt; &lt;int&gt;\n#&gt; 1 Sun       7\n#&gt; 2 Mon      16\n#&gt; 3 Tue      20\n#&gt; 4 Wed      16\n#&gt; 5 Thu      22\n#&gt; 6 Fri      16\n#&gt; 7 Sat      17\n\nWe can even create the chart using the original letters data frame and call the wday() function within the definition of the x variable.\n\nggplot(data = letters) +\n  geom_bar(aes(x = wday(date, label = TRUE))) +\n  labs(x = 1585, y = \"Letters sent\")\n\n\n\n\n\n\n\nFinally, let’s see who sent Daniel letters on Sundays. We can use the filter() function, which returns rows that match a certain argument. The below code uses the knowledge that without labels, Sunday is equivalent to 1. The code is also written in a slightly different style. Even though only a single function is called, %&gt;% is used to pipe the data frame into the function, which cleans up the function call slightly.\n\nletters %&gt;% \n  filter(wday(date) == 1)\n#&gt; # A tibble: 7 × 4\n#&gt;   writer                  source  destination date      \n#&gt;   &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;       &lt;date&gt;    \n#&gt; 1 Della Faille, Jacques   Haarlem Delft       1585-02-10\n#&gt; 2 Meulen, Andries van der Antwerp Delft       1585-03-31\n#&gt; 3 Meulen, Andries van der Antwerp Delft       1585-04-28\n#&gt; 4 Della Faille, Jacques   Haarlem Delft       1585-05-12\n#&gt; 5 Wale, Jan de            Venice  Haarlem     1585-06-23\n#&gt; 6 Della Faille, Jacques   Haarlem Delft       1585-08-18\n#&gt; 7 Della Faille, Jacques   Haarlem Delft       1585-09-08"
  },
  {
    "objectID": "post/excel-vs-r/index.html#combining-data-for-correspondents-and-dates",
    "href": "post/excel-vs-r/index.html#combining-data-for-correspondents-and-dates",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Combining data for correspondents and dates",
    "text": "Combining data for correspondents and dates\nThe group_by() and summarise() workflow developed so far results in data frames with two columns and thus only tells us about one variable from the data at a time. We can get a better understanding of Daniel’s network by looking into how many letters each writer sent per month, which would involve the creation of a data frame with three columns. One advantage of using code is that it is often relatively simple to take a functioning line of code and tweak it to create a different result. Here, I take the line of code that produced per_month and alter it by adding the writer variable to the group_by function. Now the code will group the letters written by each correspondent per month and then count them. Notice how the dimensions of the resulting data frame has changed.\n\ncorrespondent_month &lt;- letters %&gt;% \n  mutate(month = month(date, label = TRUE)) %&gt;% \n  group_by(writer, month) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count))\n#&gt; `summarise()` has grouped output by 'writer'. You can override using the\n#&gt; `.groups` argument.\ncorrespondent_month\n#&gt; # A tibble: 34 × 3\n#&gt; # Groups:   writer [12]\n#&gt;    writer                  month count\n#&gt;    &lt;chr&gt;                   &lt;ord&gt; &lt;int&gt;\n#&gt;  1 Meulen, Andries van der Mar      12\n#&gt;  2 Meulen, Andries van der Jan      10\n#&gt;  3 Meulen, Andries van der Feb       9\n#&gt;  4 Meulen, Andries van der Jul       8\n#&gt;  5 Della Faille, Jacques   Aug       7\n#&gt;  6 Della Faille, Jacques   Sep       7\n#&gt;  7 Meulen, Andries van der Apr       7\n#&gt;  8 Meulen, Andries van der May       7\n#&gt;  9 Della Faille, Jacques   May       5\n#&gt; 10 Meulen, Andries van der Jun       5\n#&gt; # … with 24 more rows\n\nWith this new object called correspondent_month it is possible to add to the bar chart on letters per month, by filling in the bars with letters per correspondent in each month. The only difference in the code below from the earlier chart is the inclusion of fill = writer in geom_bar() and then giving it a label in labs().\n\nggplot(data = correspondent_month) +\n  geom_bar(aes(x = month, y = count, fill = writer), stat = \"identity\") +\n  labs(x = 1585, y = \"Letters sent\", fill = \"Correspondents\")\n\n\n\n\n\n\n\nA bar graph is only one of many different geoms made available through ggplot, so let me finish by using the same information to produce a line graph with points showing the amount of letters each correspondent sent in a month. The structure of the command is slightly different here, because I place the aes() function in the ggplot() function. This is done because the same aesthetics will be used by both geom_point() and geom_line(). In the geom_point() function I increase the size of all of the points so that they are more clearly visible. The geom_line() function adds a new aesthetic for group, which tells the function how to connect the lines.\n\nggplot(data = correspondent_month, aes(x = month, y = count, color = writer)) + \n  geom_point(size = 3) + \n  geom_line(aes(group = writer)) + \n  labs(x = 1585, y = \"Letters sent\", color = \"Correspondents\")\n\n\n\n\n\n\n\nThese two graphs provide different ways to visualize the data. Both give a clearer picture of the development of Daniel’s correspondence over the course of 1585. Up until September, Andries was Daniel’s most signifiant correspondent. After the fall of Antwerp, the two lived together in Bremen, forestalling any need to communicate by correspondence. On the other hand, in the second half of the year, Daniel’s correspondence picked up with his brother-in-law, Jacques."
  },
  {
    "objectID": "post/excel-vs-r/index.html#conclusion",
    "href": "post/excel-vs-r/index.html#conclusion",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Conclusion",
    "text": "Conclusion\nThe examples above only display a small fraction of the analysis and visualization capabilities of R. More fully developed visualizations with a larger set of the data used here can be seen on the the Projects page of this website, including an interactive map of the letters that can be filtered by date. Going from no coding experience to the production of visualizations in R is by no means easy, though there is an ever-growing set of resources designed to make the process as easy as possible. This post is part of my attempt to add to these resources from the perspective of a historian. In future posts I will build on the foundation provided here. In particular, the above analysis did not take advantage of the geographic nature of the data. I will discuss the process of geocoding the data and mapping it in an upcoming post."
  },
  {
    "objectID": "post/excel-vs-r/index.html#footnotes",
    "href": "post/excel-vs-r/index.html#footnotes",
    "title": "Excel vs R: A Brief Introduction to R",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis post concentrates on the basic aspects of data analysis and visualization using the popular dplyr and ggplot packages for R. In future posts, I will discuss more aspects of R.↩︎\nEpisode 9 of the Not So Standard Deviations podcast has a good discussion of Excel vs R, which helped me think through my own thoughts on this subject.↩︎\nMy workflow is to enter data into Numbers, export it into a csv file, and then analyze with R. I have found data entry directly into csv too clunky, and so spreadsheets still have a place in my life.↩︎\nThere are a variety of different data types in R, but in this post I will concentrate on tabular data, or data frames, because that is the form of data derived from spreadsheet and most used within the humanities and social sciences.↩︎\nSee the explanation and examples below on how objects in R are created and manipulated.↩︎\nThis was the position that I was in only a couple of months ago.↩︎\nIf you are interested in learning how to use R, two good places to start are Garrett Grolemund and Hadley Wickham, R for Data Science and Roger Peng, R Programming for Data Science. Both are available for free and cover the topics that I discuss here in much greater detail.↩︎\nDo not worry about the [1] in the printed output. This merely informs us that this is the first number in the object x. Behind the scenes in R x is not simply a number but a vector of numbers of length one.↩︎\nA common frustration when learning to code is how persnickety computers are about typos and grammar. If a command does not work, it is likely because there is a typo somewhere.↩︎\nIf you want to know how a function works or what arguments are necessary to run the function, you can always access a function’s documentation with ?function_name(). The documentation for many functions tend to be jargon heavy, but most also contain examples of how to use the function.↩︎\nThe best resource for learning about these dplyr functions is Chapter 5 of Grolemund and Wickham, R for Data Science.↩︎\nThere are many resources on learning ggplot. Chapter 3 of Grolemund and Wickham, R for Data Science is a good place to start. Another invaluable resource for ggplot that is still under development is Kieran Healy’s Data Visualization for Social Science↩︎\nRemember that a data frame is essentially a table. To be more precise, in this case, the read_csv() function produces a tibble, which is a special kind of data frame.↩︎\nOne advantage of tibbles over default data frames is that tibbles are smart about printing out a subset of the data to the console instead of all of the content. The latter approach becomes messy when data frames have hundreds or thousands of rows.↩︎\nRStudio comes with its own viewer, enabling you to view the contents of data frames in a manner that replicates the experience of a spreadsheet.↩︎"
  },
  {
    "objectID": "post/my-approach-to-dh/index.html",
    "href": "post/my-approach-to-dh/index.html",
    "title": "My Approach to Digital Humanities",
    "section": "",
    "text": "Digital humanities holds the promise of increasing the means by which scholars are able to analyze and present data. Though some sentiments about the significance of digital humanities might be overblown, there is no doubt that the more ways we have to analyze sources the better. Learning a variety of the tools that make up the rather nebulous universe of digital humanities is like learning a new language. It opens up new possibilities that were previously closed or necessitated the expertise of others. This frames digital humanities as a collection of skills rather than a means to a predetermined end. I have adopted this perspective in learning about the possibilities opened by digital humanities and working on a digital humanities project. I am hardly the first person to take these steps, but I hope that by explaining my thought process I can set a basis for future posts on digital humanities.\nIf there has been one guiding force in my approach to digital humanities, it is to learn skills and tools in the process of production. In a way, this is simply the application of critical thinking to how I create my scholarly work. Instead of doing things in what seems the de facto manner, I have sought to question if there is either a more efficient way or a way in which I could gain or improve my competency. The goal of efficiency was particularly significant in thinking about how to organize my research and writing (DH 1.0), while learning new skills has been more important in the production of digital humanities projects (DH 2.0). This may not be the easiest way to complete a project in the short run, but by doing things the hard way, I am looking to open up new opportunities for future projects. With this theoretical approach in mind, let me now discuss a few concrete principles of my digital humanities practices concerning text, applications, and producing digital humanities projects.\nAt the basis of all my research is the use of plain text, which I usually write in Markdown. All of my research notes are in plain text, and I wrote drafts for over half of my dissertation in plain text. Writing in plain text has a number of advantages — which I will discuss in a future post — but an unexpected benefit is that writing Markdown changed my approach to text in a way that gave me a solid basis to tackle HTML, CSS, and even coding.1 In addition, Markdown is ubiquitous on the web. This blog post is written in Markdown, and I have made interactive visualizations with R Markdown, an extension of Markdown for R.\nOf course, plain text by itself only gets you so far. The power of text has to be harnessed by applications. A major turning point for me in the efficiency and power of my research workflow was the realization that investment in apps, in terms of both money and time, could provide long-term benefits. Using free apps is definitely nice, but if you consider how many hours you spend in an app that plays a significant role in your workflow, the investment is usually worth it. I have come to have real enjoyment in using certain apps. I also think that is is worth while to take some time to learn how to use each app efficiently and even to learn some history about them. There is nothing worse than adapting to a new app only to learn that it is no longer under development. Though there are exceptions, many of my most essential apps have long histories and a solid base of users that points towards continued development.2\nIn terms of the tools and choices I have made in the production of digital humanities projects, I have almost always chosen in favor of more control and customization over plug-and-play options. This goal is often in parallel with choosing to use open standards — with plain text being the ultimate open standard — and open-source software, which for me has come to include the use of programming languages such as R. Complete control and knowledge is certainly not possible, at least not for me, but more control goes along with my goal of learning by doing. This website is a simple example of this approach. I was never going to be able to make this website by hand, but instead of using perfectly good ready-made solutions such as Squarespace or Wordpress, I decided to make a static website using Hugo. Plug-and-play approaches may satisfy the visualization aspect of digital humanities, but they often do little to improve your skills, or at least they are not extendable in the same way that coding skills are. When faced with a decision about how do something for a digital humanities project, I often see an easy way and a more complex route. My approach has been to choose the more complex route. This way is often not overly difficult if you know what to do, but I usually do not have that knowledge, and so the process is slow and arduous. However, the goal is that in completing the task I learn a new set of skills or improve those I already had, making it possible for me to use them in future contexts."
  },
  {
    "objectID": "post/my-approach-to-dh/index.html#footnotes",
    "href": "post/my-approach-to-dh/index.html#footnotes",
    "title": "My Approach to Digital Humanities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMarkdown is essentially a simpler way to write HTML. It is very approachable, but it still has the basic structure of all markup languages or code in the need to properly structure the markup. If something goes wrong, it is almost always because you mistyped something.↩︎\nI use a number of apps from the Omni Group, which began writing apps for NeXT, and BBEdit, which has been actively developed for the Mac since 1992.↩︎"
  },
  {
    "objectID": "post/by-way-of-introduction/index.html",
    "href": "post/by-way-of-introduction/index.html",
    "title": "By Way of Introduction",
    "section": "",
    "text": "In this introductory post I want to lay out the reasons for the creation of this website, to discuss some content that I will be looking to create, and to set some goals for the site. First though, I should first introduce myself. I am a historian of early modern Europe. My research investigates merchant families and the social basis of trade, politics, and religion. Specifically, I have worked in the archives of the Van der Meulens and Della Failles, two merchant families from Antwerp involved in European-wide trade at the end of the sixteenth century. I am currently working on a manuscript that argues for the significance of sibling relationships and inheritance in the development of early modern capitalism. In addition, I am working on some digital humanities projects using the two archives.\nThere is no need to justify creating a website in 2017, nevertheless I want to put forward the specific reasons that led me to create this site. Fundamentally, this website provides a space for me to showcase projects either digital or traditional, syllabi for courses that I have taught, and a place to link to my cv for anyone interested. However, the actual impetus for creating this website is that I have been developing a few digital humanities projects that I want to both discuss and display. The creation of the website itself has also been a digital—if not necessarily digital humanities—project in itself. Instead of using a more ready-made solution such as SquareSpace or WordPress, this website is a static site, which means that it is composed of HTML, CSS, and JavaScript, and all of the content is produced in Markdown documents. I am using the static site generator Hugo to build the site and am deploying it through Netlify. This process enabled me to learn more about these technologies and have greater control over the process and content. You can view all of the components of the raw website on my GitHub page.\nConcerning, the actual content of this blog, I envision the posts falling into two general categories. In the first place, the blog will be a space for me to discuss the various projects that I am working on, both traditional history projects and those in digital humanities. Emphasis will be on the latter, since it was in the course of working on digital projects that I created this site. Secondly, I want to use the blog to discuss my own process and reasons for learning some of the skills of digital humanities. I want to document what I have done, and hopefully in doing so, I can provide useful information to others looking into the field of digital humanities. I should say that I use Apple products for all of my computing, and so my posts will be centered on macOS and iOS.\nI am planning on writing two main series of posts about my path in digital humanities, pointing towards strategies and resources that I have found useful. I am calling these series DH 1.0 and DH 2.0. The differences between the two are both personal and technical. What I am labelling DH 1.0 is a series of strategies that I developed while in graduate school to optimize the way that I conduct research and turn that research into written work. This mainly involved taking advantage of various GUI applications to make my research notes and writing process more organized and readily searchable. DH 2.0 will discuss the second step that I have more recently taken to produce the digital humanities projects that will be shown on the site, including going through the creation of the website itself. The skills of DH 2.0 build on those I developed in DH 1.0, but instead of GUI applications, I have moved to the use of code to analyze data and produce visualizations. I will discuss the challenges and advantages I have found in moving to what is for me a very new way of thinking. For now, you can see the fruits of my ongoing digital humanities project on the correspondence network of Daniel van der Meulen on the Projects page of the website."
  },
  {
    "objectID": "post/new-kinds-of-projects/index.html",
    "href": "post/new-kinds-of-projects/index.html",
    "title": "New kinds of Projects: DH 2.0 and Coding",
    "section": "",
    "text": "In the process of learning about how I could use digital technologies to better organize my research, I quickly started to think about how I might extend these skills to produce new kinds of outputs.1 I was familiar with the concept of digital humanities, but the step from an internal process of organizing research and writing to production seemed both too nebulous and difficult. Digital humanities also seemed to concentrate on the visual. This was intriguing, but did not present itself as the most pressing need for a graduate student who was writing a dissertation on sibling relations among 16th-century merchants. It took years for me to move from what I am calling DH 1.0 to DH 2.0, to move from research methodology to making what could properly be termed digital humanities projects. This post presents an introduction to how I eventually took this step and why I decided to learn to code instead of other available solutions.2\nIn reading tech websites and listening to tech podcasts I inevitably came into contact with the question of how to learn to program. A refrain that I heard over and over was that it was best to have a project in mind. Programming, I was told, is not an abstract entity, but something that one does to complete a set of specific tasks. In one sense this advice was encouraging. I had a possible digital humanities project in mind. I wanted to map a correspondence network of the letters found in the archive of Daniel van der Meulen. The archive contains approximately 6,600 letters that Daniel received from 1578 to 1600. A chapter of my dissertation analyzes the collection, but the work was all done by hand in spreadsheets. When it came time to visualize the data, I was not able to find the time or energy to do more than make rather simplistic maps on a website and creating some graphs with Numbers. This sufficed for the purposes of the dissertation, but it did not bring me any closer to digital humanities. Alongside the issue of time, I simply did not know what a solution could look like with more sophisticated tools much less with code. Working with data in code seemed interesting and powerful, but I had difficulty moving beyond my naive ideas about coding and digital humanities where there appeared endless possibilities but concrete steps were mysterious and nebulous.3\nAt the American Historical Association’s annual meeting in Denver in January of this year I was inspired by a number panels on the use of digital humanities in History, especially by Henry Lovejoy’s Liberated Africans Project and Kate Craig’s discussion of using digital humanities in the classroom.4 These panels motivated me to finally take the initiative to learn more about the tools of digital humanities and think about how I could create a project analyzing and visualizing the correspondence network of Daniel van der Meulen. I started by gathering together information on digital humanities in general. I found a plethora of sources such as AHA’s Digital History Resources, DH Toychest, Digital Humanities Now, and The Programming Historian. Most helpful in the early going was reading through Johanna Drucker’s Introduction to Digital Humanities Course Book: Concepts, Methods, and Tutorials for Students and Instructors. This book is based on the Digital Humanities 101 course at UCLA and covers topics from HTML to textual analysis. Most interesting for me were the sections on networks and GIS or Geographical Information Systems.\nMy initial research gave me a starting point, but now I actually had to choose a path forward and decide on a tool or set of tools to create the project. I began by playing around with Tableau Public and Palladio. Both provide more or less ready-made visualizations of spreadsheet data, the former as a desktop application and the latter as a web app. Both were interesting, but they lacked in terms of customization and did little to help me learn about producing visuals. I also tested out Gephi, an open-source application for the visualization of networks. I went through Martin Grandjean’s very helpful Introduction to Network Analysis and Visualization. However, because my project was aimed at producing a geographic representation of a network, I gravitated towards GIS tools. Initially, I was interested in QGIS, an open-source GIS application popular among geographers, but I also kept thinking of the possibility of doing something completely different and trying to do the project with code. Having no experience with programming or code, I remained hesitant, but a couple of resources showed me that what I wanted to do was possible through code.\nIn getting a sense of digital humanities and digital history I came across the work of Lincoln Mullen, a historian at George Mason. Lincoln has authored a number of digital history projects using the programming language R. R was made by statisticians, but over the twenty years of its existence it has developed into a programming language fully capable of both data analysis and visualization. I read the Introduction to Lincoln Mullen’s Computational Historical Thinking: With Applications in R. The book is very much a work in progress, but the introduction was enough to convince me that I could accomplish my goals with R. Further exploring resources on R, I came across Taylor Arnold and Lauren Tilton’s Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text.5 This book put all my reservations about learning to code and the possibilities of using R to rest. I was convinced that I needed to dive all the way in and learn R. I knew that it would not be the easiest way to produce a visualization of a correspondence network, but I also began to realize the actual potential of code."
  },
  {
    "objectID": "post/new-kinds-of-projects/index.html#footnotes",
    "href": "post/new-kinds-of-projects/index.html#footnotes",
    "title": "New kinds of Projects: DH 2.0 and Coding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more on how I got interested in digital technologies see the below post Thinking about Workflow: DH 1.0↩︎\nIn this post I am using both coding and programming in a colloquial manner. By both terms I simply mean the creation of computer code to analyze and visualize data.↩︎\nTaking a Digital Humanities course at UCLA was obviously an option, but, at least at the time, enrolling in a class never seemed particularly practical in the middle of writing my dissertation.↩︎\nBoth are former colleagues at UCLA.↩︎\nYou may be able to download a free copy of Humanities Data in R through your university.↩︎"
  },
  {
    "objectID": "post/thinking-about-workflow/index.html",
    "href": "post/thinking-about-workflow/index.html",
    "title": "Thinking about Workflow: DH 1.0",
    "section": "",
    "text": "In the spring of 2011, I was in the middle of doing research for my dissertation. I had recently returned from my second extended trip to the archives in the Netherlands and Belgium and had accumulated a ton of notes. I knew that technology had drastically altered the possibilities for research, but the fundamentals of my own workflow were hardly different than they had been when I began undergrad in the early 2000s. Sure, I used the internet to watch Netflix, but the basic tools—centered on the Microsoft Office Suite—were essentially the same. One day, I gave in to the nagging feeling that I was not getting enough out of my computer, that there were better ways of doing things. I started to poke around the internet to see if I could find ways to improve my workflow. What started as a distraction soon turned into a months-long project on finding better tools for conducting research. I quickly realized that the actual process of getting work done, doing research and writing papers, is little discussed in graduate school. Early career graduate students are so busy reading, writing, and generally trying to keep up, that the process of how to do the work is easily pushed to the background. Since then, I have tried to think critically and systematically about my workflow and have often discussed this with others.\nMy desire to embark upon a technological odyssey was driven by a very simple problem that developed naturally in the course of research for my dissertation. My primary research involved reading 16th-century correspondence. I used a simple—and I think somewhat commonsensical—approach to taking notes. For each correspondent in the archive, I would create a new Word document and do my best to paraphrase what was said in the letter. As time went by, these Word documents became quite long. After a month of taking notes on the letters of one correspondent, my Word document reached 100 pages single-spaced. At this point, writing in the document became unbearable. It would take almost a minute for the document to fully load. I would sit there and look at the bottom of the window and see the page count go up: 10 pages, then 15 pages, then 17 pages for a document I knew to be over 100 pages long. Scrolling through the document was difficult, and I had no real way to find anything in the middle of the document other than manually scrolling to where I thought it might be; search was a disaster.1 My solution at the time was simply to open up another Word document and continue typing. This solution was neither practical nor scalable.\nWhen I finally decided to do something about this issue, it did not take much searching to come up with the solution: text files. I knew that my Mac came with a program called TextEdit, but I had never thought to use it. I began to copy and past some of my longer notes from word into TextEdit. Upon opening the files, I saw to my amazement that they were completely functional in under a second. One hundred page documents opened in a flash.\nThis simple change away from Word to text documents opened up a world of possibilities. It caused me to pause and think. If this way of taking notes was so much better and so simple, why did I not know about it. What else was I missing? It took a while and a lot of trial and error before I found a system that worked for me. I first moved to rich text files, before turning to plain text. But I had begun the process. Plain text proved to be the basis for my research workflow and for what I am calling DH 1.0. Even more significant than the particular solution that I developed was thinking critically about my workflow. I began to read articles and listen to podcasts about technology and Apple. I started to use an RSS reader to subscribe to blogs. And I joined Twitter. You can obviously go too far in thinking about how to get things done to the point that it distracts you from doing the actual work, but that does not mean it is not worth the time and effort to think through how you do things. One of my goals in this blog and in future posts is to discuss the system that I developed based on plain text files and why I think it is better."
  },
  {
    "objectID": "post/thinking-about-workflow/index.html#footnotes",
    "href": "post/thinking-about-workflow/index.html#footnotes",
    "title": "Thinking about Workflow: DH 1.0",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis was with Word 2009. I do not know if new versions are better. I haven’t tried.↩︎"
  },
  {
    "objectID": "post/geocoding-with-r/index.html",
    "href": "post/geocoding-with-r/index.html",
    "title": "Geocoding with R",
    "section": "",
    "text": "In the previous post I discussed some reasons to use R instead of Excel to analyze and visualize data and provided a brief introduction to the R programming language. That post used an example of letters sent to the sixteenth-century merchant Daniel van der Meulen in 1585. One aspect missing from the analysis was a geographical visualization of the data. This post will provide an introduction to geocoding and mapping location data using the ggmap package for R, which enables the creation of maps with ggplot. There are a number of websites that can help geocode location data and even create maps.1 You could also use a full-scale geographic information systems (GIS) application such as QGIS or ArcGIS. However, an active developer community has made it possible to complete a full range of geographic analysis from geocoding data to the creation of publication-ready maps with R.2 Geocoding and mapping data with R instead of a web or GIS application brings the general advantages of using a programming language in analyzing and visualizing data. With R, you can write the code once and use it over and over, while also providing a record of all your steps in the creation of a map.3\nThis post will merely scratch the surface of the mapping capabilities of R and will not enter into the domain of the more complex specific geographic packages available for R.4 Instead, it will build on the dplyr and ggplot skills discussed in my brief introduction to R. The example of geocoding and mapping with R will also provide another opportunity to show the advantages of coding. In particular, geocoding is a good example of how code can simplify the workflow for entering data. Instead of dealing with separate spreadsheets to store information about the letters and geographic information, coding makes it possible to create the geographic information directly from the letters data. The code to find the longitude and latitude of locations can be saved as a R script and rerun if new data is added to ensure that the information is always kept up to date."
  },
  {
    "objectID": "post/geocoding-with-r/index.html#preparing-data",
    "href": "post/geocoding-with-r/index.html#preparing-data",
    "title": "Geocoding with R",
    "section": "Preparing the data with dplyr",
    "text": "Preparing the data with dplyr\nIn this example, I will use the same database of letters sent to Daniel van der Meulen in 1585 as I did in the previous post. You can find the data and the R script that goes along with this tutorial on GitHub. Before getting into the database of letters and figuring out how to geocode the locations found in the data, it is necessary to set up the environment in R by loading the libraries that we will be using. Here, I load both the tidyverse library to import and manipulate the data and the ggmap library to do the actual geocoding and mapping.\nlibrary(tidyverse)\nlibrary(ggmap)\nAs in the previous post, the data is loaded through the read_csv() function from the readr package. It is best to keep the names of objects consistent across scripts. Therefore, I name the object letters with the assignment operator. Printing out the contents reveals the that letters is a tibble with 114 letters and four columns.\nletters &lt;- read_csv(\"data/correspondence-data-1585.csv\")\nletters\n#&gt; # A tibble: 114 x 4\n#&gt;                     writer  source destination       date\n#&gt;                      &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;     &lt;date&gt;\n#&gt;  1 Meulen, Andries van der Antwerp       Delft 1585-01-03\n#&gt;  2 Meulen, Andries van der Antwerp     Haarlem 1585-01-09\n#&gt;  3 Meulen, Andries van der Antwerp     Haarlem 1585-01-11\n#&gt;  4 Meulen, Andries van der Antwerp       Delft 1585-01-12\n#&gt;  5 Meulen, Andries van der Antwerp     Haarlem 1585-01-12\n#&gt;  6 Meulen, Andries van der Antwerp       Delft 1585-01-17\n#&gt;  7 Meulen, Andries van der Antwerp       Delft 1585-01-22\n#&gt;  8 Meulen, Andries van der Antwerp       Delft 1585-01-23\n#&gt;  9    Della Faille, Marten Antwerp     Haarlem 1585-01-24\n#&gt; 10 Meulen, Andries van der Antwerp       Delft 1585-01-28\n#&gt; # ... with 104 more rows\nTo do the actual geocoding of the locations I will be using the mutate_geocode() function from the ggmap package. To geocode a number of locations at one time, the function requires a data frame with a column containing the locations we would like to geocode. The goal, then, is to get a data frame with a column that contains all of the distinct locations found in the letters data frame. In the introduction to R post I used the distinct() function to get data frames with the unique sources and destinations. We can rerun that code here and look at the results.\nsources &lt;- distinct(letters, source)\ndestinations &lt;- distinct(letters, destination)\n\nsources\n#&gt; # A tibble: 9 x 1\n#&gt;      source\n#&gt;       &lt;chr&gt;\n#&gt; 1   Antwerp\n#&gt; 2   Haarlem\n#&gt; 3 Dordrecht\n#&gt; 4    Venice\n#&gt; 5     Lisse\n#&gt; 6  Het Vlie\n#&gt; 7   Hamburg\n#&gt; 8     Emden\n#&gt; 9 Amsterdam\n\ndestinations\n#&gt; # A tibble: 5 x 1\n#&gt;   destination\n#&gt;         &lt;chr&gt;\n#&gt; 1       Delft\n#&gt; 2     Haarlem\n#&gt; 3   The Hague\n#&gt; 4  Middelburg\n#&gt; 5      Bremen\nA glance at the two data frames shows that neither provide exactly what I are looking for. Neither the sources nor the destinations data frames include all of the locations that we want to geocode. It would be possible to geocode both the sources and destinations data frames, but this would place the geocoded information in two different data frames, which is less than ideal. Instead, we can join the two data frames together using the join functions in dplyr. Coming from the world of spreadsheets, the join functions are a revelation, opening up seemingly endless possibilities for data manipulation.\nThe dplyr package includes a number of functions to join two data frames together to create a single data frame. The join functions use overlapping columns of data contained in both data frames, called keys, to match up the data. There are three join functions that are used most often. The left_join() keeps all observations from the left, or first, data frame, while dropping all rows from the right, or second, data frame that do not have a match in the left data frame. The inner_join() only keeps rows that contain matching keys in both data frames. Conversely, the full_join() brings together all rows from both the left and right data frames. The differences between these functions may not be immediately apparent, but you can experiment with them to see the variety of outputs they create.\nUsing sources as the left and destinations as the right data frames, a left_join() creates a new object with 9 rows, an inner_join() results in only one row, and a full_join() contains 13 observations. Thus, the full_join() is what we are looking for. The full_join() function — like the other join functions — takes three arguments: the two data frames to join and the key column by which they are to be joined. In this case, some extra work needs to be done to the identify the key columns. The key columns are the only columns in the two data frames, but because they have different names, it is necessary to declare that they are equivalent. This is done with the help of the concatenate or combine function, c().5 The below command creates a new data frame that I have called cities, which brings together the “source” column with the “destination” column and therefore contains all of the locations found in the letters data frame.\ncities &lt;- full_join(sources, destinations, by = c(\"source\" = \"destination\"))\ncities\n#&gt; # A tibble: 13 x 1\n#&gt;        source\n#&gt;         &lt;chr&gt;\n#&gt;  1    Antwerp\n#&gt;  2    Haarlem\n#&gt;  3  Dordrecht\n#&gt;  4     Venice\n#&gt;  5      Lisse\n#&gt;  6   Het Vlie\n#&gt;  7    Hamburg\n#&gt;  8      Emden\n#&gt;  9  Amsterdam\n#&gt; 10      Delft\n#&gt; 11  The Hague\n#&gt; 12 Middelburg\n#&gt; 13     Bremen\nPrinting out the cities data frame shows that there are 13 distinct locations in the letters data. However, the structure of the cities object is less than ideal. The name of the column is “source,” which was taken over from the sources data frame, but this is not an accurate description of the data in the column. This can be fixed with the help of rename(), which uses the structure of new_name = old_name. Here, I change the name of the “source” column to “place” and then print out the first two rows with the head() function to show the change in the column name. Notice that using the cities object within the rename() function and using the same name for the result overwrites the original object with the new one. Alternatively, you could name the new object a different name such as cities1.6\ncities &lt;- rename(cities, place = source)\nhead(cities, n = 2)\n#&gt; # A tibble: 2 x 1\n#&gt;     place\n#&gt;     &lt;chr&gt;\n#&gt; 1 Antwerp\n#&gt; 2 Haarlem"
  },
  {
    "objectID": "post/geocoding-with-r/index.html#geocoding",
    "href": "post/geocoding-with-r/index.html#geocoding",
    "title": "Geocoding with R",
    "section": "Geocoding with ggmap",
    "text": "Geocoding with ggmap\nWe now have an object with the basic structure needed to geocode the locations. However, if you run mutate_geocode() on the cities object as it is, you will receive an error. The error is a good example of a common frustration with coding. Computers are picky, and because humans also write flawed code, bugs exist, making computers picky in odd ways. In this case, we are running into a problem in which the mutate_geocode() function will not work on tibbles. As noted in my previous post, tibbles are a special kind of data frame used by the tidyverse set of packages. It is usually easier to work with tibbles in the tidyverse workflow, but here it is necessary to convert the tibble to a standard data frame object with as.data.frame(). I give the result a new name to distinguish it from the cities tibble.\ncities_df &lt;- as.data.frame(cities)\nThe locations data from the letters sent to Daniel is now ready to be geocoded. The mutate_geocode() function uses Google Maps to find the longitude and latitude of each location. The two necessary arguments are the data frame and the name of the column with the location data. The function can be used to find more information about each location, including the country and region, but here I just have the function return the longitude and latitude data. The function will query Google Maps, and so you must have an internet connection. This makes running the command relatively slow, especially if your data contains a large amount of locations. There is also a limit of 2,500 queries per day, so you may have to find other methods if you are geocoding thousands of locations.\nlocations_df &lt;- mutate_geocode(cities_df, place)\nBecause mutate_geocode() necessitates an internet connection, is somewhat slow to run, and has a daily limit, it is not something that you want to do all the time. It is a good idea to save the results by writing the object out to a csv file. Before doing this, however, we should inspect the data and make sure everything is correct. Let’s start by printing out the locations_df object and see what we have.\nlocations_df\n#&gt;         place       lon      lat\n#&gt; 1     Antwerp  4.402464 51.21945\n#&gt; 2     Haarlem  4.646219 52.38739\n#&gt; 3   Dordrecht  4.690093 51.81330\n#&gt; 4      Venice 12.315515 45.44085\n#&gt; 5       Lisse  4.557483 52.25793\n#&gt; 6    Het Vlie  5.183333 53.30000\n#&gt; 7     Hamburg  9.993682 53.55108\n#&gt; 8       Emden  7.206010 53.35940\n#&gt; 9   Amsterdam  4.895168 52.37022\n#&gt; 10      Delft  4.357068 52.01158\n#&gt; 11  The Hague  4.300700 52.07050\n#&gt; 12 Middelburg  3.610998 51.49880\n#&gt; 13     Bremen  8.801694 53.07930\nAt a quick glance, the result looks like what we would expect. locations_df is a data frame with three columns called “place,” “lon,” and “lat” with the latter two representing longitude and latitude of the location named in the first column. One thing that we might want to change, though this step is not necessary, is to convert the data frame back to a tibble, which can be done with the as_tibble() function.\nlocations &lt;- as_tibble(locations_df)\nA quick look at the values of the longitude and latitude columns in the locations data would seem to indicate that the geocoding process occurred correctly. All of the longitude and latitude values are within a reasonable range given the cities in the data. However, because the data given to the Google Maps query consisted of only the names of the cities, it is always worth double checking the returned values. As anyone who has ever used Google Maps before knows, it is possible that the query could return the location of a different place that has the same name.7\nOne way to check that the geocoding was done correctly is to map the locations with the mapview package. Using mapview requires converting the locations tibble to yet another format. This is done with the simple features package, which brings us to the world of GIS with R. The first step is to load the sf and mapview packages.\nlibrary(sf)\nlibrary(mapview)\nI will not get into the details of the sf package and type of objects that it creates here, but the function to transform the locations tibble into an sf object is understandable even without knowing the details. The function to make an sf object takes three main arguments. The first two are the data frame to be converted and the columns that contain the geographic data. This second argument uses the c() function to combine the “lon” and “lat” columns. The third argument determines the coordinate reference system (crs) for the data. Here, I indicate that I want the longitude and latitude to be plotted using the World Geographic System 1984 projection, which is referenced as European Petroleum Survey Group (EPSG) 4326. Geographic jargon aside, what matters at this stage is that EPSG 4326 is the projection used by web maps such as Google Maps.8\nlocations_sf &lt;- st_as_sf(locations, coords = c(\"lon\", \"lat\"), crs = 4326)\nWith the data in the correct format, a simple call to the mapview() function creates an interactive map with all the locations plotted as points. You can click on a point to see its name and compare it to the locations on the map. With this data, the accuracy of the location only needs to be at the city level. The location within the city is not relevant. Inspecting the data from the map shows that all of the locations were correctly geocoded.\nmapview(locations_sf)\n\n\nI can now save the locations data using the readr package and a function similar to that used to load data. I will use the write_csv() function to save the data as a csv file. Here, I save the locations tibble, but you could also save the other forms of the locations data. The second argument tells the function where to save the csv and what to call the file. Here, I place the file in the same folder as the “correspondence-data-1585.csv” and name the file “locations.csv”.\nwrite_csv(locations, \"data/locations.csv\")"
  },
  {
    "objectID": "post/geocoding-with-r/index.html#mapping",
    "href": "post/geocoding-with-r/index.html#mapping",
    "title": "Geocoding with R",
    "section": "Mapping with ggmap",
    "text": "Mapping with ggmap\nNow that we have successfully geocoded the locations from which Daniel’s correspondents sent letters and in which Daniel received them, we can move on to the task of plotting the locations on a map with ggmap.9 ggmap is essentially an extension of ggplot. It enables you to plot a map as the background of a ggplot graph. The two main plotting features of ggmap are the get_map() function, which downloads a map from a specified map provider, and the ggmap() function that plots the downloaded map on a ggplot plot. Once a map is downloaded and plotted, it is then possible to use the normal grammar of gglot to visually represent that data on the map.\nThe get_map() function can access data from three different map providers: Google Maps, Open Street Maps, and Stamen Maps. In this example, I will use Google Maps and the get_googlemap() function. The challenge with the get_map() function is downloading a map with a zoom level and location that shows all of the data with minimal extra space. We need two pieces of information to do this for get_googlemap(): a location for the center of the map and a zoom level between 1 and 21.10 Figuring out the best center and zoom level for a map may take some trial and error. However, the work that we have already done can help to make an educated first guess. We can rerun mapview(locations_sf) and look at the details provided by the map. The map produced by mapview() shows the longitude and latitude at your cursor position and tells the zoom level of the map. We can use the cursor to guess a good center of the map or zoom in to find a city, which we can then geocode. After a couple of tries, I found that Mannheim, Germany works as a good center for a map with a zoom level of 6. I can get the coordinates of Mannheim with the generic geocode() function and then place the coordinates into the get_googlemap() function. The only alteration that needs to be made is to glue together the longitude and latitude values into one object with the concatenate function, c().\ngeocode(\"mannheim\")\n#&gt;        lon      lat\n#&gt; 1 8.466039 49.48746\n\nmap &lt;- get_googlemap(center = c(8.4, 49.5), zoom = 6)\nWe can look at what the map looks like by calling the ggmap() function.\nggmap(map)\n\n\n\n\n\n\n\n\n\nGiven the historical nature of the data, some of the normal features of a Google Map are problematic. The modern road system obviously did not exist in the sixteenth century, nor are the modern political boundaries useful for the data. In the below command, I change the aesthetics of the original map using the color argument and turn off features of the map by using commands from the Google Maps API.\nbw_map &lt;- get_googlemap(center = c(8.4, 49.5), zoom = 6,\n  color = \"bw\",\n  style = \"feature:road|visibility:off&style=element:labels|visibility:off&style=feature:administrative|visibility:off\")\nNow let’s see what the map looks like, but this time let’s add the location data. This is done using the normal ggplot functions. Here, I want to add points for each place in the locations data that we created above. The aes() function within geom_point() tells ggplot that the x-axis corresponds to the longitude and the y-axis to the latitude of each place.\nggmap(bw_map) +\n  geom_point(data = locations, aes(x = lon, y = lat))\n\n\n\n\n\n\n\n\n\nThe resulting map is rather sparse and does not provide much information, but it gives a good starting point from which to build a more informative map."
  },
  {
    "objectID": "post/geocoding-with-r/index.html#adding-data",
    "href": "post/geocoding-with-r/index.html#adding-data",
    "title": "Geocoding with R",
    "section": "Adding data with dplyr",
    "text": "Adding data with dplyr\nThe above map used the locations data to plot Daniel van der Meulen’s correspondence in 1585, but because it did not use the letters data, the map could not tell us anything about the number of letters sent from or received in each location. Visualizing this information is a two-step process. Firstly, it is necessary to find out how many letters were sent from and received in each location. Because we need to distinguish between sent and received locations, this data will be contained in two data frames. To do this, we can reuse the per_source and per_destination code discussed in the previous post, which summarizes the amount of letters sent from and to each location. Secondly, we have to join the longitude and latitude information from the locations data frame to the per_source and per_destination data frames.\nAs a reminder, to create the per_source and per_destination objects I will use the group_by() and summarise() workflow. This groups the data by one or more defined variables and creates a new column that counts the number of unique observations from the variable(s). Below, I make per_source and per_destination data frames and print out the per_destination object to show its form.\nper_source &lt;- letters %&gt;% \n  group_by(source) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count))\n\nper_destination &lt;- letters %&gt;% \n  group_by(destination) %&gt;% \n  summarise(count = n()) %&gt;% \n  arrange(desc(count))\n\nper_destination\n#&gt; # A tibble: 5 x 2\n#&gt;   destination count\n#&gt;         &lt;chr&gt; &lt;int&gt;\n#&gt; 1       Delft    95\n#&gt; 2     Haarlem     8\n#&gt; 3      Bremen     6\n#&gt; 4   The Hague     3\n#&gt; 5  Middelburg     2\nThe per_source and per_destination data frames are in the basic structure that we want, but we need to add longitude and latitude columns to both of the objects so that they can be plotted on our map. Here, I will use a left_join(). As noted above, a left_join() only keeps the observations from the first data frame in the function. In other words, the result of a left_join() will have the same number of rows as the original left data frame, while adding the longitude and latitude columns from the locations data frame. The data frames will be joined by the columns containing the name of the cities. Because these “key” columns have different names, it is again necessary to denote their equivalency with c(). Below, I print out the newly created geo_per_destination data frame to show its structure.\ngeo_per_source &lt;- left_join(per_source, locations, by = c(\"source\" = \"place\"))\ngeo_per_destination &lt;- left_join(per_destination, locations, by = c(\"destination\" = \"place\"))\n\ngeo_per_destination\n#&gt; # A tibble: 5 x 4\n#&gt;   destination count      lon      lat\n#&gt;         &lt;chr&gt; &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1       Delft    95 4.357068 52.01158\n#&gt; 2     Haarlem     8 4.646219 52.38739\n#&gt; 3      Bremen     6 8.801694 53.07930\n#&gt; 4   The Hague     3 4.300700 52.07050\n#&gt; 5  Middelburg     2 3.610998 51.49880\nI now have the necessary data to create a map that will distinguish between the sources of the letters and their destinations and will allow me to show the quantities of letters sent from and received in each location."
  },
  {
    "objectID": "post/geocoding-with-r/index.html#mapping-data",
    "href": "post/geocoding-with-r/index.html#mapping-data",
    "title": "Geocoding with R",
    "section": "Mapping the data",
    "text": "Mapping the data\nCreating a quality visualization with ggplot involves iteration. Because the different parts of the plot are all written out in code, aspects can be added, subtracted, or modified until a good balance is found. Let’s start by creating a basic map using the geo_per_source and geo_per_destination data. The structure of the command is similar to that used to make the first map, but now that I am using information from two data frames, I need to use two geom_point() functions. There is also a small change to the ggmap() function to have the map take up the entire plotting area so that the longitude and latitude scales do not show. The only change to the geom_point() function is the addition of different colors for the two sets of points. This makes it easier to distinguish between places from which Daniel’s correspondents sent letters and places where he received them. Notice that the argument for the color of the points is placed outside of the aes() function. This makes all of the points plotted from each data frame a single color as opposed to mapping a change in color to a variable within the data. In this instance, I chose to specify color by name, but it is also possible to use rgb values, hex values, or a number of different color palettes.11\nggmap(bw_map) +\n  geom_point(data = geo_per_destination,\n             aes(x = lon, y = lat), color = \"red\") +\n  geom_point(data = geo_per_source,\n             aes(x = lon, y = lat), color = \"purple\")\n\n\n\n\n\n\n\n\n\nThis plot is much better than what we started with, but it still has a couple of issues. In the first place, it does not communicate any information about the quantity of letters. In addition, because the points are opaque, it is not clear that letters were both sent from and to Haarlem. The former issue can be rectified by using the size argument within the aes() function. This will tell ggplot to vary the size of each of the points in proportion to the count column. By default, the size aesthetic creates a legend to indicate the scale used. In the ggmap() function I place the legend in the top right corner of the map since there are no data points there. The latter issue is solved by adding an alpha argument to the two geom_point() functions. This argument is placed outside of the aes() function, because it is an aspect we want to apply to all points. Alpha describes the translucency of an object and takes values between 1 (opaque) and 0 (translucent).\nggmap(bw_map) +\n  geom_point(data = geo_per_destination,\n             aes(x = lon, y = lat, size = count),\n             color = \"red\", alpha = 0.5) +\n  geom_point(data = geo_per_source,\n             aes(x = lon, y = lat, size = count),\n             color = \"purple\", alpha = 0.5)\n\n\n\n\n\n\n\n\n\nThe above map is more informative, but it is hardly a finished product. For instance, there is no explanation for the differences in the color of the points, the smallest points are not easy to see, and there are no labels to indicate the names of the cities. Let’s deal with these issues one at a time to create a more fully fleshed out map. This will serve as an opportunity to demonstrate both the flexibility and complexity of ggplot code.12\nThe different colors for sent and received locations are not defined in a legend in the previous plot, because ggplot only creates a legend for arguments within an aes() function. Even though the color does not change within the data for each geom_point() function, it is possible to place the color in the aes() function when used in tandem with scale_color_manual(). Inside the aes() function the color is associated with a name that will appear in the legend. The actual color to be used is defined in a separate scale_color_manual() function with the values argument.13\nA similar type of scale function also makes it possible to manually control the minimum and maximum size of the points drawn by the size = count argument within the aes() functions. For this last plot, I decided to make the minimum size of the point 2 and the maximum size 9. The range of the sizes for points is a good example of a plot element that you can play around with, trying out different sizes until you find one that works. Note too, that the best sizes of plot elements will also depend on the format in which the finished plot will be presented.\nIn changing the background map from the default Google Map, I took out the city labels, but this makes it difficult to know which cities are represented by the different points. The plot only contains fourteen points, making it possible to label each point without too much clutter. In ggplot, labels are geoms; they are distinct elements placed on a plot with separate geom functions. Labeling points can be done with either geom_text() or geom_label(). geom_text() places text at the indicated position, while geom_label() places the text within a white text box. In addition to x and y coordinates, the two geoms require a label argument, which indicates the variable that should be used for the text. By default geom_text() and geom_label() are placed exactly on the x and y coordinates given by the data. It is possible to nudge the placement of the labels with the nudge_x and nudge_y arguments. However, instead of using these, I will take advantage of the geom_text_repel() function from the ggrepel package. This package automatically chooses the placement of individual labels to ensure that they do not overlap. Note too that I use the locations data frame for the data of the geom, since locations contains the longitude and latitude of each point on the map.\nThe final touches to the map can be made by ensuring that all of the elements are clearly noted. Labels for the plot itself can be made with the labs() function. For this plot, I will add a descriptive title and change the labels for the two legends. By default, ggplot uses the name of the indicated column as the label for the legend. This is shown in the above map, where the size of the point is labeled as “count.” The default label can be replaced with a more informative one by indicating the aesthetic to be changed. Here, I will rename the size aesthetic as “Letters.” In addition, I chose not to have a label for the color aesthetic, which is indicated by “NULL”. Finally, I altered the size of the points drawn in the color legend to a larger size. This is done with the guides() function, which changes the scales of different aspects of the plot. In this case, I use the override.aes argument to have the red and purple points in the legend be drawn at size = 6.\nlibrary(ggrepel)\n\nggmap(bw_map) +\n  geom_point(data = geo_per_destination,\n             aes(x = lon, y = lat, size = count, color = \"Destination\"), \n             alpha = 0.5) +\n  geom_point(data = geo_per_source,\n             aes(x = lon, y = lat, size = count, color = \"Source\"),\n             alpha = 0.5) +\n  scale_color_manual(values = c(Destination = \"red\", Source = \"purple\")) +\n  scale_size_continuous(range = c(2, 9)) +\n  geom_text_repel(data = locations, aes(x = lon, y = lat, label = place)) +\n  labs(title = \"Correspondence of Daniel van der Meulen, 1585\",\n       size = \"Letters\",\n       color = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 6)))"
  },
  {
    "objectID": "post/geocoding-with-r/index.html#concluding",
    "href": "post/geocoding-with-r/index.html#concluding",
    "title": "Geocoding with R",
    "section": "Conclusion",
    "text": "Conclusion\nThe above map demonstrates the geographic spread of Daniel van der Meulen’s correspondence in 1585 and shows the relative significance of the location of his correspondents and the different places in which he received letters throughout the year. More important than the details of the map for the purposes of this post is the process by which it was created. One interesting feature that I would like to emphasize in concluding is that the map uses data from three different data frames or tables: geo_per_destination, geo_per_source, and locations. All three of these data frames derive from the original letters data, while they in turn were the product of yet more data frames. By my count, running the commands contained in this post leads to the creation of 12 different data frame like objects. The below diagram outlines the workflow. The ability to split, subset, transform, and then join newly created tables in a variety of ways is a very powerful and flexible workflow. Because the data frames can be recreated by running the code, there is minimal overhead in managing them, especially in comparison to creating tables within spreadsheets. In this case, I would only recommend that the locations data frame be saved for easy access in other R scripts and sessions. The other objects can be created on demand, or even more can be added, while the individual aspects of the map can be endlessly tweaked using the power of ggplot."
  },
  {
    "objectID": "post/geocoding-with-r/index.html#footnotes",
    "href": "post/geocoding-with-r/index.html#footnotes",
    "title": "Geocoding with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGeocoding can be done with websites such as geonames and the service provided by Texas A&M. There are also a number of options for creating maps on the web.↩︎\nSee the Introduction of Robin Lovelace, Jakub Nowosad, and Jannes Meunchow, Geocomputation with R for a good overview of the GIS capabilities of R.↩︎\nAlex David Singleton, Seth Spielman, and Chris Brunsdon, “Establishing a Framework for Open Geographic Information Science,” International Journal of Geographical Information Science, 30 (2016): 1507-1521.↩︎\nA full list of geographic packages for R can be found at R Packages for the Analysis of Spatial Data. In future posts I will discuss some of these packages and their capabilities.↩︎\nIn this command the quotations around “source” and “destination” are necessary. They tell the function that these are strings of characters.↩︎\nIt is good practice to stay away from overwriting objects until you know whether the code works. However, if you mess up, it is always possible to recreate the original object by rerunning the code that created it.↩︎\nFor instance, geocoding “Naples” will return the longitude and latitude of Naples, Florida and not Naples, Italy.↩︎\nIf you want to get an idea of how an sf object differs from a normal tibble or data frame, you can print out the locations_sf object. This shows that the longitude and latitude columns have been combined into a special type of column called simple_feature and named “geometry”, while additional information about the CRS is also provided.↩︎\nFor a fuller discussion of ggmap, see David Kahle and Hadley Wickham, “ggmap: Spatial Visualization with ggplot2.”↩︎\nA zoom level of 1 is the most zoomed out, while 21 is the most zoomed in.↩︎\nW3 Schools has a good discussion on the use of rgb and hex colors. For the use of colors with ggplot, see Hadley Wickham, ggplot2: Elegant Graphics for Data Analysis, Second edition (Springer, 2016), 133–145. A full list of the named colors available in R can be found here↩︎\nFor a full discussion of the powers of ggplot, see Wickham, ggplot2.↩︎\nWickham, ggplot, 142–143.↩︎"
  },
  {
    "objectID": "post/moving-to-quarto/index.html",
    "href": "post/moving-to-quarto/index.html",
    "title": "Moving to Quarto",
    "section": "",
    "text": "It has been a while since I have posted anything new to this site. I had a pretty good pace of posting going for a while and then things got busy, a pandemic occurred, and I moved a couple of times, and somehow it has now been four years since I have made a new post. Every couple of months I would get the itch to write something new, but I just never got around to doing it. But now I hoping to make a concerted effort to revitalize the blog. There are two main reasons for doing so now: an interest in trying out Quarto and a desire to again participate more actively in the digital humanities, data science, and rstats communities.\nWhen I first began this blog in 2017, I had the goal of trying to learn as many digital skills as possible. I was in the middle of learning R, but I also wanted to gain some basic understanding of the workings of websites. There were some options like blogdown that would have combined my newfound interest in R and my desire to create a website, but I was not sure I wanted to fully commit to the R ecosystem. And I figured I could learn more by going an alternate route. I eventually settled on Hugo, which has served me well, but now I am ready to make the switch to Quarto.\nWhy Quarto? For one, I just want to try out something new. It’s exciting to learn a new system, even if Quarto is not so substantially different from RMarkdown. I also want to simplify the process of writing posts with R code and getting them up on the blog. With Hugo, I had to convert the RMarkdown files to markdown, which added time and complexity. I also have never felt totally comfortable in my understanding of how Hugo works. In practice this has rarely been a hindrance, but I am not sure that I have much more to gain by running a Hugo site unless I want to do a deeper dive on Go and how Hugo templates work. In other words, the usefulness of Hugo had mostly run its course for me, and the structure of the website was more of a hindrance to writing than an impetus to write.\nAt the same time, both the introduction of Quarto and my own path in data science and digital humanities make moving away from Hugo much easier of a choice even taking into account my desire to learn by doing. I am much more committed to the larger rstats community than I was in the summer of 2017 as an R novice. I have no qualms fully investing in the technologies provided by RStudio, now Posit. Moreover, Quarto fixes any issues I might have had with blogdown or Distill by no longer depending on R and having a command line interface. This, combined with the ability to learn more about a tool that can be used to produce academic articles, books, and presentations, as well as websites and blogs, made the decision a rather easy one.\nI have had a lot of fun moving the website over to Quarto and redesigning the site. The Quarto guides have been really helpful throughout the whole process. I was very inspired by Isabella Velásquez’s Building a blog with Quarto tutorial and her Quarto website Pipe Dreams. I also found Danielle Navarro’s post Porting a distill blog to quarto very helpful even though I was moving over from Hugo.\nI am sure that I will continue to tweak and tinker with this site, but so far I am very happy with Quarto and how the site looks. Hopefully this move will push me to write some of the blog posts I have had in my head for quite a while. With the seeming demise, or at least instability of Twitter, it seems more necessary than ever to have a place of one’s own on the internet. This is mine."
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html",
    "href": "post/great-circles-sp-sf/index.html",
    "title": "Great Circles with R",
    "section": "",
    "text": "In 1569 the Flemish cartographer and mathematician Gerardus Mercator published a new world map under the title “New and more complete representation of the terrestrial globe properly adapted for use in navigation.” The title of the map points to Mercator’s main claim for its usefulness, which he expounded upon in the map’s legends. Mercator presented his map as not only an accurate representation of the known world, but also as a particularly useful map for the purposes of navigation. As described in the third legend, Mercator aimed to maintain conformity to the shape of land masses even towards the poles and to have straight lines on the map accurately represent directionality. To achieve his goals Mercator used a projection in which lines of longitude and latitude were made perpendicular at all values by increasing the distance between degrees of latitude as they reach the pole.1 Mercator’s projection had the benefit that straight lines drawn on the map are rhumb lines, lines of constant bearing that pass every degree of longitude at the same angle. Theoretically this simplified oceanic navigation; a ship captain could draw a straight line from one port to another, calculate the bearing, and maintain that bearing along the voyage. However, 16th-century navigators used magnetic courses and not longitude and latitude values as Mercator’s map assumed.2 An accurate means to measure longitude at sea was only discovered in the second half of the 18th century with the development of the sextant and later the marine chronometer.3\nThe Mercator projection was designed with certain uses in mind. Mercator’s emphasis on perpendicular lines of longitude and latitude and the equivalence of straight lines and rhumb lines were meant to simplify navigation and have recently proved useful for online mapping services. However, the stretching of latitudes towards the poles distorts the size of land masses, making those closer to the poles appear larger than those near the equator. The stress on rhumb lines in Mercator’s map also highlights the difference between lines of constant bearing (rhumb or loxodrome lines) and the shortest distance between two points (great circles). Due to Earth’s ellipsoidal nature, the shortest distance between two points is not necessarily a straight line. For instance, to fly from Los Angeles to Amsterdam, one would not want to fly in a straight line of constant bearing at 78 degrees. Instead, you would want to make an arc to the north to take advantage of the ellipsoidal shape of the Earth. By flying along the great circle from Los Angeles to Amsterdam one would travel 1120 kilometers less than flying along the rhumb line."
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#great-circles-with-r",
    "href": "post/great-circles-sp-sf/index.html#great-circles-with-r",
    "title": "Great Circles with R",
    "section": "Great circles with R",
    "text": "Great circles with R\nIn Geocoding with R and Introduction to GIS with R I demonstrated different ways to make maps that showed the sources and destinations of letters sent to the sixteenth-century merchant Daniel van der Meulen in 1585. This post will show how to create great circle lines to connect the sources and destinations of the letters. There are a number of resources on making great circles with R, and each resource uses slightly different methods.4 The diversity of methods to get the same or similar result is both one of the more interesting and one of the more frustrating parts of coding. Here, I will show how to create great circles as a SpatialLinesDataFrame from the sp package, as well as two different methods using the sf package. The results of the three methods will be very similar, and the ability to convert between objects used by the sp and sf packages means that they can be used interchangeably. However, showing multiple methods will help to explicate the process of creating lines from longitude and latitude data and turning them into great circles. You can find the data and the R script that goes along with this post on GitHub.\nsp and sf both depend upon external packages to calculate the shortest distance between two points of longitude and latitude data. Creating a SpatialLinesDataFrame of great circles is done through the geosphere package, while the calculations for sf objects depend upon the lwgeom package.5 Despite the different sources for the calculations, the results produced by geosphere and lwgeom are all but equivalent. Internally, both the gcIntermediate() function from geosphere and st_segmentize() function from sf use a spherical model of the Earth to calculate great circles. Therefore, the path drawn by the two functions are not as exact as the distance calculations from the two packages, which use more accurate ellipsoidal models of the Earth. The primary difference between gcIntermediate() and st_segmentize() is not the internal calculations, but how they are implemented in R. Both functions work by breaking a straight line with two points into a segmented line, with the number of breaks defined by the user. gcIntermediate() calculates segments based on a specified number of breaks. On the other hand, st_segmentize() uses a maximum distance argument to calculate the number of segments. This means that whereas gcIntermediate() results in the same number of points along the great circle for each line irrespective of distance, st_segmentize() creates segments of relatively equal distance but with the amount of segments in a line varying by distance.\nLet’s start by loading the packages and data that we will use to make the great circles. To wrangle the data into the correct formats I will use the tidyverse. We need to load the sp and geosphere packages to make a SpatialLinesDataFrame of great circles and the sf package for the latter two methods. Internally, st_segmentize() uses the lwgeom package, but we will not use the package directly, and so while it is necessary to have lwgeom installed, we do not need to load it. Lastly, I will use the rnaturalearth package to provide some background maps. The data that I will be using is again the letters sent to Daniel van der Meulen in 1585 along with the latitude and longitude values of the cities within his correspondence network at this time. We can also load the countries map from rnaturalearth as both a Spatial and sf object.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(sp)\nlibrary(geosphere)\nlibrary(sf)\nlibrary(rnaturalearth)\n\n# Load the data\nletters &lt;- read_csv(\"data/correspondence-data-1585.csv\")\nlocations &lt;- read_csv(\"data/locations.csv\")\n\n# Load the background maps\ncountries_sp &lt;- ne_countries(scale = \"medium\")\ncountries_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")"
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#preparing-the-data",
    "href": "post/great-circles-sp-sf/index.html#preparing-the-data",
    "title": "Great Circles with R",
    "section": "Preparing the data",
    "text": "Preparing the data\nThe basis for all three methods of creating great circles is a data frame of routes — a data frame with a single row for each unique connection between a source and a destination — which we can then join to the longitude and latitude values in the locations data frame. The letters data contains 114 rows with each row containing a letter. We can calculate the number of letters sent along each unique route with a group_by() and count() pipeline in which we group the letters that have the same source and destination. We then want to ungroup() the data frame to prevent the group attribute from interfering with any code further down the line. The final step is to arrange the rows by the count column, which has been named “n” by count(). This ensures that the routes with more letters sent across them will be drawn later in the plot, making them more visible in the maps.\n\n# Create data frame of routes and count for letters per route\nroutes &lt;- letters %&gt;%\n  group_by(source, destination) %&gt;% \n  count() %&gt;% \n  ungroup() %&gt;% \n  arrange(n)\n\n# Print routes\nroutes\n#&gt; # A tibble: 15 × 3\n#&gt;    source    destination     n\n#&gt;    &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n#&gt;  1 Amsterdam Bremen          1\n#&gt;  2 Antwerp   Middelburg      1\n#&gt;  3 Dordrecht Haarlem         1\n#&gt;  4 Emden     Bremen          1\n#&gt;  5 Haarlem   Middelburg      1\n#&gt;  6 Haarlem   The Hague       1\n#&gt;  7 Hamburg   Bremen          1\n#&gt;  8 Het Vlie  Bremen          1\n#&gt;  9 Lisse     Delft           1\n#&gt; 10 Antwerp   The Hague       2\n#&gt; 11 Haarlem   Bremen          2\n#&gt; 12 Venice    Haarlem         2\n#&gt; 13 Antwerp   Haarlem         5\n#&gt; 14 Haarlem   Delft          26\n#&gt; 15 Antwerp   Delft          68\n\nThe routes data shows that there are 15 unique source to destination combinations within the letters data. A glimpse at routes shows that the majority of letters that Daniel received in 1585 — 68 letters — were sent from Antwerp to Delft, while a sizable number of letters were sent from Haarlem to Delft.6 Daniel received letters along other routes much less frequently. With the routes identified, and already possessing the longitude and latitude of the end points for the routes in the locations data frame, we can now move on to creating the great circles between these sets of points."
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#great-circles-with-geosphere-and-sp",
    "href": "post/great-circles-sp-sf/index.html#great-circles-with-geosphere-and-sp",
    "title": "Great Circles with R",
    "section": "Great circles with geosphere and sp\n",
    "text": "Great circles with geosphere and sp\n\nLet’s start by creating great circles as a SpatialLinesDataFrame from the sp package. Before the recent growth of the sf package, the gcIntermediate() function from the geosphere package represented the main method for creating great circles. I have tried to simplify the process, making it possible to create a SpatialLinesDataFrame in four steps.\nThe gcIntermediate() function provides one of the easiest methods for transforming coordinates data into spatial lines. A single step creates a SpatialLines object and calculates the great circle. As we will see below, the sf methods split these into two steps. The gcIntermediate() function takes two sets of longitude and latitude coordinates, one for the beginning point of the line and one for the end point. The coordinates can be supplied in the form of either a data frame or a matrix. Here, I will supply the coordinates in a data frame, since our longitude and latitude values are already in this format. We can identify the longitude and latitude values for the sources of the routes by joining locations to routes by the “source” column. Since we only want the longitude and latitude values for the gcIntermediate() function, we select() the “lon” and “lat” columns — the names of the columns that contain longitude and latitude values — creating a sources_tbl with two columns and the same amount of rows as the routes data. Joining locations to routes by “destination” creates an equivalent destinations_tbl.\n\n# tibble of longitude and latitude values of sources\nsources_tbl &lt;- routes %&gt;% \n  left_join(locations, by = c(\"source\" = \"place\")) %&gt;% \n  select(lon, lat)\n\n# tibble of longitude and latitude values of destinations\ndestinations_tbl &lt;- routes %&gt;% \n  left_join(locations, by = c(\"destination\" = \"place\")) %&gt;% \n  select(lon, lat)\n\nThe next step is to decide on the number of segments for the line and the type of output we want. In this example, I choose 50 breaks, which is more than enough for the routes that we are drawing. The end result will actually have 52 segments, because I want to include the start and end points within the line by setting the addStartEnd to TRUE.7 Lastly, in order to have the output be a Spatial object, we need to set the sp argument to TRUE.8\n\n# Great circles as a SpatialLines object\nroutes_sl &lt;- gcIntermediate(sources_tbl, destinations_tbl, \n                            n = 50, addStartEnd = TRUE, \n                            sp = TRUE)\n\nLet’s investigate the object that we have created with the gcIntermediate() function. You will not want to print routes_sl to the console, as this will print out fifteen matrices with 52 rows each, but we can use other methods to look at the object. routes_sl is a Spatial object of class SpatialLines. The different forms of data within Spatial objects are contained in slots, which can be identified with slotNames(). The lines slot contains the coordinates of the routes produced by gcIntermediate(). The gcIntermediate() function also added a coordinate reference system and set it to longitude and latitude coordinates on the WGS84 ellipsoid, equivalent to EPSG 4326.\n\n# Class of routes_sl\nclass(routes_sl)\n#&gt; [1] \"SpatialLines\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"sp\"\n\n# Slots in routes_sl\nslotNames(routes_sl)\n#&gt; [1] \"lines\"       \"bbox\"        \"proj4string\"\n\n# CRS of routes_sl\nroutes_sl@proj4string\n#&gt; Coordinate Reference System:\n#&gt; Deprecated Proj.4 representation: +proj=longlat +ellps=WGS84 +no_defs \n#&gt; WKT2 2019 representation:\n#&gt; GEOGCRS[\"unknown\",\n#&gt;     DATUM[\"Unknown based on WGS84 ellipsoid\",\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1],\n#&gt;             ID[\"EPSG\",7030]]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433],\n#&gt;         ID[\"EPSG\",8901]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"longitude\",east,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]],\n#&gt;         AXIS[\"latitude\",north,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]]]\n\nThe inclusion of a coordinate reference system makes routes_sl a truly geospatial object that can be plotted on our background map, though it does not yet possess any attribute data about the names of the locations or the number of letters sent along each route.\n\n# Make bbox of countries_sp the same as routes_sl\ncountries_sp@bbox &lt;- bbox(routes_sl)\n\n# Plot map\npar(mar = c(1, 1, 3, 1))\nplot(countries_sp, col = gray(0.8), border = gray(0.7),\n     main = \"SpatialLines great circles\")\nplot(routes_sl, col = \"dodgerblue\", add = TRUE)\n\n\n\n\n\n\n\nAt this point, the output is not hugely informative. However, we can confirm that the process of making great circles worked by observing the curvatures of the lines. Notice that the amount of curvature differs according to distance and the bearing of the line.\nWe can add the attribute data for the lines and create a SpatialLinesDataFrame object by combining routes_sl and the routes data frame through the SpatialLinesDataFrame() function. There is no need to worry about linking the lines data to the correct row in routes, because routes_sl contains a lines ID attribute that corresponds to the row numbers of sources_tbl and destinations_tbl, which were derived from routes.9\n\n# Great circles as a SpatialLinesDataFrame object\nroutes_sldf &lt;- SpatialLinesDataFrame(routes_sl, data = routes)\n\nThe result of SpatialLinesDataFrame() is a complete Spatial object that contains a CRS, attribute data in the data slot, and great circle lines in the lines slot. It is now possible to plot routes_sldf and distinguish the lines by the number of letters sent along each route. The easiest way to show this in a base plot is by adjusting the line width of the great circles by a chosen formula. In this case, I take the square route of the number of letters and add 0.25 to get the maximum and minimum line width to a reasonable size.\n\ncountries_sp@bbox &lt;- bbox(routes_sldf)\n# Map with SpatialLinesDataFrame object\npar(mar = c(1, 1, 3, 1))\nplot(countries_sp, col = gray(0.8), border = gray(0.7),\n     main = \"SpatialLinesDataFrame great circles\")\nplot(routes_sldf,\n     lwd = sqrt(routes_sldf$n/3) + 0.25,\n     col = \"dodgerblue\",\n     add = TRUE)"
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#great-circles-with-sf",
    "href": "post/great-circles-sp-sf/index.html#great-circles-with-sf",
    "title": "Great Circles with R",
    "section": "Great circles with sf\n",
    "text": "Great circles with sf\n\nThere is no straightforward way, no single function, to create great circles from longitude and latitude data with the sf package that is reminiscent of gcIntermediate(). With sf the creation of great circles is a two-step process. First, we need to create lines or objects with a geometry of LINESTRING, and then it will be possible to convert the straight line into a great circle with st_segmentize(). The first transformation is the trickier of the two. As I have shown previously, you can create an sf object with POINT geometry through the st_as_sf() function, but there is no equivalent function for creating lines. Here, I will show two different methods for creating lines and then great circles with sf. The first method will convert the routes data to an sf object of POINT geometry and take advantage of the sf package’s ability to use dplyr functions to create lines through a group_by() and summarise() pipeline. The second method vectorizes the creation of a single LINESTRING feature through a for loop. The resulting sf object of great circles will be essentially identical.\nGreat circles with sf: tidy method\nWhen I first attempted to create great circles with the sf package, I was not sure how to proceed. In fact, for many months I used the above method from geosphere and then converted the SpatialLinesDataFrame to an sf object with st_as_sf(). I discovered this first method for creating great circles with sf when I came to the realization that the form of the data in the routes data frame was problematic. It obviously makes sense from a perspective of data entry to have a row for each letter with a source and destination. However, this results in two columns that represent a similar type of data of place names. When coordinate data is added, you end up with two sets of longitude and latitude variables. In an sf object this translates to two separate geometry columns, which is possible but problematic.\nOnce I identified the problem, I could find a solution in the form of the functions in the tidyr package and the transformation of routes from a “wide” form with a “source” and “destination” column in the same row to a “long” form in which the source and destination of each letter are in separate rows. The “long” format of the routes data frame with a single “place” column makes it possible to add a single set of longitude and latitude data and then convert to an sf object. From this format we can transform an sf object with POINT geometry to LINESTRING geometry through sf’s integration with dplyr.10\nLet’s see how this works in practice. Before we begin though, we need to make a slight change to routes to add an “id” column. This will enable us to keep track of the relationship between the sources and destinations and will be used later to group the routes together. We can do this with the very helpful rowid_to_column() function from the tibble package, which even places the “id” column at the start of the data frame.\n\n# Create id column to routes\nroutes_id &lt;- rowid_to_column(routes, var = \"id\")\n\nWith this done, the first step is to transform routes_id into a “long” format with the use of gather(). The related gather() and spread() functions are almost magical when you can get them to do what you want, but this also makes them somewhat tricky to use.11 gather() combines variables and transforms them into values. The gather() function has three main arguments: key, value, and ... to choose the columns to keep or modify. The values passed to the key and value arguments provide the names for two new columns to be created by the gather() function. They can be anything you choose, but assigning descriptive names makes it easier to understand what the function is doing. You can think of the key argument as the name for the column whose values are currently column names. The key essentially identifies the “type”. The value argument is the name of the variable whose values are contained within the columns that are spread apart. The ... argument is used to demarcate the columns to either be used by gather() or to specify which columns you want to keep as is and thus make longer. This uses the same grammar as select() with -variable_name for columns that you want to be left alone.\nIn our example, we want to turn the “source” and “destination” variables or columns into values that identify whether a place is a source or destination. Here, I choose to name the key “type” and the value “place”, which aligns with the nomenclature used in the locations data. To make the function work properly, we need to choose the right columns for gather() to work upon. In this case, there are two possibilities. We can either tell gather() to use the “source” and “destination” columns or to leave “id” and “n” alone with a - before these column names. Here, I do the former.12\n\n# Transform routes to long format\nroutes_long &lt;- routes_id %&gt;% \n  gather(key = \"type\", value = \"place\", source, destination)\n\n# Print routes_long\nroutes_long\n#&gt; # A tibble: 30 × 4\n#&gt;       id     n type   place    \n#&gt;    &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    \n#&gt;  1     1     1 source Amsterdam\n#&gt;  2     2     1 source Antwerp  \n#&gt;  3     3     1 source Dordrecht\n#&gt;  4     4     1 source Emden    \n#&gt;  5     5     1 source Haarlem  \n#&gt;  6     6     1 source Haarlem  \n#&gt;  7     7     1 source Hamburg  \n#&gt;  8     8     1 source Het Vlie \n#&gt;  9     9     1 source Lisse    \n#&gt; 10    10     2 source Antwerp  \n#&gt; # … with 20 more rows\n\nBecause we performed the gather() function on two columns, routes_long possesses twice as many rows as routes. routes_long has two new columns corresponding to the names we gave them in the key and values arguments, while the “id” and “n” values have essentially been doubled such that the same values are associated with both a source and destination “type”. Now that we have a single column with place names, we can add the longitude and latitude values of the places through a left join.\n\n# Add coordinate values\nroutes_long_geo &lt;- left_join(routes_long, locations, by = \"place\")\n\nWith routes_long_geo we now have the data in the correct format to create an sf object with POINT geometry and a CRS through the st_as_sf() function. Let’s create the object and then see what it looks like.\n\n# Convert coordinate data to sf object\nroutes_long_sf &lt;- st_as_sf(routes_long_geo,\n                           coords = c(\"lon\", \"lat\"),\n                           crs = 4326)\n\n# Print routes_long_sf\nroutes_long_sf\n#&gt; Simple feature collection with 30 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 30 × 5\n#&gt;       id     n type   place                geometry\n#&gt;  * &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;POINT [°]&gt;\n#&gt;  1     1     1 source Amsterdam (4.895168 52.37022)\n#&gt;  2     2     1 source Antwerp   (4.402464 51.21945)\n#&gt;  3     3     1 source Dordrecht  (4.690093 51.8133)\n#&gt;  4     4     1 source Emden       (7.20601 53.3594)\n#&gt;  5     5     1 source Haarlem   (4.646219 52.38739)\n#&gt;  6     6     1 source Haarlem   (4.646219 52.38739)\n#&gt;  7     7     1 source Hamburg   (9.993682 53.55108)\n#&gt;  8     8     1 source Het Vlie      (5.183333 53.3)\n#&gt;  9     9     1 source Lisse     (4.557483 52.25793)\n#&gt; 10    10     2 source Antwerp   (4.402464 51.21945)\n#&gt; # … with 20 more rows\n\nFrom this point on, we are able to use sf’s dplyr integration to convert from a POINT geometry to MULTIPOINT and then LINESTRING. We can perform the former by grouping routes_long_sf by the “id” column and summarizing the points into a MULTIPOINT geometry.13 The sf implementation of summarise() includes a do.union argument. With do.union set to TRUE, the default, summarise() will try to resolve internal boundaries, but it can also change the order of points. Therefore, we will set do.union to FALSE, which uses st_combine() to more simply combine geometries in a manner similar to c (). The latter transformation from MULTIPOINT to LINESTRING involves st_cast() and takes advantage of the fact that internally MULTIPOINT and LINESTRING are derived from the same type of data, making it possible to convert back and forth between the two geometries without any loss of data. Here I will do all of this in a single pipeline, though you could do it step-by-step to see the multiple conversions that are taking place.\n\n# Convert POINT geometry to MULTIPOINT, then LINESTRING\nroutes_lines &lt;- routes_long_sf %&gt;% \n  group_by(id) %&gt;% \n  summarise(do_union = FALSE) %&gt;% \n  st_cast(\"LINESTRING\")\n\n# Print routes_lines\nroutes_lines\n#&gt; Simple feature collection with 15 features and 1 field\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 15 × 2\n#&gt;       id                               geometry\n#&gt;    &lt;int&gt;                       &lt;LINESTRING [°]&gt;\n#&gt;  1     1  (4.895168 52.37022, 8.801694 53.0793)\n#&gt;  2     2  (4.402464 51.21945, 3.610998 51.4988)\n#&gt;  3     3  (4.690093 51.8133, 4.646219 52.38739)\n#&gt;  4     4    (7.20601 53.3594, 8.801694 53.0793)\n#&gt;  5     5  (4.646219 52.38739, 3.610998 51.4988)\n#&gt;  6     6    (4.646219 52.38739, 4.3007 52.0705)\n#&gt;  7     7  (9.993682 53.55108, 8.801694 53.0793)\n#&gt;  8     8      (5.183333 53.3, 8.801694 53.0793)\n#&gt;  9     9 (4.557483 52.25793, 4.357068 52.01158)\n#&gt; 10    10    (4.402464 51.21945, 4.3007 52.0705)\n#&gt; 11    11  (4.646219 52.38739, 8.801694 53.0793)\n#&gt; 12    12 (12.31552 45.44085, 4.646219 52.38739)\n#&gt; 13    13 (4.402464 51.21945, 4.646219 52.38739)\n#&gt; 14    14 (4.646219 52.38739, 4.357068 52.01158)\n#&gt; 15    15 (4.402464 51.21945, 4.357068 52.01158)\n\nWe have successfully created an sf object with lines. In addition to the change in geometry type, we can see that the number of rows has been halved and returned to the original length of routes_id. However, in doing this, we lost the data on the name of the source and destination of the lines, as well as the number of letters sent along the route. This occurred when we grouped routes_long_sf by the “id” column, leaving other non-geometry columns to be silently dropped. We can re-add the attribute data by joining routes_lines to the original routes_id data frame, using the “id” column to perform the join. This work flow of grouping and then joining by “id” is why we needed to create the column in the first place.\n\n# Join sf object with attributes data\nroutes_lines &lt;- left_join(routes_lines, routes_id, by = \"id\")\n\nNow that the geometry of the sf object is LINESTRING, creating a great circle with st_segmentize() is straight forward. The only decision you have to make is the maximum distance for the length of a segment. The maximum distance argument can either be the number of meters for the maximum segment or an object from the units package, which makes it possible to use a more sensible unit for this argument such as kilometers.14 In this case, I will use the set_units function from the units package and set the maximum length to 20 kilometers.15\n\n# Convert rhumb lines to great circles\nroutes_sf_tidy &lt;- routes_lines %&gt;% \n  st_segmentize(units::set_units(20, km))\n\nWith routes_sf_tidy we have reached our goal, an sf object with great circle lines. We can confirm the changes made by st_segmentize() both visually and by looking at the number of coordinates or points in routes_sf_tidy compared to routes_lines. Where routes_lines has 30 set of coordinates — two for each line — routes_sf_tidy has nrow(st_coordinates(routes_sf_tidy)) as a result of st_segmentize().\n\n# Compare number of points in routes_lines and routes_sf_tidy\nnrow(st_coordinates(routes_lines))\n#&gt; [1] 30\nnrow(st_coordinates(routes_sf_tidy))\n#&gt; [1] 193\n\nWe can visualize the difference between the rhumb lines of routes_lines and the great circles of routes_sf_tidy by plotting them on the same map. Because the lines in this example are relatively short, the differences will be fairly minimal and even imperceptible in some cases. Here, I plot the great circles in black on top of the rhumb lines in magenta to highlight where the differences are visible. The most perceptible difference is the route from Venice to Haarlem. The magenta rhumb lines are also partially visible in the routes to Bremen. Routes over shorter distances such as those between Antwerp and Zeeland or Holland are more similar and the appearance of the magenta rhumb lines are less perceptible.\n\n# Rhumb lines vs great circles\nggplot() +\n  geom_sf(data = countries_sf, fill = gray(0.8), color = gray(0.7)) +\n  geom_sf(data = routes_lines, color = \"magenta\") + \n  geom_sf(data = routes_sf_tidy) + \n  coord_sf(xlim = c(2, 14), ylim = c(45, 54), datum = NA) + \n  ggtitle(\"Rhumb lines vs Great circles\") + \n  theme_minimal()\n\n\n\n\n\n\n\nWe can also compare the output of gcIntermediate and st_segmentize(). The use of a maximum length of a segment by st_segmentize() instead of the number of segments is the main differentiation between st_segmentize() and gcIntermediate(). The exact coordinates along which the great circles are drawn will necessarily differ between the two functions. In practice, the differences are likely to be unimportant, and in a map like the one we are creating here, the differences between the two sets of great circles will be imperceptible. The limited extent of the difference between the two functions can be seen by plotting routes_sldf and routes_sf_tidy on the same interactive map using the mapview package. This makes it possible to zoom in on the lines and see where they do differ, such as the line between Venice and Haarlem. The differences that are visible are on the scale of meters, and this is without optimizing the alignment for the choices of the number of segments and maximum length of a segment.\n\n# Interactive comparison of gcIntermediate and st_segmentize\nlibrary(mapview)\n\nmapview(routes_sldf, color = \"magenta\") + \n  mapview(routes_sf_tidy, color = \"black\")\n\n\n\n\n\nGreat circles with sf: for loop method\nThe above method for creating great circles using sf highlights the integration of sf into the tidyverse, but there are always more than one path to answer a question with code. As I learned more about sf objects, I got interested in more directly creating an sf object with a geometry type of LINESTRING from longitude and latitude data. As with the tidy method, the biggest challenge came from creating LINESTRING features and not in turning rhumb lines into great circles. The method I came up with essentially vectorizes the creation of a single LINESTRING feature through the use of a for loop. This uses a similar step-by-step creation of an sf object — from an sfg, to an sfc object, and then to an sf object — that I have shown in Exploration of Simple Features for R.16\nLet’s start by building a single feature with a geometry of LINESTRING using coordinates from Venice and Haarlem to outline the process. The st_linestring() function takes a matrix of two or more sets of coordinates to create an sfg object with a geometry of class LINESTRING. We can construct the matrix of coordinates by row binding two vectors — created using c() — of the longitude and latitude values for Venice and Haarlem.\n\n# Create a line between Venice and Haarlem\nst_linestring(rbind(c(12.315515, 45.44085), c(4.646219, 52.38739)))\n#&gt; LINESTRING (12.31551 45.44085, 4.646219 52.38739)\n\nIt is this process that we want to repeat for each of the 15 routes found in routes. A first instinct might be to create a matrix of the coordinates of all of the routes, but this results in a single line connecting the coordinates. Instead, what we need to do is create 15 matrices that each contain the coordinates for the source and destination of a single route. In other words, we want to take the longitude and latitude of both the source and destination from the first row of routes, bind them together into a 2x2 matrix, and then repeat the process for the second row and so forth. This is the realm of the for loop.\nBefore getting into the mechanics of the for loop, we have to get the data into a format that is similar to the vectors used to create the line between the coordinates of Venice and Haarlem. A similar workflow to that we used above to prepare the data for the gcIntermediate() function gets us what we are looking for. In this case though, we need matrices of source coordinates and destination coordinates. We could either transform sources_tbl and destinations_tbl into matrices with as.matrix() or rerun the code used to create them and use as.matrix() at the end of the pipeline. Here, I do the latter for greater transparency.\n\n# Matrix of longitude and latitude values of sources\nsources_m &lt;- routes %&gt;% \n  left_join(locations, by = c(\"source\" = \"place\")) %&gt;% \n  select(lon:lat) %&gt;% \n  as.matrix()\n\n# Matrix of longitude and latitude values of destinations\ndestinations_m &lt;- routes %&gt;% \n  left_join(locations, by = c(\"destination\" = \"place\")) %&gt;% \n  select(lon:lat) %&gt;% \n  as.matrix()\n\nNow we can deal with creating a for loop.17 There are three components of a for loop: the creation of an object that will contain the output of the for loop, the sequence along which to run the for loop, and the actual code that will be looped. Let’s break this down one part at a time. The goal is to build an object that contains a 2x2 matrix for each route. We should start out by creating an empty vector of the appropriate form and length to hold the output of the for loop. We can do this by creating a list that has a length equal to the number of rows in the routes data frame with the vector() function: linestrings_sfg &lt;- vector(mode = \"list\", length = nrow(routes)).\nThe second step is to define a sequence along which the function will loop by creating a variable that holds the place in the sequence and defining the sequence itself. The conventional variable used in a for loop is i. The sequence itself will be similar, if not identical, to the formula used to create the output object. In this case, we want a sequence from 1 — the first row — to 15 — the number of unique routes in our data, which we can determine programmatically by 1:nrow(routes). Thus, the sequence of the for loop is for (i in 1:nrow(routes)) with for acting to choose the type of loop and the formula within the parentheses defining the sequence.\nFinally, we are able to construct the code to create the desired output. We already know the inputs, output, and functions to use, which we can summarize as follows: linestrings_sfg &lt;- st_linestring(rbind(sources_m, destinations_m)).\nHowever, we do not want to run this code, as it would create a single sfg object connecting all of the points. Instead, we can get our desired output by using the sequence variable to subset both the output list and input matrices in the code. For each iteration of the loop, we want the output to fill one component of the linestrings_sfg list, which we can access with [[i]]. Concerning the inputs, we want to select one row of coordinates from sources_m and one row of coordinates from destinations_m for each iteration. This is possible with the matrix subsetting method of [row, column]. Thus, we can access the sequence of rows with sources_m[i, ] and destinations_m[i, ].\nPutting the above together, we get a complete for loop that creates linestrings_sfg, a list of 15 sfg objects.\n\n# Create empty list object of length equal to number of routes\nlinestrings_sfg &lt;- vector(mode = \"list\", length = nrow(routes))\n\n# Define sequence and body of loop\nfor (i in 1:nrow(routes)) {\n  linestrings_sfg[[i]] &lt;- st_linestring(rbind(sources_m[i, ],\n                                              destinations_m[i, ]))\n}\n\nWe can check that the for loop worked by looking at the contents of one value from the list with [[]] subsetting.\n\n# geometry of a single route\nlinestrings_sfg[[2]]\n#&gt; LINESTRING (4.402464 51.21945, 3.610998 51.4988)\n\nThe next step is to create an sfc object by transforming the list of sfg objects into a geometry column with st_sfc(). An sfc object is referred to as a geometry set and is itself a list. It is at this step that the lines become geospatial through the addition of a CRS. At this point, we can also convert the rhumb lines to great circles by using st_segmentize() in the same manner as above in the pipeline.\n\n# sfc object of great circles\nlinestrings_sfc &lt;- st_sfc(linestrings_sfg, crs = 4326) %&gt;% \n  st_segmentize(units::set_units(20, km))\n\n# Print linestrings_sfc\nlinestrings_sfc\n#&gt; Geometry set for 15 features \n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; LINESTRING (4.895168 52.37022, 5.135538 52.4182...\n#&gt; LINESTRING (4.402464 51.21945, 4.205503 51.2897...\n#&gt; LINESTRING (4.690093 51.8133, 4.67923 51.95682,...\n#&gt; LINESTRING (7.206009 53.3594, 7.406604 53.32556...\n#&gt; LINESTRING (4.646219 52.38739, 4.514556 52.2768...\n\nlinestrings_sfc is a fully geospatial object. The truncated print out of the coordinates for the first five features indicates that the lines have been segmentized and turned into great circles. The final step is to add the attribute data and convert the sfc object to a sf object with st_sf() and the routes data. The st_sf() function effectively adds linestrings_sfc to routes as a geometry column and converts the whole object to class sf and data.frame. Because we have not done anything to rearrange the rows of routes the data will line up correctly.\n\n# Create sf object from data frame and sfc geometry set\nroutes_sf &lt;- st_sf(routes, geometry = linestrings_sfc)\n\n# Print routes_sf\nroutes_sf\n#&gt; Simple feature collection with 15 features and 3 fields\n#&gt; Geometry type: LINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;       source destination n                       geometry\n#&gt; 1  Amsterdam      Bremen 1 LINESTRING (4.895168 52.370...\n#&gt; 2    Antwerp  Middelburg 1 LINESTRING (4.402464 51.219...\n#&gt; 3  Dordrecht     Haarlem 1 LINESTRING (4.690093 51.813...\n#&gt; 4      Emden      Bremen 1 LINESTRING (7.206009 53.359...\n#&gt; 5    Haarlem  Middelburg 1 LINESTRING (4.646219 52.387...\n#&gt; 6    Haarlem   The Hague 1 LINESTRING (4.646219 52.387...\n#&gt; 7    Hamburg      Bremen 1 LINESTRING (9.993682 53.551...\n#&gt; 8   Het Vlie      Bremen 1 LINESTRING (5.183333 53.3, ...\n#&gt; 9      Lisse       Delft 1 LINESTRING (4.557483 52.257...\n#&gt; 10   Antwerp   The Hague 2 LINESTRING (4.402464 51.219...\n\nThough they were produced using very different methods, routes_sf_tidy and routes_sf are identical objects apart from the presence of an “id” column in routes_sf_tidy.\nNow that we have attribute and spatial data for the great circles of the routes of the letters received by Daniel in 1585, we can create a map that uses color to distinguish the amount of letters sent along each route. Because they are essentially identical, you could use either routes_sf_tidy or routes_sf to create this map. The code to create a ggplot2 map using geom_sf() is very similar to that made above to compare routes_lines and routes_sf_tidy. In this case, though, we want the color of the lines to differ according to the “n” variable. In addition, I change the color palette used for the lines to the viridis palette on a continuous scale, hence the _c at the end of the function. I also label the color legend as “Letters”.\n\n# ggplot2 of great circle routes\nggplot() +\n  geom_sf(data = countries_sf, fill = gray(0.8), color = gray(0.7)) +\n  geom_sf(data = routes_sf, aes( color = n)) + \n  scale_color_viridis_c() +\n  labs(color = \"Letters\", title = \"Great circles with sf\") + \n  coord_sf(xlim = c(2, 14), ylim = c(45, 54), datum = NA) + \n  theme_minimal()"
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#resources",
    "href": "post/great-circles-sp-sf/index.html#resources",
    "title": "Great Circles with R",
    "section": "Resources",
    "text": "Resources\n\nGreat circles with sp\n\n\nFlowing Data – How to map connections with great circles\n\nKyle Walker – Interactive flow visualization in R\nData Planes – Flights at Night: Mapping airline routes on NASA’s night lights images\n\n\n\n\nGreat circles with sf - Martin Hadley – Great circles with sf and leaflet - Juan Mayorga – Mapping the Global Network of Transnational Fisheries"
  },
  {
    "objectID": "post/great-circles-sp-sf/index.html#footnotes",
    "href": "post/great-circles-sp-sf/index.html#footnotes",
    "title": "Great Circles with R",
    "section": "Footnotes",
    "text": "Footnotes\n\nAs Mercator explained in legend 6 the distortion of distance is infinite at the poles such that they cannot actually be represented on the map.↩︎\nJoaquim Alves Gaspar, “Revisiting the Mercator World Map of 1569: an Assessment of Navigational Accuracy,” The Journal of Navigation 69 (2016): 1183–1196.↩︎\nRoy Iliffe, “Science and the Voyages of Discovery,” in The Cambridge History of Science: Eighteenth-Century Science, ed. Roy Porter (Cambridge: Cambridge University Press, 2003).↩︎\nSee the resources section at the end of the post for a list of posts on creating great circles in R.↩︎\nPrevious to version 0.6, sf also used the geosphere package to calculate distance and create great circles with geographic coordinates. The change to use the lwgeom package, which is an implementation of the liblwgeom library, aligns sf with the methods used by PostGIS.↩︎\nThis is hardly surprising. Until the fall of 1585, Daniel’s family mainly resided in Antwerp, and he was serving as a representative to the States General, which was meeting at Delft.↩︎\nYou can also set whether you want the line to be broken at the International Date Line, but the argument is superfluous in this case.↩︎\nSetting sp = FALSE, which is the default, returns a list of matrices containing the latitude and longitude values for the path along the great circle of each route.↩︎\nEven if you set the Match.ID argument to FALSE, the lines and attribute data would be correctly aligned as long as you do not change the order of the routes data frame after creating sources_tbl and destinations_tbl.↩︎\nThis method is similar to the original workflow shown by Martin Hadley in his post on Great circles with sf and leaflet.↩︎\nFor more information on gather() and spread() see Wickham, R for Data Science, Chapter 12 and Joyce Robbins’s excellent tutorial on gather().↩︎\nIt is not necessary to place “type” and “place” in quotations, but doing so more clearly identifies these as strings and not as names of already extant objects.↩︎\nYou do not need to include the geometry column in the group_by() function. The geometry column is sticky and will remain attached to the sf object unless explicitly removed.↩︎\nThe units package is imported along with sf, so if you have sf, you should also have the units package.↩︎\nThe equivalent code without the units package would be st_segmentize(routes_lines, 20000).↩︎\nSee the same post for a more in depth treatment of the vocabulary of simple feature objects.↩︎\nIf you are familiar with for loops in R, you can skip this discussion of the steps to build the for loop.↩︎"
  },
  {
    "objectID": "course/ancient-history.html",
    "href": "course/ancient-history.html",
    "title": "Ancient History",
    "section": "",
    "text": "History 1024: Ancient History introduces students to the civilizations and peoples of the western ancient world, incorporating the peoples of the Mediterranean, the Near East, and outlying regions from the development of cities and invention of writing in Mesopotamia around 3,000 BCE to the fall of the western Roman Empire in the 5th century CE. The course examines the development and interactions of cultures and empires centered on the Mediterranean and radiating out across the Afro-Eurasian landmasses. Through lectures, textbook and primary sources readings, and online exhibits and resources, this class will focus on political, social, and cultural concepts such as political legitimacy, identity, and the development of social hierarchies. At all points emphasis will be placed on historical thinking and trying to better understand past cultures rather than memorization of facts, figures, and dates.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/european-empires.html",
    "href": "course/european-empires.html",
    "title": "European Empires, Exploration, and Exchange since 1500",
    "section": "",
    "text": "History 1200 examines the dynamic transformation of Europe and Europe’s interactions with the world across four centuries of social, cultural, political, and economic change. The course begins with Europe on the precipice of the Age of Discovery, a remarkable period in which the European world expanded beyond the confines of the Mediterranean to encompass direct contact with the riches of the Indian Ocean and the discovery of — to the Europeans — the New World of the Americas. The first half of the course explores the consequences of the Age of Discovery for Europeans and the peoples they encountered. The second half of the course takes on the question of the Great Divergence, or how Europe came to be politically and economically dominant on the world stage by the early twentieth century, investigating the ways industrialization and revolutions across the Atlantic world fundamentally altered and disrupted political and social structures in Europe and across the globe.\nThe course will focus on developments that highlight momentous changes in European society brought on by European interactions in the world: the significance of inclusion and exclusion in the development of cultural and political power, the transition from commercial to industrial capitalism, and the gradual and incomplete movement from a corporate hierarchical society to one based on the rights of autonomous individuals. Throughout the course we will draw attention to social and cultural changes, investigating the lives of common women and men as much as kings and the nobility. At all points emphasis will be placed on historical thinking and trying to better understand past cultures rather than memorization of facts, figures, and dates.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/virtue-and-commerce-s16.html",
    "href": "course/virtue-and-commerce-s16.html",
    "title": "Virtue and Commerce: Republicanism and the Development of the Global Economy",
    "section": "",
    "text": "The financial crash of 2008 brought questions of virtue and commerce to the forefront of the American consciousness as people grappled with the causes of the financial collapse. This course examines the history of virtue in the context of the expanding global economy from the Renaissance to the early twentieth century. The Italian Renaissance was made possible by both an expansion of industry, banking, and trade and a rediscovery of ancient Greek and Latin texts. As much as the economic expansion might be seen to coincide with the flourishing intellectual and artistic accomplishments of the period, many Renaissance thinkers perceived commerce to be in tension with the ancient conception of republican virtue. The course follows the growth of the European economy from the Italian Peninsula in the sixteenth century to the Dutch Republic in the seventeenth century, the development of colonial and worldwide economies in the eighteenth century, and the process of industrialization in the nineteenth century. We track the changing understandings of virtue and commerce through the writings of leading theorists of the day, including James Harrington, Bernard Mandeville, Montesquieu, Jean-Jacques Rousseau, and Max Weber.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/western-civ-s16.html",
    "href": "course/western-civ-s16.html",
    "title": "Introduction to Western Civilization: Circa A.D. 843 to circa 1715",
    "section": "",
    "text": "This second in the series of three courses on Western Civilization covers a period of almost 1,000 years from the disintegration of the Carolingian Empire, created during the reign of Charlemagne, to the death of Louis XIV, the Sun King and builder of the magnificent palace of Versailles. It was during this period — from 843 to 1715 — that the concept of Europe developed and the structures of modern Western society were put in motion. The course covers the main social, political, cultural, and economic trends of medieval and early modern Europe, while also introducing students to forms of historical thinking. The course is divided into two parts. The first part covers medieval Europe, which came to a close with the flourishing of the Renaissance and the discovery of the New World. The second part of the course discusses the early modern world created by the Age of Discovery, the Reformation, and the Scientific Revolution in which the antecedents of modern Europe were constructed.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/renaissance-s21.html",
    "href": "course/renaissance-s21.html",
    "title": "The Italian Renaissance",
    "section": "",
    "text": "History 3334 introduces students to one of the most distinctive time periods in European history, the Italian Renaissance. The course concentrates on the Italian peninsula from the rise of urban communes in the 12th century to Spanish and Imperial political dominance in the 16th century. The course will investigate a variety of actions, ideas, and beliefs through primary sources and examine a number of ways that historians have attempted to understand this exciting time period. The course is divided into five sections. The first section discusses late medieval Italy. It discusses the rise of Italian cities and trade in the Mediterranean from the 11th century to the demographic crisis caused by the outbreak of the Black Death in 1347 and the reoccurrences of the plague in the second half of the 14th century. The second section of the couse focuses on the quattrocento, highlighting the urban nature of the Renaissance. The third section of the course moves to questions about the state in the Renaissance and is centered around the reading of Machiavelli’s The Prince and selections from The Discourses. The fourth section investigates the changing world from the late 15th century as European exploration expanded and learning increasingly turned from the reading of ancient texts to the study of nature. The final section discusses the waning of the Renaissance and takes on the question of periodization and defining the Renaissance.\nThroughout the five sections primary sources, secondary literature, and the lectures will focus on questions of the urban nature of the Renaissance, the increased interest in the recovery of the culture of ancient Rome, and the significance of debates over social, political, and cultural legitimacy.\nThis course was taught in Spring 2021 during the Covid-19 pandemic and was conducted remotely. Lectures were delivered by video and there was a 50-minute discussion each week over Zoom.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/modern-world-history.html",
    "href": "course/modern-world-history.html",
    "title": "Modern World History",
    "section": "",
    "text": "History 1050 begins at the dawn of the truly global world. When Christopher Columbus reached the Americas in 1492 the world’s cultures became connected in a way that changed the lives of people on all inhabited continents. Beginning in the sixteenth century, people in the Americas, Europe, Africa, Asia, and Oceania became increasingly interconnected and dependent upon each other. Through expanding empires and developing global trade networks, individuals across the world came into contact with new kinds of people, ideas, beliefs, animals, plants, food, and manufactured goods. This course will discuss these early interactions and the consequences of this interconnected world up to the early twenty-first century through a world historical lens that will focus on the experience of non-Western peoples. Through both secondary literature and primary sources, the course will investigate themes such as the development of capitalism, the process of industrialization, and the rise and decline of empires, while constantly examining cultural changes that affected the common woman and man as much as kings and queens. At all points emphasis will be placed on historical thinking and trying to better understand past cultures rather than memorization of facts, figures, and dates.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/cultural-intellectual-18th-century.html",
    "href": "course/cultural-intellectual-18th-century.html",
    "title": "Cultural and Intellectual History of Modern Europe, Eighteenth Century",
    "section": "",
    "text": "This course investigates the historical roots of modernity through an examination of the cultural and intellectual developments associated with the Enlightenment in the eighteenth century. The Enlightenment remains both influential and controversial in the 21st century. Enlightenment thinkers developed a science of man that exposed all aspects of society to the light of reason and criticism. They gave birth to the ideals of democracy, equality, and the concept of human rights that first found voice in the American and French Revolutions. However, critics of the Enlightenment contend that these ideals implicitly excluded individuals based on gender and race and that the Enlightenment’s emphasis on reason led to the development of Imperialism and Western hegemony. Embracing this complicated legacy, this course places eighteenth-century thinkers in the context of the development of commercial society, the beginnings of globalization, and debates on the outbreak and consequences of the French Revolution.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/freemasonry-civil-society-democracy.html",
    "href": "course/freemasonry-civil-society-democracy.html",
    "title": "Freemasonry, Civil Society, and Democracy in 18th-Century Europe and America",
    "section": "",
    "text": "This course investigates Freemasonry from its European origins in the Renaissance and the Scientific Revolution to its role in the American and French Revolutions. It begins by discussing the roots of Freemasonry in the radical thought of early modern Europe. A new mechanical philosophy emerged from the Scientific Revolution which brought with it the belief that the world was best understood mathematically as matter in motion. This new mechanical natural philosophy provided the basis for both moderate and radical strands of the intellectual movement known as the Enlightenment. Mainstream Freemasonry, begun with the foundation of the Grand Lodge in London in 1717, was intimately tied to the Moderate Enlightenment ideals associated with Newtonian science and the rule of a constitutional monarchy. However, Freemasonry also provided radicals, who espoused republican and sometimes even anti-Christian ideas, with a space to practice ritual fraternalism. In this course we will see how radical mechanical philosophy provided a foundation for principles of religious toleration, fraternity, and democracy, which were to be vital for the establishment of Freemasonry in Europe and America. The course concludes by looking at the ways that cosmopolitanism and exclusion were inextricably linked within Freemasonry by investigating the beginnings of African-American Freemasonry in the early American Republic.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/europe-and-world.html",
    "href": "course/europe-and-world.html",
    "title": "Europe and the World 1200–1648",
    "section": "",
    "text": "This course examines the dynamic transformation of Europe across four centuries of social, cultural, political, and economic change. The course begins with the development of an urbanized Europe increasingly linked by commercial exchanges but still dominated by a rural society of lords and peasants. We follow the changing social relations and growth of state and religious institutions in what is known as the High Middle Ages until its close with the demographic and political crisis of the fourteenth century brought about by famine, plague, and seemingly endless wars. The second half of the course traces the recovery and spectacular expansion of Europe in the world in the early modern period from the cultural efflorescence of the Renaissance to the creation of direct trade networks connecting Asia, Africa, Europe, and the Americas. European overseas expansion occurred as political division hardened and Western Christendom split with the Reformation, leading to the creation of territorial states and wars that lasted until the end of our period. Throughout the course we will emphasize social and cultural changes, investigating the lives of common women and men as much as princes and the nobility. At all points emphasis will be placed on historical thinking and trying to better understand past cultures rather than memorization of facts and figures.\nOver such a broad chronological span it is not possible to cover all issues of this interesting period. The course will focus on key developments that highlight Europe’s interactions with the wider world. These include the institutionalization of political and religious authority and the creation of the fiscal-military state; the development of commercial capitalism in which the economic and cultural center of Europe migrated from the Mediterranean to the Atlantic; and the gradual change from a cultural perspective that emphasized truth in universals to a society that increasingly accepted the singular and particular.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/modern-revolution.html",
    "href": "course/modern-revolution.html",
    "title": "Revolution in Modern Europe",
    "section": "",
    "text": "History 1204 examines the dynamic transformation of Europe and the world over the past five hundred years, emphasizing periods of rapid and momentous changes that have been seen as revolutionary by either contemporaries or historians. From the closing of the medieval world and the waning of the Renaissance, the course commences with the wide-ranging transformations brought about by the discovery of the Americas and the splintering of the Western Church with the Reformation. The course will investigate periods or political, social, cultural, and economic upheaval, highlighting the experience of contemporaries and how the concept of revolution has changed from the sixteenth century up to the twenty-first century. Through both primary sources and the work of historians, we will pay close attention to debates over the meaning and nature of rights that were supposed to be held by individuals. The topic of revolution will also lead to discussions of the ways that people have conceptualized time in the past and how people have looked to either the past or the future for inspiration in thinking about their present actions. At all points emphasis will be placed on historical thinking and trying to better understand past cultures rather than memorization of facts, figures, and dates.\n\nSyllabus PDF"
  },
  {
    "objectID": "course/digital-history.html",
    "href": "course/digital-history.html",
    "title": "Digital History for Public History",
    "section": "",
    "text": "Course website and syllabus\nThis course is designed to introduce students to some of the tools of Digital Humanities/History with an emphasis on using open source technology that puts you in control. The course focuses on methodologies. It begins with the basics of organizing primary and secondary sources and your notes on them. We will then give an introduction to data analysis and visualization using the R programming language, culminating in the creation of your own website. Along the way we will be learning the basics of using the command line and learn about git and using GitHub for publishing websites and to become acquainted with the practices of reproducible research."
  },
  {
    "objectID": "course/digital-history.html#learning-goals",
    "href": "course/digital-history.html#learning-goals",
    "title": "Digital History for Public History",
    "section": "Learning goals",
    "text": "Learning goals\n\nDevelop an expanded awareness of the open source tools available for conducting historical research and making that research widely available to the public.\nBe comfortable writing in plain text and using Markdown.\nUnderstand the uses and drawbacks of spreadsheets and how to organize rectangular data.\nBe comfortable with the basics of the command line.\nBe able to import, explore, wrangle, and visualize data in R.\nBe able to write R scripts and document your code in quarto documents.\nGain familiarity with the tidyverse packages in R.\nGain experience with the possibilities for text, network, and spatial analysis available through various R packages.\nBe comfortable with using git and GitHub for your own work or collaboration with others.\nCreate a website using open source tools."
  },
  {
    "objectID": "course/digital-history.html#schedule",
    "href": "course/digital-history.html#schedule",
    "title": "Digital History for Public History",
    "section": "Schedule",
    "text": "Schedule\n\nIntroduction\nDigital History and Public History\nOrganizing resources\nPlain text and markdown\nData and an intro to the command line\nIntro to R and RStudio\nData wrangling\nVisualization with ggplot2\nVersion control with Git and GitHub\nMaking documents and websites with Quarto\nReview and website workshop\nSpatial analysis\nNetwork analysis\nWeb design and visualization\nConclusion"
  },
  {
    "objectID": "post/simple-feature-objects/index.html",
    "href": "post/simple-feature-objects/index.html",
    "title": "An Exploration of Simple Features for R",
    "section": "",
    "text": "The previous post provided an introduction to the sp and sf packages and how they represent spatial data in R. There I discussed the creation of Spatial and sf objects from data with longitude and latitude values and the process of making maps with the two packages. In this post I will provide further background for the sf package by going into the details of the structure of sf objects and explaining how the package implements the Simple Features open standard. It is certainly not necessary to know the ins and outs of sf objects and the Simple Features standard to use the package — it has taken me long enough to get my head around much of this — but a better knowledge of the structure and vocabulary of sf objects is helpful for understanding the effects of the plethora of sf functions. There are a variety of good resources that discuss the structure of sf objects. The most comprehensive are the package vignette Simple Features for R and the overview in Chapter 2 of the working book  Geocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow. This post is based on these sources, as well as my own sleuthing through the code for the sf package.\nBefore diving in, let’s take a step back to provide some background to the package. The sf package implements the Simple Features standard in R. The Simple Features standard is widely used by GIS software such as PostGIS, GeoJSON, and ArcGIS to represent geographic vector data. The sf package is designed to bring spatial analysis in R in line with these other systems.1 The standard defines a simple feature as a representation of a real world object by a point or points that may or may not be connected by straight line segments to form lines or polygons. A simple feature can contain both a geometry that includes points, any connecting lines, and a coordinate reference system to identify its location of Earth and attributes to describe the object, such as a name, values, color, etc. The sf package takes advantage of the wide use of Simple Features by linking directly to the GDAL, GEOS, and PROJ libraries that provide the back end for reading spatial data, making geographic calculations, and handling coordinate reference systems.2 You can see this from the message when you load the sf package, so let’s do that now.\nlibrary(sf)\n#&gt; Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\nWith this general definition of Simple Features in mind, we can look at how the sf package implements the standard through the sf class of object.3 At its most basic, an sf object is a collection of simple features that includes attributes and geometries in the form of a data frame. In other words, it is a data frame (or tibble) with rows of features, columns of attributes, and a special geometry column that contains the spatial aspects of the features. The special geometry column is itself a list of class sfc, which is made up of individual objects of class sfg. While it is possible to have multiple geometry columns, sf objects usually only have a single geometry column. We can break down the components of an sf object by looking at the printed output of an sf object.4\nThis terminology covers the basics for sf objects and provides the majority of information you need to work with them. The most likely way to create an sf object is convert another type of spatial object or a data frame with coordinates to an sf object. Rarely is there a reason to create an sf object from scratch. However, we can learn more about the nature of sf objects by delving further into their structure and explicating how sfg and sfc objects combine to store spatial data. The rest of the post will do this by creating an sf object step by step from a single sfg object, to a combination of sfg objects in an sfc object, and finally the addition of attributes to produce a full sf object."
  },
  {
    "objectID": "post/simple-feature-objects/index.html#sfg",
    "href": "post/simple-feature-objects/index.html#sfg",
    "title": "An Exploration of Simple Features for R",
    "section": "\nsfg objects",
    "text": "sfg objects\nsfg objects represent the geometry of a single feature and contain information about the feature’s coordinates, dimension, and type of geometry. The coordinates are the values of the point or points in the feature, which can have two-, three-, or four dimensions. The vast majority of data is two dimensional, possessing and X and Y coordinates, but the Simple Features standard also possesses a Z coordinate for elevation and an M coordinate for information about the measurement of a point that is rarely used. The geometry type indicates the shape of the feature, whether it consists of a point, multiple points, a line, multiple lines, a polygon, multiple polygons, or some combination of these. There are seventeen different possible types of geometries, but the vast majority of data will use the seven main types discussed here.5\nGeometry types\n\n\nPOINT – a single point\n\nMULTIPOINT – multiple points\n\n\nLINESTRING – sequence of two or more points connected by straight lines\n\nMULTILINESTRING – multiple lines\n\nPOLYGON – a closed ring with zero or more interior holes\n\nMULTIPOLYGON - multiple polygons\n\nGEOMETRYCOLLECTION – any combination of the above types\n\n\n\nWithin the Simple Features standard the geometry of a feature is encoded using well-known binary (WKB) that can also be represented in a more human readable form of well-known text (WKT). The sf package can convert between sfg and WKB or WKT very quickly, but sfg objects are created and stored in R as vectors, matrices, or lists of matrices depending on the geometry type.\nCreating geometry types\n\n\nPOINT – a vector\n\nMULTIPOINT and LINESTRING – a matrix with each row representing a point\n\nMULTILINESTRING and POLYGON – a list of matrices\n\nMULTIPOLYGON - a list of lists of matrices\n\nGEOMETRYCOLLECTION – list that combines any of the above\n\nAn sfg object of each of the seven main geometry types can be created through separate functions: st_point(), st_multipoint(), st_linestring(), etc. The functions all have two arguments: an R object of the proper class to create the specified geometry type and identification of dimensions as either XYZ or XYM if the data has three dimensions. In this post I will create POINT, MULTIPOINT, and LINESTRING geometry types, which are the most likely to be used when creating an sf object from scratch and provide the basis for the remaining types. You can see how to create the remaining types in the package vignette and Geocomputation with R. Let’s start by creating two points using vectors of longitude and latitude values for points in Los Angeles and Amsterdam.\n\n# Create an sfg object with coordinates of Los Angeles and Amsterdam\nla_sfg &lt;- st_point(c(-118.2615805, 34.1168926))\namsterdam_sfg &lt;- st_point(c(4.8979755, 52.3745403))\n\nThe relative simplicity of the command to create an sfg POINT underlines the simplicity of sfg objects as a whole. We can see this by taking a closer look at the information contained in these objects.\n\n# Print sfg POINT objects\nla_sfg\n#&gt; POINT (-118.2616 34.11689)\namsterdam_sfg\n#&gt; POINT (4.897976 52.37454)\n\nThe printing method for all sfg objects uses the well-known text style with the geometry type printed in capital letters followed by the coordinates, using commas and parentheses to distinguish between the different elements for each feature.6\n\n# Structure of a sfg POINT object\nstr(la_sfg)\n#&gt;  'XY' num [1:2] -118.3 34.1\n\nIf we look a bit further into the structure of an sfg object, we can see that it consists of three classes corresponding to the dimensions, geometry type, and the sfg class itself. In addition, the object possesses coordinates, which is a vector of length two. The coordinates can also be represented by a matrix with two columns and one row. In fact, all sfg objects can be represented as a matrix of coordinates, though more complex geometry types contain additional columns to keep track of pieces that make up the feature or features.7\n\n# Coordinates of a sfg object\nst_coordinates(la_sfg)\n#&gt;           X        Y\n#&gt; 1 -118.2616 34.11689\n\nPoints are the building block for all other geometry types. We can see this by creating a MULTIPOINT and LINESTRING simple feature, which are both formed by a matrix of points. The matrix can be created by using rbind() to bind the coordinate vectors together into rows.\n\n# Matrix of points\nrbind(c(-118.2615805, 34.1168926), c(4.8979755, 52.3745403))\n#&gt;             [,1]     [,2]\n#&gt; [1,] -118.261580 34.11689\n#&gt; [2,]    4.897976 52.37454\n\n# Create MULTIPOINT and LINESTRING sfg objects through matrix of points\nmultipoint_sfg &lt;- st_multipoint(rbind(c(-118.2615805, 34.1168926), c(4.8979755, 52.3745403)))\nlinestring_sfg &lt;- st_linestring(rbind(c(-118.2615805, 34.1168926), c(4.8979755, 52.3745403)))\n\n# Print objects\nmultipoint_sfg\n#&gt; MULTIPOINT ((-118.2616 34.11689), (4.897976 52.37454))\nlinestring_sfg\n#&gt; LINESTRING (-118.2616 34.11689, 4.897976 52.37454)\n\nPrinting the two new sfg objects shows their basic similarity. They were created by the same matrix and thus have the same coordinates. The only difference is that one is of class MULTIPOINT and the other is of class LINESTRING. In other words, the geometry of multipoint_sfg consists of points, and the geometry of linestring_sfg is a straight line between the same two points. We can see this by plotting the objects next to each other.\n\npar(mar = c(0, 1, 2, 1),\n    mfrow = c(1, 2))\nplot(multipoint_sfg, main = \"MULTIPOINT\")\nplot(linestring_sfg, main = \"LINESTRING\")\n\n\n\n\n\n\n\nAs important as what type of information is contained in sfg objects is, what is missing is equally significant in understanding their role within sf objects. sfg objects consist of a dimension, geometry type, and coordinates, but though they possess spatial aspects, they are not not geospatial. sfg objects do not possess a coordinate reference system (CRS). There is nothing within the structure of an sfg object to indicate that the X and Y values of the coordinates correspond to longitude and latitude values, much less to the datum of the coordinates. Calculating the distance between la_sfg and amsterdam_sfg demonstrates the non-geospatial nature of sfg objects. The calculation cannot take into account the ellipsoidal nature of the Earth, nor that the distance between degrees of longitude decrease as they move towards the poles. Thus, the calculation for st_distance() has no direct real-world meaning. In fact, the st_distance() function with sfg objects returns the same value as the base R dist() function from the stats package.\n\n# Distance on sfg objects does not have real-world meaning\nst_distance(la_sfg, amsterdam_sfg)\n#&gt;          [,1]\n#&gt; [1,] 124.5055\n\n# st_distance and stats::dist the same for sfg objects\ndist(rbind(c(-118.2615805, 34.1168926), c(4.8979755, 52.3745403)))\n#&gt;          1\n#&gt; 2 124.5055"
  },
  {
    "objectID": "post/simple-feature-objects/index.html#sfc",
    "href": "post/simple-feature-objects/index.html#sfc",
    "title": "An Exploration of Simple Features for R",
    "section": "\nsfc objects",
    "text": "sfc objects\nThe spatial aspects of an sf object are contained in an object of class sfc. The documentation for the sf package often refers to an sfc object as a simple feature geometry list column, which is the role that they play within sf objects. However, as a stand-alone object, an object of class sfc is a list of one or more sfg objects with attributes — in the R sense of attributes — that enable the object to have a coordinate reference system. When an sfc object is printed to the console, it is referred to as a geometry set. The sfc class possesses seven subclasses, one for an sfc object composed of each of the six main geometry types and an addition subclass for an sfc object that contains a combination of geometry types.\n\nsfc subclasses\n\nsfc_POINT\nsfc_MULTIPOINT\nsfc_LINESTRING\nsfc_MULTILINESTRING\nsfc_POLYGON\nsfc_MULTIPOLYGON\nsfc_GEOMETRY\n\nThe st_sfc() function to create an sfc object takes any number of sfg objects and a CRS in the form of either an EPSG code or a proj4string from the PROJ library. Internally, the crs argument uses the st_crs() function to look up either the EPSG code or proj4string and, if possible, set the corresponding value for the undefined epsg or proj4string through external calls to the GDAL and PROJ libraries. The default CRS for an sfc object is NA or not available. Let’s create an sfc object from one of the above sfg objects with the default CRS to see how an sfc object differs from an sfg object even when it contains the geometry of only one feature.\n\n# Create sfc object with default crs\nst_sfc(multipoint_sfg)\n#&gt; Geometry set for 1 feature \n#&gt; Geometry type: MULTIPOINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -118.2616 ymin: 34.11689 xmax: 4.897976 ymax: 52.37454\n#&gt; CRS:           NA\n#&gt; MULTIPOINT ((-118.2616 34.11689), (4.897976 52....\n\nThe sfc print method reveals the information contained in the sfg object or objects used to create it, including the geometry type or sfc subclass, dimensions, and the coordinates printed in the same well-known text format used by the sfg print method.8 However, the sfc print method also reveals the addition of the crs attribute by printing out the epsg and proj4string values. Thus, even though in this case the geospatial aspect of the sfc object is left undefined, the object can be considered geospatial. Let’s further investigate the structure of an sfc object by creating an sfc object with multiple sfg objects and use the EPSG code 4326 to identify longitude and latitude coordinates on the WGS84 ellipsoid for the CRS. Notice in the print out of the object that the proj4string is identified, though it was not included in the arguments.\n\n# Create sfc object with multiple sfg objects\npoints_sfc &lt;- st_sfc(la_sfg, amsterdam_sfg, crs = 4326)\npoints_sfc\n#&gt; Geometry set for 2 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -118.2616 ymin: 34.11689 xmax: 4.897976 ymax: 52.37454\n#&gt; Geodetic CRS:  WGS 84\n#&gt; POINT (-118.2616 34.11689)\n#&gt; POINT (4.897976 52.37454)\n\nWe can see that an sfc object is a list and treated as such in R by running View(points_sfc), which will give an output similar to any other list object. However, points_sfc is different from a normal list in that an sfc object possesses five attributes that give sfc objects their geospatial nature.\n\n# Attributes of sfc object\nattributes(points_sfc)\n#&gt; $class\n#&gt; [1] \"sfc_POINT\" \"sfc\"      \n#&gt; \n#&gt; $precision\n#&gt; [1] 0\n#&gt; \n#&gt; $bbox\n#&gt;        xmin        ymin        xmax        ymax \n#&gt; -118.261580   34.116893    4.897976   52.374540 \n#&gt; \n#&gt; $crs\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:4326 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n#&gt; \n#&gt; $n_empty\n#&gt; [1] 0\n\nHere, we can clearly see that points_sfc is of class sfc and subclass sfc_POINT since it only contains geometries of type POINT. The precision attribute corresponds to the precision element of the Simple Features standard and is used in certain geometric calculations. bbox is a calculation of the minimum and maximum values of the X and Y coordinates within the sfc object, and n_empty notes the number of empty sfg objects in the list. The crs attribute is the most interesting for our purposes and consists of a crs object, which itself is a list of length two containing a proj4string and an epsg value if one exists. Because the crs is an attribute of sfc objects, all geometries within an sfc object by definition possess the same CRS. We can inspect the contents of the crs attribute for points_sfc with the st_crs() function even if it does not provide us with any additional information.\n\n# crs attribute is of class crs\nclass(st_crs(points_sfc))\n#&gt; [1] \"crs\"\n\n# Access crs attribute of sfc object\nst_crs(points_sfc)\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:4326 \n#&gt;   wkt:\n#&gt; GEOGCRS[\"WGS 84\",\n#&gt;     ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#&gt;         MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G730)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G873)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#&gt;         MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#&gt;         ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ENSEMBLEACCURACY[2.0]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"geodetic latitude (Lat)\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;         AXIS[\"geodetic longitude (Lon)\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#&gt;     USAGE[\n#&gt;         SCOPE[\"Horizontal component of 3D system.\"],\n#&gt;         AREA[\"World.\"],\n#&gt;         BBOX[-90,-180,90,180]],\n#&gt;     ID[\"EPSG\",4326]]\n\nWe can confirm the spatial nature of points_sfc by again using the st_distance() function. Beginning with version 0.6 of the sf package, st_distance() uses the lwgeom package, which in turn links to geometric functions from the liblwgeom library used by PostGIS, to make geometric calculations on longitude and latitude values. Therefore, you may need to install the lwgeom package for the st_distance() function to work properly.\n\n# Geographic distance with sfc object\nst_distance(points_sfc)\n#&gt; Units: [m]\n#&gt;         [,1]    [,2]\n#&gt; [1,]       0 8933585\n#&gt; [2,] 8933585       0\n\nWith an sfc object that uses longitude and latitude coordinates and a set crs the st_distance() function uses complex geometric calculations on the chosen ellipsoid to accurately calculate the distance between features. By default the function returns a dense matrix of the distance between all of the features — or sfg objects — contained in the sfc object along with the units of the values from the units package. Here, we can see that the points in Los Angeles and Amsterdam are 8,955,120 meters apart from each other. You can even convert to miles or kilometers with the units package, but the most significant aspect of this command is how it differs from the above use of st_distance() with sfg objects. We now have usable geographical information about the two points we created."
  },
  {
    "objectID": "post/simple-feature-objects/index.html#sf",
    "href": "post/simple-feature-objects/index.html#sf",
    "title": "An Exploration of Simple Features for R",
    "section": "\nsf objects",
    "text": "sf objects\nNow that we have a set of geometries containing a coordinate reference system in the form of an sfc object, we can connect points_sfc to a data frame of attributes to create an sf object. First though, we need a data frame of attributes that provides information about the two points.\n\n# Create a data frame of attributes for the two points\ndata &lt;- data.frame(name = c(\"Los Angeles\", \"Amsterdam\"),\n                   language = c(\"English\", \"Dutch\"),\n                   weather = c(\"sunny\", \"rainy/cold\"))\n\nThe st_sf() function can either join a data frame to an sfc object of the same length or take a data frame that already contains an sfc list column to create an sf object. When joining a data frame to an sfc object you can name the geometry list column. Otherwise the column will be named after the sfc object. It is also worth pointing out that the methodology used by st_sf converts a tibble to a data frame, and so the output of the function will always be an object of classes sf and data.frame. Here I want to show two different ways to make an sf object with the data data frame and points_sfc, first by combining the two objects into a single data frame before executing the sf_sf() function and then letting the sf_sf() function do the joining. In the first method, the use of cbind() automatically names the geometry list column as “geometry.”\n\n# Create data frame with list column then make sf object\nst_sf(cbind(data, points_sfc))\n#&gt; Simple feature collection with 2 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -118.2616 ymin: 34.11689 xmax: 4.897976 ymax: 52.37454\n#&gt; Geodetic CRS:  WGS 84\n#&gt;          name language    weather                   geometry\n#&gt; 1 Los Angeles  English      sunny POINT (-118.2616 34.11689)\n#&gt; 2   Amsterdam    Dutch rainy/cold  POINT (4.897976 52.37454)\n\n# Make sf object from separate data frame and sfc objects\nst_sf(data, geometry = points_sfc)\n#&gt; Simple feature collection with 2 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -118.2616 ymin: 34.11689 xmax: 4.897976 ymax: 52.37454\n#&gt; Geodetic CRS:  WGS 84\n#&gt;          name language    weather                   geometry\n#&gt; 1 Los Angeles  English      sunny POINT (-118.2616 34.11689)\n#&gt; 2   Amsterdam    Dutch rainy/cold  POINT (4.897976 52.37454)\n\nI think that the latter method is preferable, but the former method demonstrates part of the internal process of st_sf() when using the latter method. In either case, both methods produce the same result, and we are presented with the same print method for an sf object that we saw at the beginning of the post. As I noted above, the result of st_sf() is an object of classes sf and data.frame, showing that sf objects are an extension of data frames. Let’s confirm that by creating an sf object and checking its class.\n\n# Create sf object by combining data frame with sfc object and name sfc column geometry\npoints_sf &lt;- st_sf(data, geometry = points_sfc)\n\n# Class of new sf object\nclass(points_sf)\n#&gt; [1] \"sf\"         \"data.frame\"\n\nThe creation of points_sf does not alter the geometry or geographical information contained in points_sfc in any way. To confirm this we can show that the geometry of points_sf — accessed via the st_geometry() function — is equivalent to points_sfc.\n\n# Geometry of points_sf is equivalent to points_sfc\nidentical(st_geometry(points_sf), points_sfc)\n#&gt; [1] TRUE\n\nIn many ways an sf object can be treated as if it were a data frame, and this is one of the main advantages of working with sf objects over Spatial objects from the sp package. As I noted in my previous post, you can manipulate sf objects using dplyr commands. However, internally sf objects are different from data frames in that the geometry column is designed to be sticky. In addition to the usual attributes associated with a data frame of column names, row names, and class, sf objects also possess an sf_column attribute, which is a vector of the names of the one or more geometry columns in the sf object.9 The sf_column attribute helps to implement the stickiness of geometry columns. The geometry column is kept in all subsetting of an sf object. For example, using dplyr’s select() function with a single column name returns an sf object with two columns, the selected column and the geometry column.10\n\n# Stickiness of geometry column\ndplyr::select(points_sf, name)\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -118.2616 ymin: 34.11689 xmax: 4.897976 ymax: 52.37454\n#&gt; Geodetic CRS:  WGS 84\n#&gt;          name                   geometry\n#&gt; 1 Los Angeles POINT (-118.2616 34.11689)\n#&gt; 2   Amsterdam  POINT (4.897976 52.37454)\n\nIf you want to remove the geometry column and transform an sf object to a data frame of tibble, you can use as_tibble() or as.data.frame() in a pipe before select(). More directly, you can remove the geometry column and convert an sf object to a tibble or data frame by setting the geometry to NULL. By converting from an sf object to a data frame, we have come full circle in the creation of an sf object from scratch.\n\n# Return sf object to a data frame by setting geometry to NULL\nst_set_geometry(points_sf, NULL)\n#&gt;          name language    weather\n#&gt; 1 Los Angeles  English      sunny\n#&gt; 2   Amsterdam    Dutch rainy/cold\n\n# Class\nclass(st_set_geometry(points_sf, NULL))\n#&gt; [1] \"data.frame\""
  },
  {
    "objectID": "post/simple-feature-objects/index.html#conclusion",
    "href": "post/simple-feature-objects/index.html#conclusion",
    "title": "An Exploration of Simple Features for R",
    "section": "Wrapping up",
    "text": "Wrapping up\nThe sf package is quickly becoming the default GIS package for use in R. It has done this by balancing the use of the Simple Features standard, enabling it to link directly to powerful GIS libraries such as PostGIS, GEOS, and GDAL, and an implementation that uses native R objects and fits within the tidyverse set of packages and methods. Investigating the structure of sf objects and how they are made through sfg objects, a sfc list column, and a data frame provides insight into how a data-frame-like object is able to possess spatial data.\nWe can summarize the role of sfg, sfc, and sf objects in the following fashion.\n\n\nsfg: geometry\n\ngeometry of a single feature\nvector, matrix, or list of matrices of coordinates with defined dimension and type of geometry\nseven main geometry types\n\n\n\nsfc: geospatial geometry\n\nlist of sfg objects\ncoordinate reference system through crs attribute\nseven subclasses based on geometries\n\n\n\nsf: geospatial geometry with attributes\n\ndata frame with geometry column of class sfc\n\nsticky geometry column through sf_column attribute"
  },
  {
    "objectID": "post/simple-feature-objects/index.html#footnotes",
    "href": "post/simple-feature-objects/index.html#footnotes",
    "title": "An Exploration of Simple Features for R",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe Simple Features standard was developed after the creation of the sp package, and so sp does not use the standard.↩︎\nBefore sf, the primary interfaces to GDAL and GEOS were through the separate rgdal and rgeos packages.↩︎\nThere are many things called “sf” or “simple features” here, so I will try to be as clear as possible in distinguishing them.↩︎\nSee Introduction to GIS with R for the code that created this object. See also the similar image in the package vignette.↩︎\nThe sf package prints the geometry types in capital letters.↩︎\nSee the Implementation Standard for Geographic information - Simple feature access, pg 61 for examples of the well-known text style.↩︎\nSee help(\"st_coordinates\") for details.↩︎\nIf you have more than five features, the print command will only print the first five geometries. You can alter this with an explicit print command with a defined “n” for the number of geometries you want to print.↩︎\nAn sf object also contains an agr attribute that can categorize the non-geometry columns as either “constant”, “aggregate”, or “identity” values.↩︎\nSince I have not loaded the dplyr package in this post, I am using the :: method to call a function from a package that is not loaded.↩︎"
  },
  {
    "objectID": "post/gis-with-r-intro/index.html",
    "href": "post/gis-with-r-intro/index.html",
    "title": "Introduction to GIS with R",
    "section": "",
    "text": "The geographic visualization of data makes up one of the major branches of the Digital Humanities toolkit. There are a plethora of tools that can visualize geographic information from full-scale GIS applications such as ArcGIS and QGIS to web-based tools like Google maps to any number of programing languages. There are advantages and disadvantages to these different types of tools. Using a command-line interface has a steep learning curve, but it has the benefit of enabling approaches to analysis and visualization that are customizable, transparent, and reproducible.1 My own interest in coding and R began with my desire to dip my toes into geographic information systems (GIS) and create maps of an early modern correspondence network. The goal of this post is to introduce the basic landscape of working with spatial data in R from the perspective of a non-specialist. Since the early 2000s, an active community of R developers has built a wide variety of packages to enable R to interface with geographic data. The extent of the geographic capabilities of R is readily apparent from the many packages listed in the CRAN task view for spatial data.2\nIn my previous post on geocoding with R I showed the use of the ggmap package to geocode data and create maps using the ggplot2 system. This post will build off of the location data obtained there to introduce the two main R packages that have standardized the use of spatial data in R. Thesp and sf packages use different methodologies for integrating spatial data into R. The sp package introduced a coherent set of classes and methods for handling spatial data in 2005.3 The package remains the backbone of many packages that provide GIS capabilities in R. The sf package implements the simple features open standard for the representation of geographic vector data in R. The package first appeared on CRAN at the end of 2016 and is under very active development. The sf package is meant to supersede sp, implementing ways to store spatial data in R that integrate with the tidyverse workflow of the packages developed by Hadley Wickham and others.\nThere are a number of good resources on working with spatial data in R. The best sources for information about the sp and sf packages that I have found are Roger Bivand, Edzer Pebesma, and Virgilio Gómez-Rubio, Applied Spatial Data Analysis with R (2013) and the working book Robin Lovelace, Jakub Nowosad, Jannes Muenchow, Geocomputation with R, which concentrate on sp and sf respectively. The vignettes for sf are also very helpful. The perspective that I adopt in this post is slightly different from these resources. In addition to more explicitly comparing sp and sf, this post approaches the two packages from the starting point of working with geocoded data with longitude and latitude values that must be transformed into spatial data. It takes the point of view of someone getting into GIS and does not assume that you are working with data that is already in a spatial format. In other words, this post provides information that I wish I knew as I learned to work with spatial data in R. Therefore, I begin the post with a general overview of spatial data and how sp and sf implement the representation of spatial data in R. The second half of the post uses an example of mapping the locations of letters sent to a Dutch merchant in 1585 to show how to create, work with, and plot sp and sf objects. I highlight the differences between the two packages and ultimately discuss some reasons why the R spatial community is moving towards the use of the sf package."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#crs",
    "href": "post/gis-with-r-intro/index.html#crs",
    "title": "Introduction to GIS with R",
    "section": "Spatial data and coordinate reference systems",
    "text": "Spatial data and coordinate reference systems\nThe distinguishing feature of spatial data is that it represents actual locations on Earth. To represent the geographic placement of an object you need two pieces of information: the coordinates of the object and a system of reference for how the coordinates relate to a physical location on Earth. The non-spherical shape of the Earth, which bulges at the equator, complicates the creation and use of a coordinate reference system or CRS and plethora of complex models have been created in attempts to accurately represent the Earth’s surface. A CRS consists of one such ellipsoid or geometric model of the shape of the Earth and a datum, which identifies the origin and orientation of the coordinate axes on the ellipsoid, as well as the units of measurement.4 There are two types of CRSs: geographic and projected. Geographic reference systems represent points on an globe, using units of degrees longitude and latitude that correspond to angles measured from the center of the Earth as calculated using the given ellipsoid. A projected reference system uses a geometric model to project a 3-dimensional ellipsoid onto a 2-dimensional plane. A projection is necessary to create any 2-dimensional map, but it results in the distortion of aspects of the Earth’s surface such as area, direction, distance, and shape. Despite the necessary distortion, projected reference systems are useful for geographic analysis, because they use linear units of measurement such as meters instead of degrees.5\nBetween the variety of ellipsoidal models, the vast array of global and local datums, and the various kinds of projections, there are innumerable CRSs to choose from. The sp and sf packages implement two different ways to identify a specific CRS and to transform objects from one CRS to another: the Proj.4 library and EPSG codes. The Proj.4 library uses an identification system of +parameter=value to define a CRS. The library uses a wide range of parameters to specify a CRS, but the most important are +proj and +datum for the projection and datum to be used. The Proj.4 parameters provides the basis for how sp and sf identify CRSs, but CRSs can also be identified through EPSG codes.6 The EPSG library supplies codes for well-known CRSs, and thus provides an easier method to identify a subset of the CRSs available through the Proj.4 library. You can access a data frame of over 5,000 EPSG codes available in R through the rgdal package with the command rgdal::make_EPSG(). If you plan to deal with spatial data on a regular basis it is probably worth being familiar with widely used EPSG code such as 4326, which is geographic reference system that uses units of longitude and latitude on the  World Geodetic System 1984 (WGS84) ellipsoid. Two useful resources for looking up and obtaining information about specific EPSG codes are EPSG.io and SpatialReference.org.\nSpatial data with a defined CRS can represent either vector or raster data. Vector data is based on points — which can be connected to form lines and polygons — that are located within a coordinate reference system. Raster data, on the other hand, consists of values within a grid system. For example, a road map is a vector data and a map using satellite imagery is raster data made up of pixels on a grid. sp has capabilities to work with both vector and raster data, while sf and the simple features standard on which it is based only deals with vector data. This post will only discuss vector data, which is more widely used in the Humanities and Social Sciences."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#overview-spsf",
    "href": "post/gis-with-r-intro/index.html#overview-spsf",
    "title": "Introduction to GIS with R",
    "section": "Overview of sp and sf packages",
    "text": "Overview of sp and sf packages\nThe complexity that comes with the identification and translation between the thousands of different coordinate reference systems makes it impossible for standard R objects such as data frames to properly represent spatial data. In addition, the best way to represent coordinates — in one or two columns — and the representation of more complex objects such as lines and polygons create problems for how to store spatial data. These shortcomings can be seen with geocoded data that contains longitude and latitude values in a data frame like that I created in geocoding with R. Most geocoded data created through Google Maps or other geocoding sites  provides coordinates in longitude and latitude values using the WGS84 ellipsoid and thus contains a CRS equivalent to the Proj.4 argument +proj=longlat +datum=WGS84 or EPSG 4326. However, this information about the CRS is implicit and not contained anywhere within the data frame itself.\nDue to the inadequacies of normal data frame objects to represent the variety of features of spatial data, the sp and sf packages both define their own classes of objects to store spatial data. From a beginner’s perspective, the main differences between the sp and sf packages derive from the manner by which sp and sf classes store information about CRS, distinguish between points, lines, and polygons, and how they connect this spatial data to non-spatial data stored in a data frame.\nThe sp package uses what are known as S4 classes in R to represent spatial data. S4 objects are made up of slots that store different types of well-defined data. The slots can be accessed with the @ symbol in the form of object@slot. The foundational sp class is the Spatial class, which has ten subclasses differentiated by the slots they contain. To get a better idea of what this all looks like, we can use the getClass() function, but first we have to load the sp package.7\n\n# Load the sp package\nlibrary(sp)\n\n# Spatial classes\ngetClass(\"Spatial\")\n#&gt; Class \"Spatial\" [package \"sp\"]\n#&gt; \n#&gt; Slots:\n#&gt;                               \n#&gt; Name:         bbox proj4string\n#&gt; Class:      matrix         CRS\n#&gt; \n#&gt; Known Subclasses: \n#&gt; Class \"SpatialPoints\", directly\n#&gt; Class \"SpatialMultiPoints\", directly\n#&gt; Class \"SpatialGrid\", directly\n#&gt; Class \"SpatialLines\", directly\n#&gt; Class \"SpatialPolygons\", directly\n#&gt; Class \"SpatialPointsDataFrame\", by class \"SpatialPoints\", distance 2\n#&gt; Class \"SpatialPixels\", by class \"SpatialPoints\", distance 2\n#&gt; Class \"SpatialMultiPointsDataFrame\", by class \"SpatialMultiPoints\", distance 2\n#&gt; Class \"SpatialGridDataFrame\", by class \"SpatialGrid\", distance 2\n#&gt; Class \"SpatialLinesDataFrame\", by class \"SpatialLines\", distance 2\n#&gt; Class \"SpatialPixelsDataFrame\", by class \"SpatialPoints\", distance 3\n#&gt; Class \"SpatialPolygonsDataFrame\", by class \"SpatialPolygons\", distance 2\n\nThe output lists the different Spatial classes and shows that the basis for all Spatial objects is the bbox and proj4string slots. The proj4string provides the CRS for an object through a Proj.4 definition, while the bbox slot provides a matrix of the minimum and maximum coordinates for the object. The Spatial subclasses add slots to the foundational slots of bbox and proj4string for the type of geometric data — points, lines, polygons, and grid for raster — and whether the object contains attribute data in the form of a data frame. We can see this from the slots in the SpatialPoints and the SpatialPointsDataFrame classes.\n\n# slots for SpatialPoints class\nslotNames(\"SpatialPoints\")\n#&gt; [1] \"coords\"      \"bbox\"        \"proj4string\"\n\n# slots for SpatialPointsDataFrame\nslotNames(\"SpatialPointsDataFrame\")\n#&gt; [1] \"data\"        \"coords.nrs\"  \"coords\"      \"bbox\"        \"proj4string\"\n\nThe SpatialPoints class contains a coords slot to store the point coordinates. The SpatialPointsDataFrame class adds a data slot that enables points to be associated with attribute data in a data frame. The non-spatial data of a Spatial*DataFrame object can be accessed with @data. However, these S4-style classes do not fit within the definition of tidy data and cannot be manipulated using the dplyr methods I have discussed in previous posts. Instead, all manipulation of the @data slot is done with base R commands. sp objects also cannot be plotted with ggplot2, but need to use the base or trellis plotting systems, though it is possible to force them into a compatible form with the broom package.\nIn contrast to the Spatial class, the sf class — yes, the package and class have the same name — is an extension of data frames. Essentially, sf objects can be treated as data frames that also contain spatial data, as opposed to spatial data that may or may not also contain data frames. This enables sf objects to fit within the tidyverse workflow, making it possible to manipulate them with dplyr commands. The tidy nature of sf objects also means that they can can be plotted with ggplot2, though currently this capability is only possible with the development version of ggplot2.\nsf objects consist of rows of features, hence the name simple features, which have both non-spatial and spatial forms of data. The spatial data of an sf object is contained in a special geometry column that is of class sfc. The geometry column contains the same basic types of spatial data as the slots in Spatial objects: the CRS, coordinates, and type of geometric object. The sfc class has seven subclasses to denote the types of geometric objects within the geometry column, which are derived from the simple features standard. The possible geometric objects are point, linestring, polygon, multipoint, multilinestring, multipolygon, and geometrycollection for any combination of the other types.8\nWith this basic introduction to spatial data and the sp and sf packages out of the way, the remainder of the post will demonstrate the creation and visualization of Spatial and sf objects by creating maps of the letters sent to Daniel van der Meulen in 1585. For this example, I want to map the total number of letters sent from and received in each location as I did previously with ggamp. This will involve working with spatial data in the form of points, lines, and polygons. The points will represent the sources and destinations of the letters Daniel received and will be plotted on two different kinds of background maps using the rnaturalearth package to access coastal (lines) and country (polygons) world maps from the Natural Earth open-source repository of maps. In the process I will show how to make base R maps with sp and ggplot2 plots with sf and development version of ggplot2."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#preparing-data",
    "href": "post/gis-with-r-intro/index.html#preparing-data",
    "title": "Introduction to GIS with R",
    "section": "Preparing the data",
    "text": "Preparing the data\nThe first thing that we need to do is get information about the number of letters sent from and received in each city and join this non-spatial data with the longitude and latitude information that was created in the geocoding with R post. This will provide the necessary components to create spatial points object in both sp and sf. You can find the data and the R script that goes along with this example on GitHub. Let’s start by loading the necessary packages and the two files containing data on the letters and the location of the cities in the letters data.\n\n# Load the packages\nlibrary(tidyverse)\nlibrary(sp)\nlibrary(sf)\nlibrary(rnaturalearth)\n\n# Load the data\nletters &lt;- read_csv(\"data/correspondence-data-1585.csv\")\nlocations &lt;- read_csv(\"data/locations.csv\")\n\nThe letters data frame contains 114 rows of letters with columns representing the writer, source, destination, and date of each letter. This example will only make use of the source and destination of the letters. The locations data frame contains columns for the name, longitude, and latitude of the 13 cities found in the letters data. There are a couple of ways that the data can be prepared, but in this case I want the data for the source and destination of the letters to be in a single data frame in order to minimize the objects that we will be dealing with in the post. The goal is to create a data frame with a column for the name of the city, the number of letters, and whether this information refers to letters sent or letters received. This can be done by separately finding the number of letters sent to each location and the number of letters received in each location and then joining the two data frames together.\nWe can get the number of letters sent from and received in each location by using count(), which is a shortcut for group_by() %&gt;% summarise(n = n()). This provides us with a count column, but we also want to structure the data frames for source and destination in the same way. Therefore, I rename the column containing the city information to “place.” Doing this removes information about whether the observation is a source or destination, and so I add in this data with add_column(), creating a “type” column.\n\n# Letters per source\nsources &lt;- letters %&gt;% \n  count(source) %&gt;% \n  rename(place = source) %&gt;% \n  add_column(type = \"Source\")\n\n# Letters per destination\ndestinations &lt;- letters %&gt;% \n  count(destination) %&gt;% \n  rename(place = destination) %&gt;% \n  add_column(type = \"Destination\")\n\nAt this point, you can use either rbind() or full_join() to bind the rows of sources and destinations together to create a letters_data data frame that contains the non-spatial data we will be using. In addition, we need to make a small but important change to the type of data in the “type” column to make it possible to differentiate between the sources and destinations in the base R plots created below. We can transform the “type” column to a factor using mutate() and as_factor() from the forcats package. This changes the way that R represents the data behind the scenes such that “source” and “destination” can be represented by numeric integers, which is necessary for working with colors in base R plots.9\n\n# Bind the rows of the two data frames\n# and change type column to factor\nletters_data &lt;- rbind(sources, destinations) %&gt;% \n  mutate(type = as_factor(type))\n\nLet’s look at the result. We can see that the data is in the format that we are looking for and that the “type” column has been correctly transformed into factors.\n\n# Print letters_data\nletters_data\n#&gt; # A tibble: 14 × 3\n#&gt;    place          n type       \n#&gt;    &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;      \n#&gt;  1 Amsterdam      1 Source     \n#&gt;  2 Antwerp       76 Source     \n#&gt;  3 Dordrecht      1 Source     \n#&gt;  4 Emden          1 Source     \n#&gt;  5 Haarlem       30 Source     \n#&gt;  6 Hamburg        1 Source     \n#&gt;  7 Het Vlie       1 Source     \n#&gt;  8 Lisse          1 Source     \n#&gt;  9 Venice         2 Source     \n#&gt; 10 Bremen         6 Destination\n#&gt; 11 Delft         95 Destination\n#&gt; 12 Haarlem        8 Destination\n#&gt; 13 Middelburg     2 Destination\n#&gt; 14 The Hague      3 Destination\n\nHaving created the data for the number of letters sent from and received in each location, we now need to add the longitude and latitude of the cities. We can do this with a left join with the locations data frame, using the “place” column as the key to link the two data frames.\n\n# Join letters_data to locations\ngeo_data &lt;- left_join(letters_data, locations, by = \"place\")\n\n# Print data with longitude and latitude columns\ngeo_data\n#&gt; # A tibble: 14 × 5\n#&gt;    place          n type          lon   lat\n#&gt;    &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Amsterdam      1 Source       4.90  52.4\n#&gt;  2 Antwerp       76 Source       4.40  51.2\n#&gt;  3 Dordrecht      1 Source       4.69  51.8\n#&gt;  4 Emden          1 Source       7.21  53.4\n#&gt;  5 Haarlem       30 Source       4.65  52.4\n#&gt;  6 Hamburg        1 Source       9.99  53.6\n#&gt;  7 Het Vlie       1 Source       5.18  53.3\n#&gt;  8 Lisse          1 Source       4.56  52.3\n#&gt;  9 Venice         2 Source      12.3   45.4\n#&gt; 10 Bremen         6 Destination  8.80  53.1\n#&gt; 11 Delft         95 Destination  4.36  52.0\n#&gt; 12 Haarlem        8 Destination  4.65  52.4\n#&gt; 13 Middelburg     2 Destination  3.61  51.5\n#&gt; 14 The Hague      3 Destination  4.30  52.1\n\nThe result is a data frame with 14 rows and 5 columns that give the number of letters sent from and received in the different locations and the longitude and latitude of those places. We now have all of the information that we need to transform this data into a spatial object with both sp and sf. We will then be able to map the points on base maps provided by the rnaturalearth package."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#sp",
    "href": "post/gis-with-r-intro/index.html#sp",
    "title": "Introduction to GIS with R",
    "section": "Spatial data with sp\n",
    "text": "Spatial data with sp\n\nThe creation of a Spatial object involves providing data for the slots expected for each subclass. Creating a SpatialPoints object requires data for the coords and proj4string slots.10 The information for the coords slot can be filled by a data frame of the longitude and latitude values from geo_data. This can be done by creating a new data frame that only contains the “lon” and “lat” columns of geo_data. As noted above, the CRS for our data is implicitly longitude and latitude values on the WGS84 ellipsoid, which we can identify through the CRS() function containing a proj.4 argument. We can either specify the specific projection and datum arguments with +proj=longlat +datum=WGS84 or use the EPSG code with +init=epsg:4326.\n\n# Create data frame of only longitude and latitude values\ncoords &lt;- select(geo_data, lon, lat)\n\n# Create SpatialPoints object with coords and CRS\npoints_sp &lt;- SpatialPoints(coords = coords,\n                           proj4string = CRS(\"+proj=longlat +datum=WGS84\"))\n\nPrinting out our newly created SpatialPoints object does not make for particularly exciting reading. We see the coordinates, which are now in matrix form, and the CRS of the object. We can get a better insight into the structure of the object with the str() command, even if the output is a bit off-putting at first. This shows the three slots for the SpatialPoints object and the values contained within the slots.\n\n# Print SpatialPoints object\npoints_sp\n#&gt; SpatialPoints:\n#&gt;             lon      lat\n#&gt;  [1,]  4.895168 52.37022\n#&gt;  [2,]  4.402464 51.21945\n#&gt;  [3,]  4.690093 51.81330\n#&gt;  [4,]  7.206010 53.35940\n#&gt;  [5,]  4.646219 52.38739\n#&gt;  [6,]  9.993682 53.55108\n#&gt;  [7,]  5.183333 53.30000\n#&gt;  [8,]  4.557483 52.25793\n#&gt;  [9,] 12.315515 45.44085\n#&gt; [10,]  8.801694 53.07930\n#&gt; [11,]  4.357068 52.01158\n#&gt; [12,]  4.646219 52.38739\n#&gt; [13,]  3.610998 51.49880\n#&gt; [14,]  4.300700 52.07050\n#&gt; Coordinate Reference System (CRS) arguments: +proj=longlat +datum=WGS84\n#&gt; +no_defs\n\n# Structure of SpatialPoints object\nstr(points_sp)\n#&gt; Formal class 'SpatialPoints' [package \"sp\"] with 3 slots\n#&gt;   ..@ coords     : num [1:14, 1:2] 4.9 4.4 4.69 7.21 4.65 ...\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. .. ..$ : NULL\n#&gt;   .. .. ..$ : chr [1:2] \"lon\" \"lat\"\n#&gt;   ..@ bbox       : num [1:2, 1:2] 3.61 45.44 12.32 53.55\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. .. ..$ : chr [1:2] \"lon\" \"lat\"\n#&gt;   .. .. ..$ : chr [1:2] \"min\" \"max\"\n#&gt;   ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n#&gt;   .. .. ..@ projargs: chr \"+proj=longlat +datum=WGS84 +no_defs\"\n#&gt;   .. .. ..$ comment: chr \"GEOGCRS[\\\"unknown\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.25722\"| __truncated__\n\nWe have now successfully created a Spatial object. However, points_sp does not contain any of the non-spatial data that was created in preparing the data. We can attach this non-spatial data to the Spatial data by creating a SpatialPointsDataFrame object. This is done in the same manner as a SpatialPoints object but also includes filling the data slot with the letters_data data frame.11 Because the data in both the coords and data slots derived from the same source, the coordinates and non-spatial data will be correctly aligned by row number.\n\n# Create SpatialPointsDataFrame object\npoints_spdf &lt;- SpatialPointsDataFrame(coords = coords,\n                                      data = letters_data,  \n                                      proj4string = CRS(\"+proj=longlat +datum=WGS84\"))\n\nPrinting out points_spdf shows a result not dramatically different from printing out the geo_data data frame. However, inspecting the structure of the object shows that it has a very different composition.\n\n# Print out SpatialPointsDataFrame\npoints_spdf\n#&gt;             coordinates      place  n        type\n#&gt; 1  (4.895168, 52.37022)  Amsterdam  1      Source\n#&gt; 2  (4.402464, 51.21945)    Antwerp 76      Source\n#&gt; 3   (4.690093, 51.8133)  Dordrecht  1      Source\n#&gt; 4    (7.20601, 53.3594)      Emden  1      Source\n#&gt; 5  (4.646219, 52.38739)    Haarlem 30      Source\n#&gt; 6  (9.993682, 53.55108)    Hamburg  1      Source\n#&gt; 7      (5.183333, 53.3)   Het Vlie  1      Source\n#&gt; 8  (4.557483, 52.25793)      Lisse  1      Source\n#&gt; 9  (12.31552, 45.44085)     Venice  2      Source\n#&gt; 10  (8.801694, 53.0793)     Bremen  6 Destination\n#&gt; 11 (4.357068, 52.01158)      Delft 95 Destination\n#&gt; 12 (4.646219, 52.38739)    Haarlem  8 Destination\n#&gt; 13  (3.610998, 51.4988) Middelburg  2 Destination\n#&gt; 14    (4.3007, 52.0705)  The Hague  3 Destination\n\n# Structure of SpatialPointsDataFrame object\nstr(points_spdf)\n#&gt; Formal class 'SpatialPointsDataFrame' [package \"sp\"] with 5 slots\n#&gt;   ..@ data       : tibble [14 × 3] (S3: tbl_df/tbl/data.frame)\n#&gt;   .. ..$ place: chr [1:14] \"Amsterdam\" \"Antwerp\" \"Dordrecht\" \"Emden\" ...\n#&gt;   .. ..$ n    : int [1:14] 1 76 1 1 30 1 1 1 2 6 ...\n#&gt;   .. ..$ type : Factor w/ 2 levels \"Source\",\"Destination\": 1 1 1 1 1 1 1 1 1 2 ...\n#&gt;   ..@ coords.nrs : num(0) \n#&gt;   ..@ coords     : num [1:14, 1:2] 4.9 4.4 4.69 7.21 4.65 ...\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. .. ..$ : NULL\n#&gt;   .. .. ..$ : chr [1:2] \"lon\" \"lat\"\n#&gt;   ..@ bbox       : num [1:2, 1:2] 3.61 45.44 12.32 53.55\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. .. ..$ : chr [1:2] \"lon\" \"lat\"\n#&gt;   .. .. ..$ : chr [1:2] \"min\" \"max\"\n#&gt;   ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n#&gt;   .. .. ..@ projargs: chr \"+proj=longlat +datum=WGS84 +no_defs\"\n#&gt;   .. .. ..$ comment: chr \"GEOGCRS[\\\"unknown\\\",\\n    DATUM[\\\"World Geodetic System 1984\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.25722\"| __truncated__\n\nThe nature of a SpatialPointsDataFrame object means that they cannot be manipulated in the same way as a normal data frame. You can access the data frame as you would any of the other slots in a Spatial object with the points_spdf@data notation. However, if you want to make changes to the points_spdf through an aspect of the data slot, you cannot use dplyr commands. Instead, you need to use base R methods, which can be a bit more convoluted. For instance, if you want to get the observations of places in which Daniel either received or was sent a certain number of letters you would use @data to access the data frame, [] to subset, and $ to access the column that you want to subset. You can see an example of this below, creating a SpatialPointsDataFrame that contains the three locations in which “n” is greater than 10 and simply printing out the result to demonstrate the workflow.\n\n# Access the data frame of a SpatialPointsDataFrame\npoints_spdf@data\n#&gt; # A tibble: 14 × 3\n#&gt;    place          n type       \n#&gt;    &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;      \n#&gt;  1 Amsterdam      1 Source     \n#&gt;  2 Antwerp       76 Source     \n#&gt;  3 Dordrecht      1 Source     \n#&gt;  4 Emden          1 Source     \n#&gt;  5 Haarlem       30 Source     \n#&gt;  6 Hamburg        1 Source     \n#&gt;  7 Het Vlie       1 Source     \n#&gt;  8 Lisse          1 Source     \n#&gt;  9 Venice         2 Source     \n#&gt; 10 Bremen         6 Destination\n#&gt; 11 Delft         95 Destination\n#&gt; 12 Haarlem        8 Destination\n#&gt; 13 Middelburg     2 Destination\n#&gt; 14 The Hague      3 Destination\n\n# Example of subsetting `points_spdf` to return locations with \"n\" greater than 10\npoints_spdf[points_spdf@data$n &gt; 10, ]\n#&gt;            coordinates   place  n        type\n#&gt; 1 (4.402464, 51.21945) Antwerp 76      Source\n#&gt; 2 (4.646219, 52.38739) Haarlem 30      Source\n#&gt; 3 (4.357068, 52.01158)   Delft 95 Destination\n\nTo make a map with the SpatialPoints objects of the correspondence of Daniel, we need to get a background map of western Europe to provide geographical context. There are a number of R packages that provide access to various maps which can be read in as Spatial data. Here, I will make use of the rnaturalearth package to get access to a map of the coastlines of the world and another for the countries in the world.12 This will create two different kinds of Spatial objects. Because both maps contain non-spatial attribute data, they will be of class SpatialLinesDataFrame and SpatialpolygonsDataFrame.\n\n# Get coastal and country world maps as Spatial objects\ncoast_sp &lt;- ne_coastline(scale = \"medium\")\ncountries_sp &lt;- ne_countries(scale = \"medium\")\n\nNot only are coast_sp and countries_sp useful for mapping the points data, they also provide an opportunity to look at the structure of more complex Spatial objects. Printing out either object fills up the entire console with an overflow of information. Instead, we can get an idea of the structure of the two objects with the str() command. To ensure that we do not overloaded with information with this command, I will only show the first two levels, which shows information about the slots but not on the data within the slots.\n\n# Structure of SpatialLinesDataFrame\nstr(coast_sp, max.level = 2)\n#&gt; Formal class 'SpatialLinesDataFrame' [package \"sp\"] with 4 slots\n#&gt;   ..@ data       :'data.frame':  1428 obs. of  2 variables:\n#&gt;   ..@ lines      :List of 1428\n#&gt;   ..@ bbox       : num [1:2, 1:2] -180 -85.2 180 83.6\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n\n#Structure of SpatialPolygonsDataFrame\nstr(countries_sp, max.level = 2)\n#&gt; Formal class 'SpatialPolygonsDataFrame' [package \"sp\"] with 5 slots\n#&gt;   ..@ data       :'data.frame':  241 obs. of  63 variables:\n#&gt;   ..@ polygons   :List of 241\n#&gt;   ..@ plotOrder  : int [1:241] 12 184 39 227 42 33 16 87 113 9 ...\n#&gt;   ..@ bbox       : num [1:2, 1:2] -180 -90 180 83.6\n#&gt;   .. ..- attr(*, \"dimnames\")=List of 2\n#&gt;   ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n#&gt;   ..$ comment: chr \"TRUE\"\n\nThe output from str() shows that coast_sp and countries_sp are mainly distinguished by the presence of a lines or a polygons slot. Both of the objects contain data frames, but they are very different. The data frame in coast_sp only contains two columns, but countries_sp has 63 columns. This is because the countries map from rnaturalearth contains a variety of population, economic, and labeling data. We do not need to deal with the data slots from either of these objects, but they do provide a good example of what these kind of Spatial objects look like.\nThe last thing that we should do before plotting the points on our background maps is to make sure that our objects all have the same CRS. If points_spdf and coast_sp have different CRSs, the points will not be correctly plotted. We can check this with either the proj4string() function or printing out the proj4string slot. Checking this shows that all of the Spatial objects are using geographic coordinates on the WSG84 ellipsoid, enabling us to move forward with plotting.\n\n# Check CRS of Spatial objects\ncoast_sp@proj4string\n#&gt; Coordinate Reference System:\n#&gt; Deprecated Proj.4 representation:\n#&gt;  +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\nproj4string(countries_sp)\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\"\nproj4string(points_spdf)\n#&gt; [1] \"+proj=longlat +datum=WGS84 +no_defs\""
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#mapping-sp",
    "href": "post/gis-with-r-intro/index.html#mapping-sp",
    "title": "Introduction to GIS with R",
    "section": "Mapping with sp\n",
    "text": "Mapping with sp\n\nJust as Spatial objects cannot be manipulated with dplyr commands, they cannot be plotted using ggplot2. Spatial objects can be plotted using the base R plot system or with its own plotting method that uses the Trellis plotting system with the spplot() command. Here, I will stick to the base plotting methods, but if you want to look at spplot() methods, Edzer Pebesma has created a good gallery of sp plots. For a more in depth discussion on both plotting methods see Chapter 3 of Bivand, Pebesma, and Gómez-Rubio, Applied Spatial Data Analysis with R. I will discuss creating a map with the base R plotting system in some detail, because the commands are not particularly intuitive, and I have found it more difficult to find good information about making base plots compared to the use of ggplot2.\nThe plotting method for base R uses incremental commands to add different layers, allowing you to plot multiple types of data and add annotations such as legends and titles. The process can be somewhat tedious, and if you decide to change an aspect of the plot you need to rerun the entire string of commands. The necessity of building up every level of the plot with explicit commands has advantages and disadvantages when compared to ggplot2, which automates certain aspects of plot creation. For instance, base plots do not automate the creation of legends in the same way that ggplot2 does. This makes the creation of legends more difficult, but because you have to construct each piece of the legend, its production can be more transparent than with ggplot2.\nBase plotting methods enable you to manipulate a wide variety of parameters about your plots. You can see the whole list with ?par. For this post, I will concentrate on some of the more widely used parameters to plot the points data we have created above on two different background maps. With these maps I want to visualize three different types of data: the location of the points represented in points_spdf@coords, whether the point represents a source or destination of the letters from the “type” column, and a representation of the amount of letters sent or received in the location from the “n” column. The latter two types of data can be represented through color and the size of the points respectively. Let’s see how this works in practice.\nEven before beginning to plot our Spatial objects, I want to set up the plotting environment by modifying the default margins to a smaller size that will work for all of these plots. The margins of a plot are identified by the mar or mai arguments within par(). Here, I will use mar, which provides the margins in units of lines. To change the margins we need to pass a vector of values for the bottom, left, top, and right margins. In this case, I will use par(mar = c(1, 1, 3, 1))  to give some minimal margins and some room for a title at the top of the plot.\nThe location of the points of a SpatialPoints object are plotted automatically with plot(points_spdf). However, the default for plotting SpatialPoints is to represent the points by an addition sign. We can change the symbol to represent the points with pch or the plotting character parameter. There are 25 available characters, and here I will use pch = 20, which is a solid circle.\nModifying the color and size of these circles is a bit trickier. The color of the circles can be adjusted with the col parameter according to the information in points_spdf$type. We are able to map the colors of the points to the “type” variable, because we earlier changed the column to be factors instead of characters. If this had not been done, the plot() command would fail. The default colors for the first two levels of factors are black and red, which are not particularly pleasant to look at. However, we can create a new color palette with the palette() function. Here, I am using the named colors “darkorchid” and “darkorange”, while also adding some transparency to the points with the alpha() function from ggplot2 since some of the points overlap.13 You can get a list of all of the named colors in R with colors(). The palette has to be created before beginning to make the plot, and this will change the color palette for all further base plots. You can return to the default palette with palette(\"default\").\nThe cex parameter is used to modify the size of the points that will be plotted. We can set the size of the points to points_spdf$n, but first we need to make some modifications. We do not want the point sizes to be the actual value in the “n” column. A point with cex = 95 — the maximum number of letters in the data — would create a point larger then the plot. Therefore, we need to adjust the minimum and maximum size of the points according to some formula. This could be done in a variety of ways, but here I choose the formula sqrt(points_spdf$n)/2 + 0.25, which reduces the minimum cex to 0.75 and the maximum to a bit over 5. The other additions that I make are to add a box around the plot with box() and a title with title() in separate commands. These two commands decorate the plot and provides and example of the way that layers can be added to a base plot. Let’s see how this looks.\n\n# Create a new color palette to distinguish source and destination\npalette(alpha(c(\"darkorchid\", \"darkorange\"), 0.7))\n# Set margins for bottom, left, top, and right of plot\npar(mar = c(1, 1, 3, 1))\n\n# Plot points\nplot(points_spdf,\n     pch = 20,\n     col = points_spdf$type,\n     cex = sqrt(points_spdf$n)/2 + 0.25)\n\n# Add a box around the plot\nbox()\n# Add a title\ntitle(main = \"Correspondence of Daniel van der Meulen, 1585\")\n\n\n\n\n\n\n\nOk, we have points, but we do not have a map. We can add a background map with a separate call to the plot() function that contains add = TRUE. One nice aspect of using the base plotting methods with Spatial objects is that even though the coastline map we are using is a world map, adding it to our previous plot properly scales the map to the geographical extent of the data from the first plotting function. The only other change is to make the color of the coastlines black with the col parameter. This needs to be done, because we changed the color palette above. If we did not make this change, the coastlines would be drawn in dark orchid.\nThe final step in making this map is to create legends to show the meaning of the colors and size of the points. This will require making two legends, which can be done with two calls to the legend() function. It is a bit fiddly to make these legends and build up every aspect, but the basis for what needs to be done is already present in the plot() functions. The legend() function takes a position for where to place the legend and the data to use for it. It is then a matter of using parameters to create informative legends.\nCreating a legend for the distinction between sources and destinations uses data from the levels of the factors in the type column, which can be represented by levels(points_spdf$type). We can map the colors to the levels with a vector containing the two levels, and here I use the shortcut of 1:2 to represent the two colors in the palette we created above. The pch parameter provides a symbol to represent the colors. In this case, I decided to use a filled square. The pt.cex parameter is used to increase the size of the colored squares, which we need to use because the cex parameter within legend() determines the size of the entire legend.\nThe legend to relate the size of the points to the number of letters sent from or received in each location is a bit more difficult to create, as we do not yet have all the necessary components for the legend. A legend for the size of the points will possess three or four points of different sizes and the number of letters they represent. ggplot2 does this creation automatically, but with base plots you have to choose the number and size of the points yourself. This means we need to make our own data for the legend, which I do here by creating a pointsize vector that will be used to show the size of points representing 1, 50, and 100 letters.14 Once this is done, the creation of the legend is fairly straightforward. The key is to remember to make the point sizes for our vector of numbers equal to the points on the map by applying the same formula and using the same pch. A title for the legend makes it clearer that the map represents letters.15\n\n# Pointsize vector for legend\npointsize &lt;- c(1, 50, 100)\npar(mar = c(1, 1, 3, 1))\n\n# Plot points\nplot(points_spdf,\n     pch = 20,\n     col = points_spdf$type,\n     cex = sqrt(points_spdf$n)/2 + 0.25)\n# Plot coastlines background map\nplot(coast_sp,\n     col = \"black\",\n     add = TRUE)\n# Add a box around the plot\nbox()\n\n# Legend for colors\nlegend(\"topright\", legend = levels(points_spdf$type),\n       pt.cex = 2,\n       col = 1:2,\n       pch = 15)\n\n# legend for size of points\nlegend(\"right\", legend = pointsize,\n       pt.cex = (sqrt(pointsize)/2 + 0.25),\n       col = \"black\",\n       pch = 20,\n       title = \"Letters\")\n\n# Title for the map\ntitle(main = \"Correspondence of Daniel van der Meulen, 1585\")\n\n\n\n\n\n\n\nThe resulting map is maybe a bit sparse, but it provides all the necessary information on the locations and magnitude of Daniel’s correspondence in 1585. One change that we might want to make is to color in the land to distinguish more clearly between land and water and to add some color to the plot. This cannot be done with our SpatialLines of the coastlines, which does not have a geometric interior that could be filled. Instead we need to use a SpatialPolygons object such as countries_sp. To make this second map we can reuse many of the parameters from our first map, but we do need to make one significant change to the plotting workflow. Because we want to fill in the land area of the map, we need to plot the background map before plotting the points. Otherwise the background map will cover the points. However, this disturbs the automatic subsetting of the world map that occurred in the first plot. We can get around this by changing the bounding box of countries_sp (countries_sp@bbox) to match that of points_spdf with the bbox() function.\n\n# Make bounding box for countries_sp match\n# bounding box of points_spdf\ncountries_sp@bbox &lt;- bbox(points_spdf)\n\nOnce the countries map is properly limited to the extent of the points, we can reuse most of the parameters from the first map. The only difference from the previous plot other than placing the background map first in the plotting order is the choice of colors for countries_sp. For a SpatialPolygons object the col parameter adjusts the fill color for the polygons, and the border parameter modifies the color of the lines of the polygons. Using a map with modern country boundaries is clearly anachronistic for plotting letters from 1585, but the boundaries do help to contextualize the locations. If you want to obscure the country borders completely, you can make the col and border parameters the same color. In this case, I will use the gray() function, which enables you to choose a gray level from 0 to 1 with 0 representing black and 1 for white.\n\npar(mar = c(1, 1, 3, 1))\n\n# Plot countries map and color with grays\nplot(countries_sp,\n     col = gray(0.8),\n     border = gray(0.7))\n# Plot points\nplot(points_spdf,\n     pch = 20,\n     col = points_spdf$type, \n     cex = sqrt(points_spdf$n)/2 + 0.25,\n     add = TRUE)\n# Add a box around the plot\nbox()\n\n# Legend for colors\nlegend(\"topright\",\n       legend = levels(points_spdf$type),\n       pt.cex = 2,\n       col = 1:2,\n       pch = 15)\n# legend for size of points\nlegend(\"right\",\n       legend = pointsize,\n       pt.cex = (sqrt(pointsize)/2 + 0.25),\n       col = \"black\",\n       pch = 20,\n       title = \"Letters\")\n\n# Title for the map\ntitle(main = \"Correspondence of Daniel van der Meulen, 1585\")\n\n\n\n\n\n\n\nThe above maps along with the creation of Spatial objects and the use of maps from outside sources demonstrates the value of the sp package. The above examples only show the basics of working with different kinds of Spatial objects and does not take advantage of any of the spatial transformations or calculations that using sp enables. However, the R GIS community is increasingly moving towards the sf package. The sf package provides almost all of the capabilities of sp, but it uses objects that are easier to work with than the S4-style classes of sp. The next section will replicate the workflow of creating and mapping spatial points data using sf methods, which will serve to illuminate the differences between the two packages."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#sf",
    "href": "post/gis-with-r-intro/index.html#sf",
    "title": "Introduction to GIS with R",
    "section": "Spatial data with sf\n",
    "text": "Spatial data with sf\n\nAs I noted above, an sf object is a data-frame-like object that contains a special geometry column that is of class sfc. The geometry column stores the spatial forms of data such as the CRS, coordinates, and type of geometric object. The creation of an sf object with a geometry column of class sfc_POINT is similar to creating a SpatialPointsDataFrame, but it can be done in a single step with a data frame that contains longitude and latitude columns. The st_as_sf() function uses a vector of the coordinate columns for the coords argument and either a EPSG code or a Proj.4 definition for the crs.\n\n# Create sf object with geo_data data frame and CRS\npoints_sf &lt;- st_as_sf(geo_data, coords = c(\"lon\", \"lat\"), crs = 4326)\n\nThe class() function shows that points_sf is an sf object that is an extension of a data.frame, and in this case is an extension of tbl_df, because geo_data is a tibble. Printing points_sf results in an output similar to that of a tibble or data frame with additional information about the spatial aspects of the data provided by the “geometry” column. The information about the geometry shows the type of geometrical object, dimension, bounding box, and the CRS in both EPSG and proj.4 format when possible.\n\n# Getting the class of an sf object shows that it is based on tibble and data frame.\nclass(points_sf)\n#&gt; [1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# Printing out an sf object is similar to tibble or data frame\npoints_sf\n#&gt; Simple feature collection with 14 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 14 × 4\n#&gt;    place          n type                   geometry\n#&gt;  * &lt;chr&gt;      &lt;int&gt; &lt;fct&gt;               &lt;POINT [°]&gt;\n#&gt;  1 Amsterdam      1 Source      (4.895168 52.37022)\n#&gt;  2 Antwerp       76 Source      (4.402464 51.21945)\n#&gt;  3 Dordrecht      1 Source       (4.690093 51.8133)\n#&gt;  4 Emden          1 Source        (7.20601 53.3594)\n#&gt;  5 Haarlem       30 Source      (4.646219 52.38739)\n#&gt;  6 Hamburg        1 Source      (9.993682 53.55108)\n#&gt;  7 Het Vlie       1 Source          (5.183333 53.3)\n#&gt;  8 Lisse          1 Source      (4.557483 52.25793)\n#&gt;  9 Venice         2 Source      (12.31552 45.44085)\n#&gt; 10 Bremen         6 Destination  (8.801694 53.0793)\n#&gt; 11 Delft         95 Destination (4.357068 52.01158)\n#&gt; 12 Haarlem        8 Destination (4.646219 52.38739)\n#&gt; 13 Middelburg     2 Destination  (3.610998 51.4988)\n#&gt; 14 The Hague      3 Destination    (4.3007 52.0705)\n\nYou can inspect the class of the geometry column itself by using $ to subset the geometry column. Or you can directly access information about the spatial data in the “geometry” column with the st_geometry() function.\n\n# Class of the geometry or sfc column\nclass(points_sf$geometry)\n#&gt; [1] \"sfc_POINT\" \"sfc\"\n\n# Retrieve the geometry of an sf object\n# to see coordinates, type of feature, and CRS\nst_geometry(points_sf)\n#&gt; Geometry set for 14 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 3.610998 ymin: 45.44085 xmax: 12.31552 ymax: 53.55108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; POINT (4.895168 52.37022)\n#&gt; POINT (4.402464 51.21945)\n#&gt; POINT (4.690093 51.8133)\n#&gt; POINT (7.20601 53.3594)\n#&gt; POINT (4.646219 52.38739)\n\nHaving already loaded the Natural Earth coastline and countries maps as Spatial objects, there are two possibilities for getting the same data as sf objects. The sf package provides a method for converting from Spatial classes to sf with the st_as_sf() function that was used above to create an sf object from a data frame. It is useful to know that you can easily convert an object from Spatial to sf and back, but you will obviously not always have Spatial objects already created. The rnaturalearth package enables you to create sf objects with the returnclass = \"sf\" argument. Here, I will use the latter method.16\n\n# Get coastal and country world maps as sf objects\ncoast_sf &lt;- ne_coastline(scale = \"medium\", returnclass = \"sf\")\ncountries_sf &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nThe data-frame-like nature of sf objects makes working with spatial data more transparent and in-line with other data workflows compared to the sp package. For starters, it is more reasonable to print out sf objects to the console, though the size of coast_sf and countries_sf makes this less than ideal. However, a big advantage of sf over sp is that you can easily peruse the contents of an object with the View() command or clicking on the object in the Environment pane in RStudio. With the View() command an sf object looks like any other data frame instead of a list of lists like a Spatial object. This makes it much easier to get an overview of the data that you are working with. Let’s print out the first six rows, or features in the sf vocabulary, of coast_sf with the head() function to give an idea of what the object looks like. The structure of the output is similar to that of points_sf with a data frame containing a geometry column and information about the geometry of the object. In this case, we can see that coast_sf is of class MULTILINESTRING.\n\n# Print first six rows of coast_sf\nhead(coast_sf)\n#&gt; Simple feature collection with 6 features and 2 fields\n#&gt; Geometry type: MULTILINESTRING\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -81.42168 ymin: -17.16387 xmax: 180 ymax: 24.94111\n#&gt; CRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n#&gt;   scalerank featurecla                       geometry\n#&gt; 0         0  Coastline MULTILINESTRING ((180 -16.1...\n#&gt; 1         0  Coastline MULTILINESTRING ((177.2575 ...\n#&gt; 2         0  Coastline MULTILINESTRING ((127.3727 ...\n#&gt; 3         0  Coastline MULTILINESTRING ((-81.32231...\n#&gt; 4         0  Coastline MULTILINESTRING ((-80.79941...\n#&gt; 5         0  Coastline MULTILINESTRING ((-80.62568...\n\nUnlike Spatial objects that use slots to distinguish the classes, all sf objects can generally be treated as data frames when wrangling the data. The only difference in the structure of points_sf compared to coast_sf or countries_sf is the class of the geometry column. This makes it possible to use workflows from the tidyverse. For instance, returning the subset of locations from points_sf that have a “n” greater than 10, as I showed above with points_spdf, can be done with dplyr’s filter() function.\n\n# Subset of locations with \"n\" greater than 10\nfilter(points_sf, n &gt; 10)\n#&gt; Simple feature collection with 3 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 4.357068 ymin: 51.21945 xmax: 4.646219 ymax: 52.38739\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 3 × 4\n#&gt;   place       n type                   geometry\n#&gt; * &lt;chr&gt;   &lt;int&gt; &lt;fct&gt;               &lt;POINT [°]&gt;\n#&gt; 1 Antwerp    76 Source      (4.402464 51.21945)\n#&gt; 2 Haarlem    30 Source      (4.646219 52.38739)\n#&gt; 3 Delft      95 Destination (4.357068 52.01158)\n\nThe advantages of using dplyr to manipulate spatial objects is more apparent when working with complex objects such as countries_sf and using the pipe (%&gt;%) to link commands. For example, if you want to look at the countries in South America, you can much more easily navigate the 241 rows and 63 columns of non-spatial data in the sf version of the countries world map than the sp version. Printing out the names of the columns with colnames() shows that we can filter countries in South America through the “continent” column. We can simplify the non-spatial data by selecting only the “name” column to identify the countries. Even more impressively, it is possible to combine data wrangling commands with spatial transformations such as changing the CRS of the object with st_transform(). As a demonstration, I can change the CRS of the subset of countries_sf to use the Mollweide projection, which prioritizes accuracy of area over shape and angle. The result of the pipeline of commands is a legible sf object with 13 rows for the countries in South America with a new CRS. Note that because there is no EPSG code for the Mollweide projection the epsg of the new object is NA, but the proj4string is changed to +proj=moll. Notice too that the coordinates have been changed from degrees to meters.\n\n# South American countries with new CRS\ncountries_sf %&gt;% \n  filter(continent == \"South America\") %&gt;% \n  select(name) %&gt;% \n  st_transform(crs = \"+proj=moll +datum=WGS84\")\n#&gt; Simple feature collection with 13 features and 1 field\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -10204810 ymin: -6475765 xmax: -3470373 ymax: 1532816\n#&gt; CRS:           +proj=moll +datum=WGS84\n#&gt; First 10 features:\n#&gt;            name                       geometry\n#&gt; 1     Argentina MULTIPOLYGON (((-4588849 -6...\n#&gt; 2       Bolivia MULTIPOLYGON (((-5605494 -2...\n#&gt; 3        Brazil MULTIPOLYGON (((-4505022 -3...\n#&gt; 4         Chile MULTIPOLYGON (((-4714647 -6...\n#&gt; 5      Colombia MULTIPOLYGON (((-7824020 31...\n#&gt; 6       Ecuador MULTIPOLYGON (((-8024339 -3...\n#&gt; 7  Falkland Is. MULTIPOLYGON (((-4403206 -6...\n#&gt; 8        Guyana MULTIPOLYGON (((-5715634 68...\n#&gt; 9          Peru MULTIPOLYGON (((-7000354 -5...\n#&gt; 10     Paraguay MULTIPOLYGON (((-5605494 -2...\n\nWe can even visualize this transformation of countries_sf by piping directly to a base plot() command. Like sp, sf has its own plotting methods, which differ slightly from the base plotting system.17 The changes to the defaults that I make here are to remove the automatically created legend of country names with key.pos = NULL and provide a title for the plot. I also add graticules, or grid lines along longitude and latitude, to more clearly show the effects of the change in projection. Before making the map, we need to return the color palette to its default settings to prevent the borders of the countries from being drawn in dark orchid.\n\n# Return to default palette\npalette(\"default\")\n\n# Map of South American countries\ncountries_sf %&gt;% \n  filter(continent == \"South America\") %&gt;% \n  select(name) %&gt;% \n  st_transform(crs = \"+proj=moll +datum=WGS84\") %&gt;% \n  plot(key.pos = NULL, graticule = TRUE, main = \"South America\")"
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#mapping-sf",
    "href": "post/gis-with-r-intro/index.html#mapping-sf",
    "title": "Introduction to GIS with R",
    "section": "Mapping with sf and ggplot2\n",
    "text": "Mapping with sf and ggplot2\n\nReturning to the example of the correspondence of Daniel van der Meulen, it would be possible to use sf to recreate the base R plots made with the sp package. However, another advantage of sf is that it works with ggplot2, and so I will concentrate on making maps using the ggplot2 system similar to those I made with ggmap in a previous post. ggplot2 implements the plotting of sf objects through the creation of a specific geom, geom_sf(). geom_sf() is only available in versions of ggplot2 that are greater than 2.2.1, which is the current CRAN version. Therefore, you need to download the development version of ggplot2 with devtools::install_github(\"tidyverse/ggplot2\") to plot sf objects.\nThe manner in which ggplot2 creates a plot through a single command rather than a series of separate commands as in base plots means that we need to explicitly deal with the incongruence in the geographic extent of points_sf and the background maps. If you make a ggplot2 plot with points_sf and coast_sf without modifications, you will get a wold map with some small points in western Europe. There are two ways to deal with this incongruence by either changing the bounding box of the background map before making the plot or within the plot itself. As far as I know, there is no equivalent method in the sf package to the command we used above to modify the bbox slot of the countries_sp object. However, it is possible to geographically subset an sf object by a bounding box of coordinates using the ms_clip() function from the rmapshaper package. The other option is to leave coast_sf and countries_sf as they are and adjust the geographic extent of the objects in the plot command itself with the coord_sf() function and the xlim and ylim arguments. Here, I will use the latter method.\nThere are some oddities in using geom_sf() compared to other geoms due to the character of geom_sf(). By its nature, geom_sf() is unlike other geoms in that it can be used to create points, lines, or polygons depending on the contents of the sf object. This variability results in the need to declare the geometrical object used in a show.legend argument for the legend to be properly formatted. Another difference that I have found from other geoms is that geom_sf() does not properly identify sf objects unless you explicitly identify them with data = sf_object. While these issues are something to be aware of, they do not substantively affect the the plots that we can create.\nLet’s start by creating a simple plot using ggplot2 with points_sf and coast_sf. Note the explicit vocabulary to identify the sf objects as the data and the use of show.legend = \"point\" for the legend. Otherwise the plot recreates the basic structure of the above base R plots by mapping the aesthetics of color to the “type” variable and size to “n”. Some transparency is added to all of the points with the alpha argument. The last change is to subset the geographic region of the plot, and thus of coast_sf to a chosen bounding box with coord_sf(). You can experiment with different values for xlim and ylim to see what works best.\n\nggplot() + \n  geom_sf(data = coast_sf) + \n  geom_sf(data = points_sf,\n          aes(color = type, size = n),\n          alpha = 0.7,\n          show.legend = \"point\") +\n  coord_sf(xlim = c(-1, 14), ylim = c(44, 55))\n\n\n\n\n\n\n\nThe map created by ggplot2 shows the same information as the base plots. It includes legends by default so that we do not need to build them up from scratch, though with geom_sf() we do need to use show.legend = \"point\" to format the legends properly. The aesthetics of this map are quite different from the base plots and serves to show the defaults that come with the use of geom_sf() in ggplot2. The defaults for geom_sf() use the iconic ggplot gray background, but instead of the usual white grid lines, graticules for the longitude and latitude are shown. Notice that the tick marks for the plot are correctly labeled in units of degrees longitude and latitude and that the axes are not labeled, as the context makes clear that the x-axis is latitude and the y-axis is longitude — at least if you remember which one is which.\nThe basic structure of what we want is present in the above map, but we can improve upon it by adding some information and making aesthetic changes to the map. We can simplify the map and make it more similar to the above base R plots by removing the graticules, which can be done by passing NA to the datum argument within the coord_sf() function. On the other hand, it is possible to add information to the plot by labeling the points with geom_text_repel() from the ggrepel package, which will ensure that the labels for the points do not overlap. geom_text_repel() does not work with sf objects, but because we have made no geographic transformations to the data, we can use the data from the locations data frame to map the names of the locations to the longitude and latitude values.\nFurther changes to the non-data aspects of the map can be made through the theme system used by ggplot2. We can alter the look of the map by choosing a different built in theme, changing individual elements of the aesthetics, or a combination of the two. You can play around with the theming system to create the look you want. In this case, I will make changes to the theme and to individual aspects of the plot. Changing the theme to theme_light() changes the background to white, but it also adds axes labels, which we will want to remove. The labels for the axes, legends, and plot as a whole can be modified within the labs() function. To remove the labeling of the axes for the plot I set the x and y labels to NULL. I also change the labels for the legends to give a more descriptive name to the size legend and use title case for the color legend. The last change I make is to increase the size of the points within the color legend with the guides() function, increasing the size of the points to 6.\n\n# Load ggrepel package\nlibrary(ggrepel)\n\nggplot() + \n  geom_sf(data = coast_sf) + \n  geom_sf(data = points_sf, \n          aes(color = type, size = n), \n          alpha = 0.7, \n          show.legend = \"point\") +\n  coord_sf(xlim = c(-1, 14), ylim = c(44, 55),\n           datum = NA) + # removes graticules\n  geom_text_repel(data = locations, \n                  aes(x = lon, y = lat, label = place)) +\n  labs(title = \"Correspondence of Daniel van der Meulen, 1585\",\n       size = \"Letters\",\n       color = \"Type\",\n       x = NULL,\n       y = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 6))) +\n  theme_light()\n\n\n\n\n\n\n\nThe result is a map that is much cleaner, and because of the points are labeled, more informative. We can reuse the thematic tweaks to create a plot with the countries background map to make it possible to fill in the land area as we did above. I will use the same gray scale for the colors to fill the polygons and for the lines of the polygons. The other change that I make in this plot is to use the theme_bw(), which adds a border around the plot.\n\nggplot() + \n  geom_sf(data = countries_sf,\n          fill = gray(0.8), color = gray(0.7)) + \n  geom_sf(data = points_sf, \n          aes(color = type, size = n), \n          alpha = 0.7, \n          show.legend = \"point\") +\n  coord_sf(xlim = c(-1, 14), ylim = c(44, 55),\n           datum = NA) + # removes graticules\n  geom_text_repel(data = locations, \n                  aes(x = lon, y = lat, label = place)) +\n  labs(title = \"Correspondence of Daniel van der Meulen, 1585\",\n       size = \"Letters\",\n       color = \"Type\",\n       x = NULL,\n       y = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 6))) +\n  theme_light()"
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#conclusion",
    "href": "post/gis-with-r-intro/index.html#conclusion",
    "title": "Introduction to GIS with R",
    "section": "Conclusion",
    "text": "Conclusion\nThis post has only scraped the surface of the power of the sp and sf packages and the more general GIS capabilities of R. My goal with this post has been to provide an understanding of the nature of spatial data, how sp and sf implement the creation of spatial objects in R, and some of the consequences of the implementation details for modifying and plotting Spatial and sf objects. sp and its complimentary packages have long provided a foundation to make R a viable GIS platform. The recent development of the sf package has modernized the implementation of spatial data in R and made it possible to integrate spatial data into the tidyverse and ggplot2 plotting system. sf has made it easier to work with spatial data in R by minimizing the distinction between spatial data and other forms of data you might deal with in R. There still remain uses for the sp package, and I think it is helpful to see the distinction between the two packages to better understand both, but sf is clearly the future for the representation of geographic vector data in R."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#gis-resources",
    "href": "post/gis-with-r-intro/index.html#gis-resources",
    "title": "Introduction to GIS with R",
    "section": "Additional resources for GIS with R",
    "text": "Additional resources for GIS with R\n\nThe CRAN task view for Analysis of Spatial Data provides a good overview of the variety of packages that deal with GIS.\n\nsp - The best resource for learning more about the sp package is Roger Bivand, Edzer Pebesma, and Virgilio Gómez-Rubio, Applied Spatial Data Analysis with R (2013). - There are a number of good tutorials on working with Spatial objects and a particularly useful resource is Nick Eubank’s, GIS in R.\n\nsf - The package vignettes for sf are very helpful for providing an introduction to the package. - The currently in development book Geocomputation with R by Robin Lovelace, Jakub Nowosad, Jannes Muenchow is already an invaluable resource and was of great use for writing this post. - The R Spatial blog is a good place to keep up on developments of sf and complimentary packages that have adopted the use of sf objects."
  },
  {
    "objectID": "post/gis-with-r-intro/index.html#footnotes",
    "href": "post/gis-with-r-intro/index.html#footnotes",
    "title": "Introduction to GIS with R",
    "section": "Footnotes",
    "text": "Footnotes\n\nEdzer Pebesma, Daniel Nüst, and Roger Bivand, “The R Software Environment in Reproducible Geoscientific Research,” Eos 93 (2012): 163–164.↩︎\nFor a good general introduction to the use and history of GIS with R, see the working book Robin Lovelace, Jakub Nowosad, Jannes Muenchow, Geocomputation with R.↩︎\nE. J. Pebesma and R. S. Bivand, “Classes and methods for spatial data in R,” R News 5, no. 2 (2005): 9–13.↩︎\nFor more information on CRSs see Lovelace, Nowosad, and Muenchow, Geocomputation with R and Bivand, Pebesma, and Gómez-Rubio, 84–91.↩︎\nThe distance between degrees of longitude and latitude are only equal at the equator. Lines of longitude get progressively closer together as they approach the poles where they all meet. This variance means you cannot calculate distance between points with longitude and latitude values without the use of complex geometric models.↩︎\nThe details of how sp and sf interface with the Proj.4 library are different, but this should not be something that most users will have to worry about.  See the discussion in sf issue 280.↩︎\nSee chapter 2 of Bivand, Edzer Pebesma, and Virgilio Gómez-Rubio for a more in depth discussion of Spatial objects.↩︎\nSee the vignette 1. Simple Features for R and the very good discussion of sf objects in chapter 2 of Lovelace, Nowosad, and Muenchow, Geocomputation with R.↩︎\nOn factors see chapter 15 of Garrett Grolemund and Hadley Wickham, R for Data Science and Amelia McNamara and Nicholas J Horton, “Wrangling Categorical Data in R”.↩︎\nThe bbox slot will be created from the information provided by the other two slots.↩︎\nI could use the geo_data data frame, but this would unnecessarily result in having the coordinates in both the data and coords slots.↩︎\nYou may need to download these maps the first time that you use them.↩︎\nInstead of choosing your own colors, you could use any of the number of R packages that provide color palettes such as RColorBrewer, but for this example I will keep it simple and make my own palette.↩︎\nMake sure to create the pointsize vector before beginning the chain of plot() functions.↩︎\nI rerun the par() command for the margins to ensure that the margins stay consistent across these plots.↩︎\nThe sf package has the ability to read in and convert a number of spatial data types used in GIS with st_read(). See the vignette on reading and writing simple features.↩︎\nThe details of plotting sf objects are covered in a package vignette.↩︎"
  },
  {
    "objectID": "post/network-analysis-with-r/index.html",
    "href": "post/network-analysis-with-r/index.html",
    "title": "Introduction to Network Analysis with R",
    "section": "",
    "text": "Over a wide range of fields network analysis has become an increasingly popular tool for scholars to deal with the complexity of the interrelationships between actors of all sorts. The promise of network analysis is the placement of significance on the relationships between actors, rather than seeing actors as isolated entities. The emphasis on complexity, along with the creation of a variety of algorithms to measure various aspects of networks, makes network analysis a central tool for digital humanities.1 This post will provide an introduction to working with networks in R, using the example of the network of cities in the correspondence of Daniel van der Meulen in 1585.\nThere are a number of applications designed for network analysis and the creation of network graphs such as gephi and cytoscape. Though not specifically designed for it, R has developed into a powerful tool for network analysis. The strength of R in comparison to stand-alone network analysis software is three fold. In the first place, R enables reproducible research that is not possible with GUI applications. Secondly, the data analysis power of R provides robust tools for manipulating data to prepare it for network analysis. Finally, there is an ever growing range of packages designed to make R a complete network analysis tool. Significant network analysis packages for R include the statnet suite of packages and igraph. In addition, Thomas Lin Pedersen has recently released the tidygraph and ggraph packages that leverage the power of igraph in a manner consistent with the tidyverse workflow. R can also be used to make interactive network graphs with the htmlwidgets framework that translates R code to JavaScript.\nThis post begins with a short introduction to the basic vocabulary of network analysis, followed by a discussion of the process for getting data into the proper structure for network analysis. The network analysis packages have all implemented their own object classes. In this post, I will show how to create the specific object classes for the statnet suite of packages with the network package, as well as for igraph and tidygraph, which is based on the igraph implementation. Finally, I will turn to the creation of interactive graphs with the vizNetwork and networkD3 packages."
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#network-analysis-nodes-and-edges",
    "href": "post/network-analysis-with-r/index.html#network-analysis-nodes-and-edges",
    "title": "Introduction to Network Analysis with R",
    "section": "Network Analysis: Nodes and Edges",
    "text": "Network Analysis: Nodes and Edges\nThe two primary aspects of networks are a multitude of separate entities and the connections between them. The vocabulary can be a bit technical and even inconsistent between different disciplines, packages, and software. The entities are referred to as nodes or vertices of a graph, while the connections are edges or links. In this post I will mainly use the nomenclature of nodes and edges except when discussing packages that use different vocabulary.\nThe network analysis packages need data to be in a particular form to create the special type of object used by each package. The object classes for network, igraph, and tidygraph are all based on adjacency matrices, also known as sociomatrices.2 An adjacency matrix is a square matrix in which the column and row names are the nodes of the network. Within the matrix a 1 indicates that there is a connection between the nodes, and a 0 indicates no connection. Adjacency matrices implement a very different data structure than data frames and do not fit within the tidyverse workflow that I have used in my previous posts. Helpfully, the specialized network objects can also be created from an edge-list data frame, which do fit in the tidyverse workflow. In this post I will stick to the data analysis techniques of the tidyverse to create edge lists, which will then be converted to the specific object classes for network, igraph, and tidygraph.\nAn edge list is a data frame that contains a minimum of two columns, one column of nodes that are the source of a connection and another column of nodes that are the target of the connection. The nodes in the data are identified by unique IDs. If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. With the example of letters sent between cities, the distinction between source and target is clearly meaningful, and so the network is directed. For the examples below, I will name the source column as “from” and the target column as “to”. I will use integers beginning with one as node IDs.3 An edge list can also contain additional columns that describe attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\nEdge lists contain all of the information necessary to create network objects, but sometimes it is preferable to also create a separate node list. At its simplest, a node list is a data frame with a single column — which I will label as “id” — that lists the node IDs found in the edge list. The advantage of creating a separate node list is the ability to add attribute columns to the data frame such as the names of the nodes or any kind of groupings. Below I give an example of minimal edge and node lists created with the tibble() function.\n\nlibrary(tidyverse)\nedge_list &lt;- tibble(from = c(1, 2, 2, 3, 4), to = c(2, 3, 4, 2, 1))\nnode_list &lt;- tibble(id = 1:4)\n\nedge_list\n#&gt; # A tibble: 5 × 2\n#&gt;    from    to\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     2\n#&gt; 2     2     3\n#&gt; 3     2     4\n#&gt; 4     3     2\n#&gt; 5     4     1\nnode_list\n#&gt; # A tibble: 4 × 1\n#&gt;      id\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n#&gt; 4     4\n\nCompare this to an adjacency matrix with the same data.\n\n#&gt;   1 2 3 4\n#&gt; 1 0 1 0 0\n#&gt; 2 0 0 1 1\n#&gt; 3 0 1 0 0\n#&gt; 4 1 0 0 0"
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#creating-edge-and-node-lists",
    "href": "post/network-analysis-with-r/index.html#creating-edge-and-node-lists",
    "title": "Introduction to Network Analysis with R",
    "section": "Creating edge and node lists",
    "text": "Creating edge and node lists\nTo create network objects from the database of letters received by Daniel van der Meulen in 1585 I will make both an edge list and a node list. This will necessitate the use of the dplyr package to manipulate the data frame of letters sent to Daniel and split it into two data frames or tibbles with the structure of edge and node lists. In this case, the nodes will be the cities from which Daniel’s correspondents sent him letters and the cities in which he received them. The node list will contain a “label” column, containing the names of the cities. The edge list will also have an attribute column that will show the amount of letters sent between each pair of cities. The workflow to create these objects will be similar to that I have used in my brief introduction to R and in geocoding with R. If you would like to follow along, you can find the data used in this post and the R script used on GitHub.\nThe first step is to load the tidyverse library to import and manipulate the data. Printing out the letters data frame shows that it contains four columns: “writer”, “source”, “destination”, and “date”. In this example, we will only deal with the “source” and “destination” columns.\n\nlibrary(tidyverse)\nletters &lt;- read_csv(\"data/correspondence-data-1585.csv\")\n\nletters\n#&gt; # A tibble: 114 × 4\n#&gt;    writer                  source  destination date      \n#&gt;    &lt;chr&gt;                   &lt;chr&gt;   &lt;chr&gt;       &lt;date&gt;    \n#&gt;  1 Meulen, Andries van der Antwerp Delft       1585-01-03\n#&gt;  2 Meulen, Andries van der Antwerp Haarlem     1585-01-09\n#&gt;  3 Meulen, Andries van der Antwerp Haarlem     1585-01-11\n#&gt;  4 Meulen, Andries van der Antwerp Delft       1585-01-12\n#&gt;  5 Meulen, Andries van der Antwerp Haarlem     1585-01-12\n#&gt;  6 Meulen, Andries van der Antwerp Delft       1585-01-17\n#&gt;  7 Meulen, Andries van der Antwerp Delft       1585-01-22\n#&gt;  8 Meulen, Andries van der Antwerp Delft       1585-01-23\n#&gt;  9 Della Faille, Marten    Antwerp Haarlem     1585-01-24\n#&gt; 10 Meulen, Andries van der Antwerp Delft       1585-01-28\n#&gt; # … with 104 more rows\n\nNode list\nThe workflow to create a node list is similar to the one I used to get the list of cities in order to geocode the data in a previous post. We want to get the distinct cities from both the “source” and “destination” columns and then join the information from these columns together. In the example below, I slightly change the commands from those I used in the previous post to have the name for the columns with the city names be the same for both the sources and destinations data frames to simplify the full_join() function. I rename the column with the city names as “label” to adopt the vocabulary used by network analysis packages.\n\nsources &lt;- letters %&gt;%\n  distinct(source) %&gt;%\n  rename(label = source)\n\ndestinations &lt;- letters %&gt;%\n  distinct(destination) %&gt;%\n  rename(label = destination)\n\nTo create a single dataframe with a column with the unique locations we need to use a full join, because we want to include all unique places from both the sources of the letters and the destinations.\n\nnodes &lt;- full_join(sources, destinations, by = \"label\")\nnodes\n#&gt; # A tibble: 13 × 1\n#&gt;    label     \n#&gt;    &lt;chr&gt;     \n#&gt;  1 Antwerp   \n#&gt;  2 Haarlem   \n#&gt;  3 Dordrecht \n#&gt;  4 Venice    \n#&gt;  5 Lisse     \n#&gt;  6 Het Vlie  \n#&gt;  7 Hamburg   \n#&gt;  8 Emden     \n#&gt;  9 Amsterdam \n#&gt; 10 Delft     \n#&gt; 11 The Hague \n#&gt; 12 Middelburg\n#&gt; 13 Bremen\n\nThis results in a data frame with one variable. However, the variable contained in the data frame is not really what we are looking for. The “label” column contains the names of the nodes, but we also want to have unique IDs for each city. We can do this by adding an “id” column to the nodes data frame that contains numbers from one to whatever the total number of rows in the data frame is. A helpful function for this workflow is rowid_to_column(), which adds a column with the values from the row ids and places the column at the start of the data frame.4 Note that rowid_to_column() is a pipeable command, and so it is possible to do the full_join() and add the “id” column in a single command. The result is a nodes list with an ID column and a label attribute.\n\nnodes &lt;- rowid_to_column(nodes, \"id\")\nnodes\n#&gt; # A tibble: 13 × 2\n#&gt;       id label     \n#&gt;    &lt;int&gt; &lt;chr&gt;     \n#&gt;  1     1 Antwerp   \n#&gt;  2     2 Haarlem   \n#&gt;  3     3 Dordrecht \n#&gt;  4     4 Venice    \n#&gt;  5     5 Lisse     \n#&gt;  6     6 Het Vlie  \n#&gt;  7     7 Hamburg   \n#&gt;  8     8 Emden     \n#&gt;  9     9 Amsterdam \n#&gt; 10    10 Delft     \n#&gt; 11    11 The Hague \n#&gt; 12    12 Middelburg\n#&gt; 13    13 Bremen\n\nEdge list\nCreating an edge list is similar to the above, but it is complicated by the need to deal with two ID columns instead of one. We also want to create a weight column that will note the amount of letters sent between each set of nodes. To accomplish this I will use the same group_by() and summarise() workflow that I have discussed in previous posts. The difference here is that we want to group the data frame by two columns — “source” and “destination” — instead of just one. Previously, I have named the column that counts the number of observations per group “count”, but here I adopt the nomenclature of network analysis and call it “weight”. The final command in the pipeline removes the grouping for the data frame instituted by the group_by() function. This makes it easier to manipulate the resulting per_route data frame unhindered.5\n\nper_route &lt;- letters %&gt;%  \n  group_by(source, destination) %&gt;%\n  summarise(weight = n(), .groups = \"drop\")\nper_route\n#&gt; # A tibble: 15 × 3\n#&gt;    source    destination weight\n#&gt;    &lt;chr&gt;     &lt;chr&gt;        &lt;int&gt;\n#&gt;  1 Amsterdam Bremen           1\n#&gt;  2 Antwerp   Delft           68\n#&gt;  3 Antwerp   Haarlem          5\n#&gt;  4 Antwerp   Middelburg       1\n#&gt;  5 Antwerp   The Hague        2\n#&gt;  6 Dordrecht Haarlem          1\n#&gt;  7 Emden     Bremen           1\n#&gt;  8 Haarlem   Bremen           2\n#&gt;  9 Haarlem   Delft           26\n#&gt; 10 Haarlem   Middelburg       1\n#&gt; 11 Haarlem   The Hague        1\n#&gt; 12 Hamburg   Bremen           1\n#&gt; 13 Het Vlie  Bremen           1\n#&gt; 14 Lisse     Delft            1\n#&gt; 15 Venice    Haarlem          2\n\nLike the node list, per_route now has the basic form that we want, but we again have the problem that the “source” and “destination” columns contain labels rather than IDs. What we need to do is link the IDs that have been assigned in nodes to each location in both the “source” and “destination” columns. This can be accomplished with another join function. In fact, it is necessary to perform two joins, one for the “source” column and one for “destination.” In this case, I will use a left_join() with per_route as the left data frame, because we want to maintain the number of rows in per_route. While doing the left_join, we also want to rename the two “id” columns that are brought over from nodes. For the join using the “source” column I will rename the column as “from”. The column brought over from the “destination” join is renamed “to”. It would be possible to do both joins in a single command with the use of the pipe. However, for clarity, I will perform the joins in two separate commands. Because the join is done across two commands, notice that the data frame at the beginning of the pipeline changes from per_route to edges, which is created by the first command.\n\nedges &lt;- per_route %&gt;% \n  left_join(nodes, by = c(\"source\" = \"label\")) %&gt;% \n  rename(from = id)\n\nedges &lt;- edges %&gt;% \n  left_join(nodes, by = c(\"destination\" = \"label\")) %&gt;% \n  rename(to = id)\n\nNow that edges has “from” and “to” columns with node IDs, we need to reorder the columns to bring “from” and “to” to the left of the data frame. Currently, the edges data frame still contains the “source” and “destination” columns with the names of the cities that correspond with the IDs. However, this data is superfluous, since it is already present in nodes. Therefore, I will only include the “from”, “to”, and “weight” columns in the select() function.\n\nedges &lt;- select(edges, from, to, weight)\nedges\n#&gt; # A tibble: 15 × 3\n#&gt;     from    to weight\n#&gt;    &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt;  1     9    13      1\n#&gt;  2     1    10     68\n#&gt;  3     1     2      5\n#&gt;  4     1    12      1\n#&gt;  5     1    11      2\n#&gt;  6     3     2      1\n#&gt;  7     8    13      1\n#&gt;  8     2    13      2\n#&gt;  9     2    10     26\n#&gt; 10     2    12      1\n#&gt; 11     2    11      1\n#&gt; 12     7    13      1\n#&gt; 13     6    13      1\n#&gt; 14     5    10      1\n#&gt; 15     4     2      2\n\nThe edges data frame does not look very impressive; it is three columns of integers. However, edges combined with nodes provides us with all of the information necessary to create network objects with the network, igraph, and tidygraph packages."
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#creating-network-objects",
    "href": "post/network-analysis-with-r/index.html#creating-network-objects",
    "title": "Introduction to Network Analysis with R",
    "section": "Creating network objects",
    "text": "Creating network objects\nThe network object classes for network, igraph, and tidygraph are all closely related. It is possible to translate between a network object and an igraph object. However, it is best to keep the two packages and their objects separate. In fact, the capabilities of network and igraph overlap to such an extent that it is best practice to have only one of the packages loaded at a time. I will begin by going over the network package and then move to the igraph and tidygraph packages.\nnetwork\n\nlibrary(network)\n\nThe function used to create a network object is network(). The command is not particularly straight forward, but you can always enter ?network() into the console if you get confused. The first argument is — as stated in the documentation — “a matrix giving the network structure in adjacency, incidence, or edgelist form.” The language demonstrates the significance of matrices in network analysis, but instead of a matrix, we have an edge list, which fills the same role. The second argument is a list of vertex attributes, which corresponds to the nodes list. Notice that the network package uses the nomenclature of vertices instead of nodes. The same is true of igraph. We then need to specify the type of data that has been entered into the first two arguments by specifying that the matrix.type is an \"edgelist\". Finally, we set ignore.eval to FALSE so that our network can be weighted and take into account the number of letters along each route.\n\nroutes_network &lt;- network(edges,\n                          vertex.attr = nodes,\n                          matrix.type = \"edgelist\",\n                          ignore.eval = FALSE)\n\nYou can see the type of object created by the network() function by placing routes_network in the class() function.\n\nclass(routes_network)\n#&gt; [1] \"network\"\n\nPrinting out routes_network to the console shows that the structure of the object is quite different from data-frame style objects such as edges and nodes. The print command reveals information that is specifically defined for network analysis. It shows that there are 13 vertices or nodes and 15 edges in routes_network. These numbers correspond to the number of rows in nodes and edges respectively. We can also see that the vertices and edges both contain attributes such as label and weight. You can get even more information, including a sociomatrix of the data, by entering summary(routes_network).\n\nroutes_network\n#&gt;  Network attributes:\n#&gt;   vertices = 13 \n#&gt;   directed = TRUE \n#&gt;   hyper = FALSE \n#&gt;   loops = FALSE \n#&gt;   multiple = FALSE \n#&gt;   bipartite = FALSE \n#&gt;   total edges= 15 \n#&gt;     missing edges= 0 \n#&gt;     non-missing edges= 15 \n#&gt; \n#&gt;  Vertex attribute names: \n#&gt;     id label vertex.names \n#&gt; \n#&gt;  Edge attribute names: \n#&gt;     weight\n\nIt is now possible to get a rudimentary, if not overly aesthetically pleasing, graph of our network of letters. Both the network and igraph packages use the base plotting system of R. The conventions for base plots are significantly different from those of ggplot2 — which I have discussed in previous posts — and so I will stick with rather simple plots instead of going into the details of creating complex plots with base R. In this case, the only change that I make to the default plot() function for the network package is to increase the size of nodes with the vertex.cex argument to make the nodes more visible. Even with this very simple graph, we can already learn something about the data. The graph makes clear that there are two main groupings or clusters of the data, which correspond to the time Daniel spent in Holland in the first three-quarters of 1585 and after his move to Bremen in September.\n\nplot(routes_network, vertex.cex = 3)\n\n\n\n\n\n\n\nThe plot() function with a network object uses the Fruchterman and Reingold algorithm to decide on the placement of the nodes.6 You can change the layout algorithm with the mode argument. Below, I layout the nodes in a circle. This is not a particularly useful arrangement for this network, but it gives an idea of some of the options available.\n\nplot(routes_network, vertex.cex = 3, mode = \"circle\")\n\n\n\n\n\n\n\nigraph\nLet’s now move on to discuss the igraph package. First, we need to clean up the environment in R by removing the network package so that it does not interfere with the igraph commands. We might as well also remove routes_network since we will not longer be using it. The network package can be removed with the detach() function, and routes_network is removed with rm().7 After this, we can safely load igraph.\n\n# Detatch network and load igraph\ndetach(package:network)\nrm(routes_network)\nlibrary(igraph)\n\nTo create an igraph object from an edge-list data frame we can use the graph_from_data_frame() function, which is a bit more straight forward than network(). There are three arguments in the graph_from_data_frame() function: d, vertices, and directed. Here, d refers to the edge list, vertices to the node list, and directed can be either TRUE or FALSE depending on whether the data is directed or undirected.\n\nroutes_igraph &lt;- graph_from_data_frame(d = edges,\n                                       vertices = nodes,\n                                       directed = TRUE)\n\nPrinting the igraph object created by graph_from_data_frame() to the console reveals similar information to that from a network object, though the structure is more cryptic.\n\nroutes_igraph\n#&gt; IGRAPH 743b80e DNW- 13 15 -- \n#&gt; + attr: name (v/c), label (v/c), weight (e/n)\n#&gt; + edges from 743b80e (vertex names):\n#&gt;  [1] 9-&gt;13 1-&gt;10 1-&gt;2  1-&gt;12 1-&gt;11 3-&gt;2  8-&gt;13 2-&gt;13 2-&gt;10 2-&gt;12 2-&gt;11 7-&gt;13\n#&gt; [13] 6-&gt;13 5-&gt;10 4-&gt;2\n\nThe main information about the object is contained in DNW- 13 15 --. This tells that routes_igraph is a directed network (D) that has a name attribute (N) and is weighted (W). The dash after W tells us that the graph is not bipartite. The numbers that follow describe the number of nodes and edges in the graph respectively. Next, name (v/c), label (v/c), weight (e/n) gives information about the attributes of the graph. There are two vertex attributes (v/c) of name — which are the IDs — and labels and an edge attribute (e/n) of weight. Finally, there is a print out of all of the edges.\nJust as with the network package, we can create a plot with an igraph object through the plot() function. The only change that I make to the default here is to decrease the size of the arrows. By default igraph labels the nodes with the label column if there is one or with the IDs.\n\nplot(routes_igraph,\n     vertex.size = 30,\n     vertex.label.cex = 0.5,\n     edge.arrow.size = 0.8)\n\n\n\n\n\n\n\nLike the network graph before, the default of an igraph plot is not particularly aesthetically pleasing, but all aspects of the plots can be manipulated. Here, I just want to change the layout of the nodes to use the graphopt algorithm created by Michael Schmuhl. This algorithm makes it easier to see the relationship between Haarlem, Antwerp, and Delft, which are three of the most signifiant locations in the correspondence network, by spreading them out further.\n\nplot(routes_igraph,\n     layout = layout_with_graphopt,\n     vertex.size = 30,\n     vertex.label.cex = 0.5,\n     edge.arrow.size = 0.8)\n\n\n\n\n\n\n\ntidygraph and ggraph\nThe tidygraph and ggraph packages are newcomers to the network analysis landscape, but together the two packages provide real advantages over the network and igraph packages. tidygraph and ggraph represent an attempt to bring network analysis into the tidyverse workflow. tidygraph provides a way to create a network object that more closely resembles a tibble or data frame. This makes it possible to use many of the dplyr functions to manipulate network data. ggraph gives a way to plot network graphs using the conventions and power of ggplot2. In other words, tidygraph and ggraph allow you to deal with network objects in a manner that is more consistent with the commands used for working with tibbles and data frames. However, the true promise of tidygraph and ggraph is that they leverage the power of igraph. This means that you sacrifice few of the network analysis capabilities of igraph by using tidygraph and ggraph.\nWe need to start as always by loading the necessary packages.\n\nlibrary(tidygraph)\nlibrary(ggraph)\n\nFirst, let’s create a network object using tidygraph, which is called a tbl_graph. A tbl_graph consists of two tibbles: an edges tibble and a nodes tibble. Conveniently, the tbl_graph object class is a wrapper around an igraph object, meaning that at its basis a tbl_graph object is essentially an igraph object.8 The close link between tbl_graph and igraph objects results in two main ways to create a tbl_graph object. The first is to use an edge list and node list, using tbl_graph(). The arguments for the function are almost identical to those of graph_from_data_frame() with only a slight change to the names of the arguments.\n\nroutes_tidy &lt;- tbl_graph(nodes = nodes,\n                         edges = edges,\n                         directed = TRUE)\n\nThe second way to create a tbl_graph object is to convert an igraph or network object using as_tbl_graph(). Thus, we could convert routes_igraph to a tbl_graph object.\n\nroutes_igraph_tidy &lt;- as_tbl_graph(routes_igraph)\n\nNow that we have created two tbl_graph objects, let’s inspect them with the class() function. This shows that routes_tidy and routes_igraph_tidy are objects of class \"tbl_graph\" \"igraph\", while routes_igraph is object class \"igraph\".\n\nclass(routes_tidy)\n#&gt; [1] \"tbl_graph\" \"igraph\"\nclass(routes_igraph_tidy)\n#&gt; [1] \"tbl_graph\" \"igraph\"\nclass(routes_igraph)\n#&gt; [1] \"igraph\"\n\nPrinting out a tbl_graph object to the console results in a drastically different output from that of an igraph object. It is an output similar to that of a normal tibble.\n\nroutes_tidy\n#&gt; # A tbl_graph: 13 nodes and 15 edges\n#&gt; #\n#&gt; # A directed acyclic simple graph with 1 component\n#&gt; #\n#&gt; # Node Data: 13 × 2 (active)\n#&gt;      id label    \n#&gt;   &lt;int&gt; &lt;chr&gt;    \n#&gt; 1     1 Antwerp  \n#&gt; 2     2 Haarlem  \n#&gt; 3     3 Dordrecht\n#&gt; 4     4 Venice   \n#&gt; 5     5 Lisse    \n#&gt; 6     6 Het Vlie \n#&gt; # … with 7 more rows\n#&gt; #\n#&gt; # Edge Data: 15 × 3\n#&gt;    from    to weight\n#&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1     9    13      1\n#&gt; 2     1    10     68\n#&gt; 3     1     2      5\n#&gt; # … with 12 more rows\n\nPrinting routes_tidy shows that it is a tbl_graph object with 13 nodes and 15 edges. The command also prints the first six rows of “Node Data” and the first three of “Edge Data”. Notice too that it states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time. The nodes tibble is activated by default, but you can change which tibble is active with the activate() function. Thus, if I wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, I could use activate() and then arrange(). Here I simply print out the result rather than saving it.\n\nroutes_tidy %&gt;% \n  activate(edges) %&gt;% \n  arrange(desc(weight))\n#&gt; # A tbl_graph: 13 nodes and 15 edges\n#&gt; #\n#&gt; # A directed acyclic simple graph with 1 component\n#&gt; #\n#&gt; # Edge Data: 15 × 3 (active)\n#&gt;    from    to weight\n#&gt;   &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n#&gt; 1     1    10     68\n#&gt; 2     2    10     26\n#&gt; 3     1     2      5\n#&gt; 4     1    11      2\n#&gt; 5     2    13      2\n#&gt; 6     4     2      2\n#&gt; # … with 9 more rows\n#&gt; #\n#&gt; # Node Data: 13 × 2\n#&gt;      id label    \n#&gt;   &lt;int&gt; &lt;chr&gt;    \n#&gt; 1     1 Antwerp  \n#&gt; 2     2 Haarlem  \n#&gt; 3     3 Dordrecht\n#&gt; # … with 10 more rows\n\nSince we do not need to further manipulate routes_tidy, we can plot the graph with ggraph. Like ggmap, ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the creation of network plots. As in all network graphs, there are three main aspects to a ggraph plot: nodes, edges, and layouts. The vignettes for the ggraph package helpfully cover the fundamental aspects of ggraph plots. ggraph adds special geoms to the basic set of ggplot geoms that are specifically designed for networks. Thus, there is a set of geom_node and geom_edge geoms. The basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object. In addition, the available layouts algorithms primarily derive from igraph. Lastly, ggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. The ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots. Here, I will use the latter method.\nLet’s see what a basic ggraph plot looks like. The plot begins with ggraph() and the data. I then add basic edge and node geoms. No arguments are necessary within the edge and node geoms, because they take the information from the data provided in ggraph().\n\nggraph(routes_tidy, layout = \"auto\") + \n  geom_edge_link() + \n  geom_node_point() + \n  theme_graph()\n#&gt; Using \"sugiyama\" as default layout\n#&gt; Warning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\n\n\nAs you can see, the structure of the command is similar to that of ggplot with the separate layers added with the + sign. The basic ggraph plot looks similar to those of network and igraph, if not even plainer, but we can use similar commands to ggplot to create a more informative graph. We can show the “weight” of the edges — or the amount of letters sent along each route — by using width in the geom_edge_link() function. To get the width of the line to change according to the weight variable, we place the argument within an aes() function. In order to control the maximum and minimum width of the edges, I use scale_edge_width() and set a range. I choose a relatively small width for the minimum, because there is a significant difference between the maximum and minimum number of letters sent along the routes. We can also label the nodes with the names of the locations since there are relatively few nodes. Conveniently, geom_node_text() comes with a repel argument that ensures that the labels do not overlap with the nodes in a manner similar to the ggrepel package. I add a bit of transparency to the edges with the alpha argument. I also use labs() to relabel the legend “Letters”.\n\nggraph(routes_tidy, layout = \"graphopt\") + \n  geom_node_point() +\n  geom_edge_link(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_text(aes(label = label), repel = TRUE) +\n  labs(edge_width = \"Letters\") +\n  theme_graph()\n\n\n\n\n\n\n\nIn addition to the layout choices provided by igraph, ggraph also implements its own layouts. For example, you can use ggraph's concept of circularity to create arc diagrams. Here, I layout the nodes in a horizontal line and have the edges drawn as arcs. Unlike the previous plot, this graph indicates directionality of the edges.9 The edges above the horizontal line move from left to right, while the edges below the line move from right to left. Intsead of adding points for the nodes, I just include the label names. I use the same width aesthetic to denote the difference in the weight of each edge. Note that in this plot I use an igraph object as the data for the graph, which makes no practical difference.\n\nggraph(routes_igraph, layout = \"linear\") + \n  geom_edge_arc(aes(width = weight), alpha = 0.8) + \n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_text(aes(label = label)) +\n  labs(edge_width = \"Letters\") +\n  theme_graph()"
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#interactive-network-graphs-with-visnetwork-and-networkd3",
    "href": "post/network-analysis-with-r/index.html#interactive-network-graphs-with-visnetwork-and-networkd3",
    "title": "Introduction to Network Analysis with R",
    "section": "Interactive network graphs with visNetwork and networkD3\n",
    "text": "Interactive network graphs with visNetwork and networkD3\n\nThe htmlwidgets set of packages makes it possible to use R to create interactive JavaScript visualizations. Here, I will show how to make graphs with the visNetwork and networkD3 packages. These two packages use different JavaScript libraries to create their graphs. visNetwork uses vis.js, while networkD3 uses the popular d3 visualization library to make its graphs. One difficulty in working with both visNetwork and networkD3 is that they expect edge lists and node lists to use specific nomenclature. The above data manipulation conforms to the basic structure for visNetwork, but some work will need to be done for networkD3. Despite this inconvenience, both packages possess a wide range of graphing capabilities and both can work with igraph objects and layouts.\n\nlibrary(visNetwork)\nlibrary(networkD3)\n\nvisNetwork\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list. The resulting graph is fun to play around with. You can move the nodes and the graph will use an algorithm to keep the nodes properly spaced. You can also zoom in and out on the plot and move it around to re-center it.\n\nvisNetwork(nodes, edges)\n\n\n\n\n\nvisNetwork can use igraph layouts, providing a large variety of possible layouts. In addition, you can use visIgraph() to plot an igraph object directly. Here, I will stick with the nodes and edges workflow and use an igraph layout to customize the graph. I will also add a variable to change the width of the edge as we did with ggraph. visNetwork() uses column names from the edge and node lists to plot network attributes instead of arguments within the function call. This means that it is necessary to do some data manipulation to get a “width” column in the edge list. The width attribute for visNetwork() does not scale the values, so we have to do this manually. Both of these actions can be done with the mutate() function and some simple arithmetic. Here, I create a new column in edges and scale the weight values by dividing by 5. Adding 1 to the result provides a way to create a minimum width.\n\nedges &lt;- mutate(edges, width = weight/5 + 1)\n\nOnce this is done, we can create a graph with variable edge widths. I also choose a layout algorithm from igraph and add arrows to the edges, placing them in the middle of the edge.\n\nvisNetwork(nodes, edges) %&gt;% \n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;% \n  visEdges(arrows = \"middle\")\n\n\n\n\n\nnetworkD3\nA little more work is necessary to prepare the data to create a networkD3 graph. To make a networkD3 graph with a edge and node list requires that the IDs be a series of numeric integers that begin with 0. Currently, the node IDs for our data begin with 1, and so we have to do a bit of data manipulation. It is possible to renumber the nodes by subtracting 1 from the ID columns in the nodes and edges data frames. Once again, this can be done with the mutate() function. The goal is to recreate the current columns, while subtracting 1 from each ID. The mutate() function works by creating a new column, but we can have it replace a column by giving the new column the same name as the old column. Here, I name the new data frames with a d3 suffix to distinguish them from the previous nodes and edges data frames.\n\nnodes_d3 &lt;- mutate(nodes, id = id - 1)\nedges_d3 &lt;- mutate(edges, from = from - 1, to = to - 1)\n\nIt is now possible to plot a networkD3 graph. Unlike visNetwork(), the forceNetwork() function uses a series of arguments to adjust the graph and plot network attributes. The “Links” and “Nodes” arguments provide the data for the plot in the form of edge and node lists. The function also requires “NodeID” and “Group” arguments. The data being used here does not have any groupings, and so I just have each node be its own group, which in practice means that the nodes will all be different colors. In addition, the below tells the function that the network has “Source” and “Target” fields, and thus is directed. I include in this graph a “Value”, which scales the width of the edges according to the “weight” column in the edge list. Finally, I add some aesthetic tweaks to make the nodes opaque and increase the font size of the labels to improve legibility. The result is very similar to the first visNetwork() plot that I created but with different aesthetic stylings.\n\nforceNetwork(Links = edges_d3, Nodes = nodes_d3,\n             Source = \"from\", Target = \"to\", \n             NodeID = \"label\", Group = \"id\", Value = \"weight\", \n             opacity = 1, fontSize = 16, zoom = TRUE)\n\n\n\n\n\nOne of the main benefits of networkD3 is that it implements a d3-styled Sankey diagram. A Sankey diagram is a good fit for the letters sent to Daniel in 1585. There are not too many nodes in the data, making it easier to visualize the flow of letters. Creating a Sankey diagram uses the sankeyNetwork() function, which takes many of the same arguments as forceNetwork(). This graph does not require a group argument, and the only other change is the addition of a “unit.” This provides a label for the values that pop up in a tool tip when your cursor hovers over a diagram element.10\n\nsankeyNetwork(Links = edges_d3, Nodes = nodes_d3, Source = \"from\", Target = \"to\", \n              NodeID = \"label\", Value = \"weight\", fontSize = 16, unit = \"Letter(s)\")"
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#resource-on-network-analysis",
    "href": "post/network-analysis-with-r/index.html#resource-on-network-analysis",
    "title": "Introduction to Network Analysis with R",
    "section": "Resource on network analysis",
    "text": "Resource on network analysis\nThis post has attempted to give a general introduction to creating and plotting network type objects in R using the network, igraph, tidygraph, and ggraph packages for static plots and visNetwork and networkD3 for interactive plots. I have presented this information from the position of a non-specialist in network theory. I have only covered a very small percentage of the network analysis capabilities of R. In particular, I have not discussed the statistical analysis of networks. Happily, there is a plethora of resources on network analysis in general and in R in particular.\nThe best introduction to networks that I have found for the uninitiated is Katya Ognyanova’s Network Visualization with R. This presents both a helpful introduction to the visual aspects of networks and a more in depth tutorial on creating network plots in R. Ognyanova primarily uses igraph, but she also introduces interactive networks.\nThere are two relatively recent books published on network analysis with R by Springer. Douglas A. Luke, A User’s Guide to Network Analysis in R (2015) is a very useful introduction to network analysis with R. Luke covers both the statnet suit of packages and igragh. The contents are at a very approachable level throughout. More advanced is Eric D. Kolaczyk and Gábor Csárdi’s, Statistical Analysis of Network Data with R (2014). Kolaczyk and Csárdi’s book mainly uses igraph, as Csárdi is the primary maintainer of the igraph package for R. This book gets further into advanced topics on the statistical analysis of networks. Despite the use of very technical language, the first four chapters are generally approachable from a non-specialist point of view.\nThe list curated by François Briatte is a good overview of resources on network analysis in general. The Networks Demystified series of posts by Scott Weingart is also well worth perusal."
  },
  {
    "objectID": "post/network-analysis-with-r/index.html#footnotes",
    "href": "post/network-analysis-with-r/index.html#footnotes",
    "title": "Introduction to Network Analysis with R",
    "section": "Footnotes",
    "text": "Footnotes\n\nOne example of the interest in network analysis within digital humanities is the newly launched Journal of Historical Network Research.↩︎\nFor a good description of the network object class, including a discussion of its relationship to the igraph object class, see Carter Butts, “network: A Package for Managing Relational Data in R”, Journal of Statistical Software, 24 (2008): 1–36↩︎\nThis is the specific structure expected by visNetwork, while also conforming to the general expectations of the other packages.↩︎\nThis is the expected order for the columns for some of the networking packages that I will be using below.↩︎\nungroup() is not strictly necessary in this case. However, if you do not ungroup the data frame, it is not possible to drop the “source” and “destination” columns, as I do later in the script.↩︎\nThomas M. J. Fruchterman and Edward M. Reingold, “Graph Drawing by Force-Directed Placement,” Software: Practice and Experience, 21 (1991): 1129–1164.↩︎\nThe rm() function is useful if your working environment in R gets disorganized, but you do not want to clear the whole environment and start over again.↩︎\nThe relationship between tbl_graph and igraph objects is similar to that between tibble and data.frame objects.↩︎\nIt is possible to have ggraph draw arrows, but I have not shown that here.↩︎\nIt can take a bit of time for the tool tip to appear.↩︎"
  },
  {
    "objectID": "post/debkeepr-cran/index.html",
    "href": "post/debkeepr-cran/index.html",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "",
    "text": "debkeepr is now—finally—on CRAN with version 0.1.1! debkeepr integrates non-decimal currencies that use tripartite or tetrapartite systems into the methodologies of Digital Humanities and the practices of reproducible research. The package makes it possible for historical non-decimal currencies, such as the tripartite system of pounds, shillings, and pence (£465 12s. 8d.), to behave like decimalized values through the implementation of the deb_lsd, deb_tetra, and deb_decimal vector types. These types are based on the infrastructure provided by the vctrs package. The name of the package derives from its ability to analyze historical account books that used the system of Double-entry bookkeeping.\nYou can now install the released version of debkeepr from CRAN or the development version on GitHub:\n# Install CRAN release\ninstall.packages(\"debkeepr\")\n\n# Install development version\n# install.packages(\"pak\")\npak::pak(\"jessesadler/debkeepr\")\nPlease open an issue if you have any questions, comments, or requests.\nThis release has been a long time coming. I first started to develop debkeepr in 2018 to deal with the problem of performing mathematical operations on non-decimal currencies of pounds, shillings, and pence that was crucial to my research on merchant families in the 16th and 17th centuries. I introduced the package with this blog post in September 2018. After talking to Hadley Wickham at the Tidyverse developer day at RStudio::conf 2019, I completely rewrote the package, using the vctrs package to develop fully integrated S3 vector types that could represent non-decimal currencies. I discussed this process at RStudio::conf 2020. Since then, I have been developing the package on and off, while also using it for my historical research.1 But I never quite got around to putting the package on CRAN. However, progress was made. I kept up with changes to vctrs and dplyr and added a new class for tetrapartite values: pounds, shillings, pence, and farthings. The forthcoming second edition of the R Packages book by Hadley Wickham and Jenny Bryan and the recently updated chapter on Releasing to CRAN helped motivate me to finally prep and submit the package for CRAN release. On 22 March 2023 version 0.1.1 of debkeepr was accepted on CRAN after one quite minor round of changes."
  },
  {
    "objectID": "post/debkeepr-cran/index.html#usage",
    "href": "post/debkeepr-cran/index.html#usage",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "Usage",
    "text": "Usage\nThe debkeepr package introduces three new vector types—deb_lsd(), deb_tetra(), and deb_decimal()—to deal with two interrelated problems inherent in historical non-decimal currencies and other such value systems.\n\nHistorical currencies consist of three or four separate non-decimal units.\nThe bases of the shillings, pence, and optionally farthing units differed by region, coinage, and era.\n\nThe deb_lsd type maintains the tripartite structure of non-decimal currencies and provides a bases attribute to record the bases for the shillings and pence units. The deb_tetra type extends the concept of the deb_lsd type to incorporate currencies and other types of values that consist of four units. The deb_decimal type provides a means to decimalize both deb_lsd and deb_tetra types, while keeping track of the two or three non-decimal bases and the unit represented as attributes.2\n\nlibrary(debkeepr)\n\n# Create deb_lsd, deb_tetra, and deb_decimal vectors\n# with default bases of 20s. 12d.\n\n# deb_lsd vector: £8 13s. 4d.\ndeb_lsd(l = 8, s = 13, d = 4)\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 8:13s:4d\n#&gt; # Bases: 20s 12d\n\n# deb_tetra vector: £8 13s. 4d. 3f.\ndeb_tetra(l = 8, s = 13, d = 4, f = 3)\n#&gt; &lt;deb_tetra[1]&gt;\n#&gt; [1] 8:13s:4d:3f\n#&gt; # Bases: 20s 12d 4f\n\n# deb_decimal: £8 13s. 4d. in decimalized shillings\ndeb_decimal(173.33, unit = \"s\")\n#&gt; &lt;deb_decimal[1]&gt;\n#&gt; [1] 173.33\n#&gt; # Unit: solidus\n#&gt; # Bases: 20s 12d\n\n# Cast £8 13s. 4d. to a decimal form\ndeb_as_decimal(deb_lsd(8, 13, 4))\n#&gt; &lt;deb_decimal[1]&gt;\n#&gt; [1] 8.666667\n#&gt; # Unit: libra\n#&gt; # Bases: 20s 12d"
  },
  {
    "objectID": "post/debkeepr-cran/index.html#resources",
    "href": "post/debkeepr-cran/index.html#resources",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "Resources",
    "text": "Resources\ndebkeepr provides three vignettes and example data to help get started using the package.\n\n\nGetting Started with debkeepr vignette provides a thorough introduction to the package and the three debkeepr vector types.\n\nTransactions in Richard Dafforne’s Journal vignette goes through examples of financial and arithmetic calculations dealing with various currencies taken from the practice journal in Richard Dafforne’s Merchant’s Mirrour (1660), a 17th-century textbook for learning accounting practices.\n\nAnalysis of Richard Dafforne’s Journal and Ledger vignette analyzes and visualizes the practice double-entry bookkeeping ledger from Dafforne’s Merchant’s Mirrour as found in the dafforne_transactions and dafforne_accounts data provided in debkeepr.\n\nThe rest of this post will provide a quick overview of how debkeepr can assist historians in research involving non-decimal currencies and bring the study of historical account books into the practices of reproducible research."
  },
  {
    "objectID": "post/debkeepr-cran/index.html#arithmetic-with-non-decimal-currencies",
    "href": "post/debkeepr-cran/index.html#arithmetic-with-non-decimal-currencies",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "Arithmetic with non-decimal currencies",
    "text": "Arithmetic with non-decimal currencies\nArithmetic with non-decimal currencies is cumbersome and error-prone, at least if you were not educated within non-decimal systems. The primary issue is the need to do compound unit arithmetic to normalize units according to their bases.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: A scratchpad from the archive of Daniel van der Meulen\n\n\n\n\n\n\n\n\n\nFigure 2: Compound unit arithmetic\n\n\n\n\n\ndebkeepr types provide a way to access arithmetic operations and mathematical functions for non-decimal values, drastically simplifying these calculations. For instance, we can add together the values in Figure 2 with either + or sum().\n\n# Add multiple deb_lsd vectors\ndeb_lsd(28, 15, 8) + deb_lsd(32, 8, 11) + deb_lsd(54, 18, 7) + deb_lsd(18, 12, 9)\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 134:15s:11d\n#&gt; # Bases: 20s 12d\n\n# Sum of one deb_lsd vector\nsum(\n  deb_lsd(l = c(28, 32, 54, 18),\n          s = c(15, 8, 18, 12),\n          d = c(8, 11, 7, 9))\n)\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 134:15s:11d\n#&gt; # Bases: 20s 12d\n\nAddition and subtraction might be cumbersome with non-decimal values, but multiplication and division get tricky quite quickly. We can see this using examples from the article on Arithmetic in the third edition of the Encyclopedia Britannica, printed in 1797 and comparing these to the process with debkeepr types.\nThe Encyclopedia Britannica outlines the process for multiplying £15 3s. 8d. sterling by 32 and £17 3s. 8d. sterling by 75. It breaks down larger multiplication operations into smaller ones. Multiplying £15 3s. 8d. sterling by 32 becomes a matter of sequentially multiplying the monetary value by 8 and then by 4. Multiplying £17 3s. 8d. sterling by 75 is presented as a three step process of multiplying by 3, then by 5, and then 5 again as 3 * 5 * 5 = 75. With debkeepr you can use multiplication as you would any other numeric vector.\n\n# Multiply £15 3s. 8d. sterling by 32\ndeb_lsd(15, 3, 8) * 32\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 485:17s:4d\n#&gt; # Bases: 20s 12d\n\n# Multiply £17 3s. 8d. sterling by 75\ndeb_lsd(17, 3, 8) * 75\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 1288:15s:0d\n#&gt; # Bases: 20s 12d\n\n\n\n\n\n\n\n\nFigure 3: Multiplication in Encyclopedia Britannica\n\n\nThe process for division presented in the Encyclopedia Britannica might bring you back to grade school and long division problems, though the operation also includes multiplication due to the need to convert between units. The Encyclopedia shows the division of £465 12s. 8d. sterling by 72 and 345 hundredweight 1 quarter 8 lbs by 22. This latter example provides an opportunity to demonstrate how you can use the bases argument in debkeepr-type vectors to adapt them to other value systems besides the default of 20 shillings to the pound and 12 pence to the shilling. In this case, there are 4 quarters in a hundredweight and 28 pounds in a quarter. This example also gives proof of the gains in accuracy, as the answer is shown as 15 cwt. 2 q. 21 lb., but it should actually be 22 lb.3 Note too that the output of debkeepr types shows the bases of the vectors: # Bases: 4s 28d.\n\n# Divide £465 12s. 8d. sterling by 72\ndeb_lsd(465, 12, 8) / 72\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 6:9s:4.111d\n#&gt; # Bases: 20s 12d\n\n# Divide 345 hundredweight 1 quarter 8 lbs by 22\ndeb_lsd(345, 1, 8, bases = c(4, 28)) / 22\n#&gt; &lt;deb_lsd[1]&gt;\n#&gt; [1] 15:2s:22d\n#&gt; # Bases: 4s 28d\n\n\n\n\n\n\n\n\nFigure 4: Division in Encyclopedia Britannica\n\n\nThe example of dividing a measurement of weight also provides an opportunity to use the deb_tetra type that allows for four units, here hundredweight, quarters, stones, and pounds. We can, thus, rewrite the value to be divided as 345 cwt. 1 q. 0 st. 8 lb.\n\n# Divide 345 cwt. 1 q. 0 st. 8 lb. by 22\ndeb_tetra(345, 1, 0, 8, bases = c(4, 2, 14)) / 22\n#&gt; &lt;deb_tetra[1]&gt;\n#&gt; [1] 15:2s:1d:8f\n#&gt; # Bases: 4s 2d 14f\n\nFor many more examples, including how to work with vectors of different types and different bases see the Getting Started with debkeepr vignette and the Transactions in Richard Dafforne’s Journal vignette."
  },
  {
    "objectID": "post/debkeepr-cran/index.html#debkeepr-and-double-entry-bookkeeping",
    "href": "post/debkeepr-cran/index.html#debkeepr-and-double-entry-bookkeeping",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "\ndebkeepr and double-entry bookkeeping",
    "text": "debkeepr and double-entry bookkeeping\n\n\n\n\n\nFigure 5: Account book of Jacques della Faille, 1585\n\n\nThe ability to treat non-decimal values of three (deb_lsd) or four (deb_tetra) units as numeric vectors greatly simplifies analysis of these types of values. However, it is the integration of debkeepr types in data frame columns—particularly in the form of what debkeepr refers to as transaction data frames—that makes it possible to bring the analysis of historical account books recorded in various non-decimal currencies into the practices of Digital Humanities and reproducible research. debkeepr provides a methodology to enter data from historical account books and create deb_lsd or deb_tetra columns with deb_gather_lsd() and deb_gather_tetra() respectively. These data frames can then be analyzed and visualized using methods such as those provided by the tidyverse, particularly with dplyr and ggplot2.\n\nset.seed(240)\n# Raw data of a transaction data frame\n# Input each unit of non-decimal currency as separate column\nraw_data &lt;- data.frame(\n  id = 1:5,\n  credit = c(1, 3, 2, 2, 3),\n  debit  = c(3, 2, 1, 3, 1),\n  l = sample(20:100, 5),\n  s = sample(1:19, 5),\n  d = sample(1:11, 5)\n)\nraw_data\n#&gt;   id credit debit  l  s  d\n#&gt; 1  1      1     3 24  6 10\n#&gt; 2  2      3     2 34 12  5\n#&gt; 3  3      2     1 83  2  4\n#&gt; 4  4      2     3 41  5  9\n#&gt; 5  5      3     1 92 11 11\n\n# Create deb_lsd column\ndeb_gather_lsd(raw_data,\n               l = l, s = s, d = d,\n               lsd_col = lsd, replace = TRUE)\n#&gt;   id credit debit        lsd\n#&gt; 1  1      1     3  24:6s:10d\n#&gt; 2  2      3     2  34:12s:5d\n#&gt; 3  3      2     1   83:2s:4d\n#&gt; 4  4      2     3   41:5s:9d\n#&gt; 5  5      3     1 92:11s:11d\n\ndebkeepr includes two data frames of example data. dafforne_transactions is a transactions data frame that has columns for transaction id, credit account, debit account, date of the transaction, and the value of the transaction as a deb_lsd column with bases 20s. and 12d., along with some addition information. This is paired with a data frame containing information about the 46 accounts in Dafforne’s practice ledger. See ?dafforne_transactions and ?dafforne_accounts for more details. For a fuller discussion of using these data frames, see the Analysis of Richard Dafforne’s Journal and Ledger vignette.\n\n# Load tidyverse packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Transactions data frame\ndafforne_transactions\n#&gt; # A tibble: 177 × 8\n#&gt;       id credit debit date                  lsd journal ledger description      \n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;lsd[20s:12d]&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            \n#&gt;  1     1      2     1 1633-01-01    1000:15s:7d       1 1/1    Various coins of…\n#&gt;  2     2      2     3 1633-01-01     477:10s:0d       1 1/1    60 Leeds dozens …\n#&gt;  3     3      2     4 1633-01-01       55:0s:6d       1 2/1    5 barrels that r…\n#&gt;  4     4      2     5 1633-01-01      240:0s:0d       1 2/1    For 800 French c…\n#&gt;  5     5      2     6 1633-01-01      229:0s:0d       1 2/1    2290 guilders re…\n#&gt;  6     6      2     8 1633-01-01       3:17s:8d       1 3/1    Expenses for the…\n#&gt;  7     7      7     2 1633-01-01      150:0s:0d       1 1/2    Ready money from…\n#&gt;  8     8      9    11 1633-01-04      360:0s:0d       1 4/3    100 Leeds dozens…\n#&gt;  9     9      1     9 1633-01-04      144:0s:0d       2 3/1    For 2/5 of the 1…\n#&gt; 10    10      5    10 1633-01-04      120:0s:0d       2 3/2    400 French crown…\n#&gt; # ℹ 167 more rows\n\n# Accounts data frame\ndafforne_accounts\n#&gt; # A tibble: 46 × 5\n#&gt;       id account                                    ledger investor  description\n#&gt;    &lt;int&gt; &lt;chr&gt;                                       &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      \n#&gt;  1     1 Cash                                            1 Ego       Account fo…\n#&gt;  2     2 Stock                                           1 Ego       Account of…\n#&gt;  3     3 Wares                                           1 Ego       Account fo…\n#&gt;  4     4 Kettles                                         2 Ego       Account fo…\n#&gt;  5     5 Jean du Boys - account current                  2 Jean du … Factor in …\n#&gt;  6     6 Jacob Symonson - account by him in company      2 Jacob Sy… Aggregatio…\n#&gt;  7     7 Jacob Symonson - account current                2 Jacob Sy… Current ac…\n#&gt;  8     8 Jacob Symonson - Cochineal                      3 Jacob Sy… Sale of co…\n#&gt;  9     9 George Pinchback                                3 George P… Account wi…\n#&gt; 10    10 Jacob Symonson - ready money                    3 Jacob Sy… Account of…\n#&gt; # ℹ 36 more rows\n\ndebkeepr provides a set of functions that work with data frames with the same structure as dafforne_transactions. deb_account_summary() is useful for obtaining an overview of the account book at its closing, providing a summary of the total credits, debits, and current value of each account.\n\ndafforne_transactions |&gt; \n  deb_account_summary(credit = credit, debit = debit, lsd = lsd)\n#&gt; # A tibble: 46 × 4\n#&gt;    account_id         credit          debit        current\n#&gt;         &lt;dbl&gt; &lt;lsd[20s:12d]&gt; &lt;lsd[20s:12d]&gt; &lt;lsd[20s:12d]&gt;\n#&gt;  1          1   1956:10s:11d    2903:13s:0d   -947:-2s:-1d\n#&gt;  2          2     2006:3s:9d      150:0s:0d     1856:3s:9d\n#&gt;  3          3      570:0s:0d      570:0s:0d        0:0s:0d\n#&gt;  4          4       75:0s:8d       75:0s:8d        0:0s:0d\n#&gt;  5          5      813:3s:0d      813:3s:0d        0:0s:0d\n#&gt;  6          6     568:1s:11d      869:2s:7d    -301:0s:-8d\n#&gt;  7          7   2958:18s:10d   2958:18s:10d        0:0s:0d\n#&gt;  8          8    1580:10s:0d    1580:10s:0d        0:0s:0d\n#&gt;  9          9     1744:1s:4d     1744:1s:4d        0:0s:0d\n#&gt; 10         10      606:2s:6d      606:2s:6d        0:0s:0d\n#&gt; # ℹ 36 more rows\n\nWe can present this information in a line-range plot in which the upper range represents the total credit, the lower range the total debit, and a point for the current value. This also allows an opportunity to point out some limitations of the deb_lsd and deb_tetra vector types; ggplot2 currently does not know how to pick a scale for deb_lsd and deb_tetra columns. However, deb_decimal columns work as expected. It is therefore necessary to cast the deb_lsd or deb_tetra columns to deb_decimal with deb_as_decimal(), either before or within the ggplot() call.4 For a line-range plot we also need to turn the debit column into negative values.\n\n# Plot summary of accounts\ndafforne_transactions |&gt; \n  mutate(lsd = deb_as_decimal(lsd)) |&gt; \n  deb_account_summary() |&gt; # Columns have default names of credit, debit, and lsd\n  mutate(debit = -debit) |&gt;\n  ggplot() + \n    geom_linerange(aes(x = account_id, ymin = debit, ymax = credit)) + \n    geom_point(aes(x = account_id, y = current,\n                     color = if_else(current == 0, \"Closed\", \"Open\"))) +\n    scale_color_manual(values = c(Open = \"black\", Closed = \"blue\")) + \n    scale_y_continuous(labels = scales::label_dollar(prefix = \"\\u00a3\")) + \n    labs(x = \"Accounts\",\n         y = \"Pounds sterling\",\n         color = \"Status\",\n         title = \"Summary of the accounts\") + \n    theme_light()"
  },
  {
    "objectID": "post/debkeepr-cran/index.html#debkeepr-and-reproducibility",
    "href": "post/debkeepr-cran/index.html#debkeepr-and-reproducibility",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "\ndebkeepr and reproducibility",
    "text": "debkeepr and reproducibility\n\n\n\n\n\n\n\nFigure 6: Scratchpad: Doing math by hand\n\n\nThe above plot only scratches the surface of the possibilities for exploring and analyzing account books that use historical non-decimal currencies with debkeepr. The Transactions in Richard Dafforne’s Journal vignette provides additional examples. Yet, what I think is the most significant about debkeepr is not any one plot, time saving feature, or type of analysis, but rather the possibility of using the practices of reproducible research in the context of historical economic data. Individual calculations are easier to track and validate when written in code rather than scrawled out on a scratch piece of paper or entered into a calculator. Even more importantly, debkeepr provides a workflow for entering data from account books into a data base, tidying the data, and then exploring, analyzing, and visualizing the data in ways that integrate with the tidyverse and more general practices of data analysis in R.\nI have seen this in my own work on the accounts of the estate of Jan della Faille de Oude. Without debkeepr the process of transcribing the over 2,100 transactions and analyzing the 480 accounts appeared to have limited use for quite a bit of work. Instead, I picked a handful of crucial accounts and worked by hand. With debkeepr the effort becomes worthwhile and whole plethora of research questions and ways to communicate were opened up. It is my hope that debkeepr can help bring to light crucial and interesting social interactions that are buried in economic manuscripts, making these stories accessible to a wider audience."
  },
  {
    "objectID": "post/debkeepr-cran/index.html#footnotes",
    "href": "post/debkeepr-cran/index.html#footnotes",
    "title": "debkeepr: An R package for the analysis of non-decimal currencies",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor instance, in my talk, “Rough Estimation: Inheritance, Accounting, and Sibling Rivalry in an Early Modern Merchant Family.” See the video and/or slides.↩︎\ndebkeepr uses the nomenclature of l, s, and d to represent pounds, shillings, and pence units in non-decimal currencies. The abbreviations derive from the Latin terms libra, solidus, and denarius. It uses the default of 20 shillings to the pound and 12 pence to the shilling that became ingrained in much of Europe during the Carolingian Empire in the 9th century. However, a variety of other bases also developed over time, and so these bases can be modified by the user using the bases argument.↩︎\nChecking the long division at the bottom of the calculation, 22 goes into 44 twice not once.↩︎\nThe only context in which casting to deb_decimal results in any loss of information is in labeling the plotted values. However, this can be rectified through deb_text(), which provides a flexible way to format debkeepr types as text.↩︎"
  },
  {
    "objectID": "post/rtweet/index.html",
    "href": "post/rtweet/index.html",
    "title": "Downloading Twitter data with rtweet",
    "section": "",
    "text": "Last night, the 17th of November, 2022, I, like so many others, was rather gleefully, but also sadly, doom scrolling through Twitter as Elon Musk seemed to be rocketing the social network into the ground at amazing speed.\n\n\nIt seemed a good time to heed the warning that so many had put out about downloading your twitter data: your tweets, those you follow, your followers, and maybe the Twitter lists that you have created or followed. I knew that much of this could be done with the rtweet package, and I had seen a few people tweet out ways to get the data, in particular, I had saved Amelia McNamara’s tweet from 11 November 2022.\n\nI followed her script, played around with the functions in rtweet a bit, and tried to fit the basics of a script to download most of the important data in one tweet.\n\n\nBelow, I will expand on the Tweet to show more fully how to download and save your Twitter data. This closely follows Amelia’s script but adds a bit of data.\nGetting the data\nLet’s get the data of the tweets, followers, those followed, and lists of a single user. Here, I use my own account of @vivalosburros. If you want to go even further in moving away from Twitter and use rtweet to delete your tweets, see Julia Silge’s post.\n\n# Load rtweet and tidyverse for cleaning and saving data\nlibrary(rtweet)\nlibrary(tidyverse)\n\n# Authenticate twitter\nauth_setup_default()\n\n# Twitter user name: I use my own name @vivalosburros\nuser &lt;- \"vivalosburros\"\n\n# Your tweets -------------------------------------------------------------\n\ntweets &lt;- get_timeline(user, n = Inf)\n\n# Who you follow ----------------------------------------------------------\n\nfriends &lt;- get_friends(user)\nfriend_screennames &lt;- lookup_users(friends$to_id)\n\n# Followers ---------------------------------------------------------------\n\nfollowers &lt;- get_followers(user)\nfollower_screennames &lt;- lookup_users(followers$from_id)\n\n# Lists -------------------------------------------------------------------\n\nlists &lt;- lists_users(user)\n# Create a list of tibbles of the different lists you follow\nlists_screenames &lt;- map(lists$list_id, lists_members)\n\nClean the data\nThe data we get is returned as tibbles with many columns that have data that might not be all that useful to you. However, you can easily skip this data cleaning step if you just want the raw data. The variables that you are interested in might be different than the ones I have below.\n\n# Select desired columns: Feel free to skip\n\ntweets &lt;- tweets |&gt; \n  select(created_at:full_text, retweet_count:retweeted)\n\nfriend_screennames &lt;- friend_screennames |&gt; \n  select(id:url)\n\nfollower_screennames &lt;- follower_screennames |&gt; \n  select(id:url)\n\nlists_screenames &lt;- lists_screenames |&gt; \n  map(~ select(., user_id:url))\n\nSaving the data\nNow we can save the data in a folder called data/ with the readr. The only tricky part is using walk2() from purrr to save the list of tibbles that has the Twitter lists you follow, if you follow more than one.\n\n# Save the tweets, following, and followers data\nwrite_csv(tweets, \"data/tweets.csv\")\nwrite_csv(friend_screennames, \"data/following.csv\")\nwrite_csv(follower_screennames, \"data/followers.csv\")\n\n# Save the list data and name csv with the list name\n\n# Get names of lists\nlist_names &lt;- lists$name\nwalk2(lists_screenames,\n      list_names,\n      ~ write_csv(.x, paste0(\"data/list-\", .y, \".csv\")))\n\nNow you have all your data. All that is left is to watch the ship sink and make fun of billionaires who think they are geniuses 🫡."
  },
  {
    "objectID": "post/debkeepr-intro/index.html",
    "href": "post/debkeepr-intro/index.html",
    "title": "Introducing debkeepr",
    "section": "",
    "text": "Note: The API for debkeepr described in this post has been deprecated. For the new API, see the debkeepr website and the blog post announcing debkeepr’s release on CRAN.\nYou can now install the released version of debkeepr from CRAN or the development version on GitHub:\nAfter an extensive period of iteration and a long but rewarding process of learning about package development, I am pleased to announce the release of my first R package. The package is called debkeepr, and it derives directly from my historical research on early modern merchants. debkeepr provides an interface for working with non-decimal currencies that use the tripartite system of pounds, shillings, and pence that was used throughout Europe in the medieval and early modern periods. The package includes functions to apply arithmetic and financial operations to single or multiple values and to analyze account books that use double-entry bookkeeping with the latter providing the basis for the name of debkeepr. In this post I want to discuss the motivation behind the creation of the package and provide some examples for how debkeepr can help those who encounter non-decimal currencies in their research."
  },
  {
    "objectID": "post/debkeepr-intro/index.html#pounds-shillings-and-pence-lsd-monetary-systems",
    "href": "post/debkeepr-intro/index.html#pounds-shillings-and-pence-lsd-monetary-systems",
    "title": "Introducing debkeepr",
    "section": "Pounds, shillings, and pence: lsd monetary systems",
    "text": "Pounds, shillings, and pence: lsd monetary systems\nThe system of expressing monetary values in the form of pounds, shillings, and pence dates back to the Carolingian Empire and the change from the use of gold coins that derived from the late Roman Empire to silver pennies that had taken place by the eighth century. Needing ways to count larger quantities of the new silver denarius, people began to define a solidus, originally a gold coin introduced by the Emperor Constantine, as a unit of account equivalent to 12 denarii. For even larger valuations, the denarius was further defined in relation to a pound or libra of silver. Though the actual number of coins struck from a pound of silver differed over time, the rate of 240 coins lasted long enough to create the custom of counting coins in dozens (solidi) and scores of dozens (librae). The librae, solidi, and denarii (lsd) monetary system was translated into various European languages, and though the ratios between the three units often differed by region and period, the basic structure of the system remained in place until decimalization began following the French Revolution.1\nThe pounds, shillings, and pence system complicates even relatively simple arithmetic operations. Especially in our decimalized world, manipulating values that are made up of three different units in which two of the units are non-decimal quickly becomes cumbersome. The principles of grade school arithmetic come running back, as the practice of addition and carrying over values to the next unit gains new relevancy. Calculations with pounds, shillings, and pence values are further complicated when dealing with more than one currency or money of account, particularly if one or more of the currencies used bases for the shillings and pence units that differed from the common bases of 20 shillings to a pound and 12 pence to a shilling.\nThe economic historian encounters the difficulties of handling non-decimal currencies in two main contexts. In reading through documents that may or may not be primarily economic in nature, the researcher comes across sets of values that need to be manipulated to better understand their meaning. A common case is the discovery of values that need to be added together to see if they are equivalent to a value in another document, or there may be values in one currency that have to be converted to another currency for comparison. The second context in which historians confront non-decimal monetary values is in explicitly economic documents such as entire account books that may contain hundreds or thousands of transactions. In both contexts the researcher is often stuck performing arithmetic calculations by hand just as merchants and bookkeepers had to do in the past. I do not know how many pages of scratched out calculations I went through in my own research on the estate of Jan della Faille de Oude, a wealthy merchant from Antwerp who died in 1582.\n\n\n\n\n\n\n\n\nFigure 1: Scratchpad: Doing math by hand\n\n\n\nThe tripartite and non-decimal nature of pounds, shillings, and pence values presents particular difficulties for the analysis of large sets of accounts. A first instinct of the researcher is to place the transactions from an account book into a table or data base to better understand these large groups of data, but how should the pounds, shillings, and pence values be entered? Should they be placed into three separate variables or brought together somehow into one, and how should the arithmetic be performed? So long as there is no clear way to handle one, not to mention multiple, non-decimal currency within a data base, the use of digital tools to investigate and analyze historical accounts books will be hindered."
  },
  {
    "objectID": "post/debkeepr-intro/index.html#debkeepr",
    "href": "post/debkeepr-intro/index.html#debkeepr",
    "title": "Introducing debkeepr",
    "section": "debkeepr",
    "text": "debkeepr\ndebkeepr seeks to overcome these issues and integrate non-decimal pounds, shillings, and pence values into the decimalized environment of R, opening the vast analytical and visual capabilities of R to historical monies of account. debkeepr accomplishes this through the creation of the lsd class and functions intended to work with this new class of object in R. The lsd class unifies the separate pounds, shillings, and pence units into a numeric vector of length three and tracks the non-decimal bases for the shillings and pence units through a bases attribute. lsd objects are stored as lists, making it possible for a single object to contain multiple pounds, shillings, and pence values and to implement lsd objects as list columns in a data frame or tibble. The lsd class is intended to both minimize the difficulty historians encounter in one-off calculations and provide a consistent means for the exploration and analysis of large sets of accounts.2\nI have set up a pkgdown site for debkeepr, and you can find the source code on GitHub. The package includes three vignettes, two of which deal with data from the practice journal and ledger from Richard Dafforne’s The Merchant’s Mirrour, Or Directions for the Perfect Ordering and Keeping of His Accounts (London, 1660), a bookkeeping manual that was printed throughout the seventeenth century. Data frames for the transactions and accounts from this set of account books are also included with the package.\n\nVignettes\n\nGetting Started with debkeepr vignette: An in depth overview of debkeepr’s functions and their use in various contexts.\nTransactions in Richard Dafforne’s Journal vignette: Examples of financial and arithmetic calculations dealing with various currencies taken from the example journal in Richard Dafforne’s Merchant’s Mirrour (1660).\nAnalysis of Richard Dafforne’s Journal and Ledger vignette: An analysis of the example journal and ledger in Dafforne’s Merchant’s Mirrour using the dafforne_transactions and dafforne_accounts data provided in debkeepr.\n\nIn the rest of this post I want to quickly highlight how debkeepr can assist historians in handling pounds, shillings, and and pence values in both one-off calculations and in the context of an entire account book. The vignettes go into much greater detail than will be done here."
  },
  {
    "objectID": "post/debkeepr-intro/index.html#arithmetic-with-debkeepr",
    "href": "post/debkeepr-intro/index.html#arithmetic-with-debkeepr",
    "title": "Introducing debkeepr",
    "section": "Arithmetic with debkeepr",
    "text": "Arithmetic with debkeepr\nAt the heart of debkeepr is the ability to normalize pounds, shillings, and pence values to specified non-decimal unit bases in the process or making various calculations. Even in the simplest case of addition, debkeepr makes the process easier and less error prone. Let’s use an example from the image below that shows arithmetic on a scratch piece of paper dealing with trade between Holland and the Barbary coast in the 1590s.3 On the top right four values in the money of account of pounds Flemish are added together.\n\n\n\n\n\n\nFigure 2: A scratchpad from the archive of Daniel van der Meulen\n\n\n\n\n\n\n\n\n\n\n\n\npounds\nshillings\npence\n\n\n\n\n74\n10\n0\n\n\n26\n8\n8\n\n\n107\n14\n0\n\n\n28\n19\n0\n\n\n\n\n\n\ndebkeepr provides multiple means to add together values of pounds Flemish, which used the standard bases of 20 shillings per pound and 12 pence per shilling.4\n\nAdd the separate units by hand and then normalize.\nDo the addition with debkeepr by supplying numeric vectors of length three.\nCreate an object of class lsd and proceed with the addition.\n\nlibrary(debkeepr)\n\n# Normalize £235 51s. 8d.\ndeb_normalize(c(235, 51, 8), bases = c(20, 12))\n#&gt;       l  s d\n#&gt; [1] 237 11 8\n\n\n# Addition of values with debkeepr\ndeb_sum(c(74, 10, 0),\n        c(26, 8, 8),\n        c(107, 14, 0),\n        c(28, 19, 0))\n#&gt;       l  s d\n#&gt; [1] 237 11 8\n\n\n# Create lsd object from list of vectors, then do addition\nlsd_values &lt;- deb_as_lsd(list(c(74, 10, 0),\n                              c(26, 8, 8),\n                              c(107, 14, 0),\n                              c(28, 19, 0)),\n                         bases = c(20, 12))\ndeb_sum(lsd_values)\n#&gt;       l  s d\n#&gt; [1] 237 11 8\nMultiplication and division of pounds, shillings, and pence values were equally frequent calculations, but they are more complex to do by hand. Bookkeeping and merchant manuals often included rules for the multiplication and division of compound values such as pounds, shillings, and pence. The article on arithmetic in the third edition of the Encyclopedia Britannica, printed in 1797, provides a good example of how to do compound unit arithmetic. debkeepr greatly simplifies this process.\n# Multiply £15 3s. 8d. sterling by 32\ndeb_multiply(c(15, 3, 8), x = 32)\n#&gt;       l  s d\n#&gt; [1] 485 17 4\n\n# Multiply £17 3s. 8d. sterling by 75\ndeb_multiply(c(17, 3, 8), x = 75)\n#&gt;        l  s d\n#&gt; [1] 1288 15 0\n\n\n\n\n\n\n\n\nFigure 3: Multiplication in Encyclopedia Britannica\n\n\n\nThe examples for division in the Encyclopedia Britannica include the division of pounds, shillings, and pence, as well as the division of weight measured in terms of hundredweight, quarters, and pounds. A hundredweight consisted of four quarters and there were 28 pounds (or two stones) in a quarter. While debkeepr was created with pounds, shillings, and pence values in mind, the measurement of weight in terms of hundredweight can be integrated by altering the bases argument. This example even serves to show a mistake in the printing in the Encyclopedia, as the answer is shown as 15 cwt. 2 q. 21 lb., but it should actually be 22 lb. Checking the long division at the bottom of the calculation, 22 goes into 44 twice not once. Notice too that the division of £465 12s. 8d. ends with a remainder of 8 even though this is not included in the answer provided. This remainder leads to the decimal in the pence unit in the answer provided by deb_divide(). Setting the round argument to 0 would convert the pence unit to a whole number.\n# Divide £465 12s. 8d. sterling by 72\ndeb_divide(c(465, 12, 8), x = 72)\n#&gt;     l s       d\n#&gt; [1] 6 9 4.11111\n\n# Divide 345 hundredweight 1 quarter 8 lbs by 22\ndeb_divide(c(345, 1, 8), x = 22, bases = c(4, 28))\n#&gt;      l s  d\n#&gt; [1] 15 2 22\n\n\n\n\n\n\n\n\nFigure 4: Division in Encyclopedia Britannica\n\n\n\nThese examples replicate answers already provided, but they serve to demonstrate the gains in both speed and accuracy for these common calculations. Further examples, including the use of debkeepr to convert between currencies that may or may not have different bases for the shillings and pence units, can be found in both Getting Started with debkeepr and the Transactions in Richard Dafforne’s Journal vignette."
  },
  {
    "objectID": "post/debkeepr-intro/index.html#analysis-of-account-books",
    "href": "post/debkeepr-intro/index.html#analysis-of-account-books",
    "title": "Introducing debkeepr",
    "section": "Analysis of account books",
    "text": "Analysis of account books\nAs a way of providing example data of pounds, shillings, and pence values in a data frame, debkeepr includes data from the example journal and ledger in Dafforne’s Merchant’s Mirrour from 1660. dafforne_transactions has 177 transactions between 46 accounts from the journal. Each transaction has a creditor and debtor account, showing where each transactional value came from and where it went to, the date of the transaction, and the value in pounds sterling contained in an lsd list column. Extra details include the pages on which the transaction can be found in Dafforne’s journal and ledger and a short description of the transaction. A PDF copy of the journal is available for download if you want to see what the original source looks like. The raw data from which dafforne_transactions derives also demonstrates the process of entering pounds, shillings, and pence units into three separate columns to create the data base and then using deb_lsd_gather() to transform these variables into an lsd list column.\n# Transactions from the Dafforne's example journal\ndafforne_transactions\n#&gt; # A tibble: 177 x 8\n#&gt;       id credit debit date       lsd    journal ledger description        \n#&gt;    &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;date&gt;     &lt;S3: &gt;   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;              \n#&gt;  1     1      2     1 1633-01-01 1000,…       1 1/1    Various coins of g…\n#&gt;  2     2      2     3 1633-01-01 477, …       1 1/1    60 Leeds dozens at…\n#&gt;  3     3      2     4 1633-01-01 55, 0…       1 2/1    5 barrels that rem…\n#&gt;  4     4      2     5 1633-01-01 240, …       1 2/1    For 800 French cro…\n#&gt;  5     5      2     6 1633-01-01 229, …       1 2/1    2290 guilders rema…\n#&gt;  6     6      2     8 1633-01-01 3, 17…       1 3/1    Expenses for the r…\n#&gt;  7     7      7     2 1633-01-01 150, …       1 1/2    Ready money from J…\n#&gt;  8     8      9    11 1633-01-04 360, …       1 4/3    100 Leeds dozens s…\n#&gt;  9     9      1     9 1633-01-04 144, …       2 3/1    For 2/5 of the 100…\n#&gt; 10    10      5    10 1633-01-04 120, …       2 3/2    400 French crowns …\n#&gt; # ... with 167 more rows\nThe Analysis of Richard Dafforne’s Journal and Ledger vignette provides a fuller breakdown of the data, but here I would like to show how to get a summary of the 46 accounts in the books and create a plot using ggplot2. One side effect of the use of a list column to represent pounds, shillings, and pence values is that it cannot be used for the purposes of plotting. However, this issue is overcome by the robust support for decimalization in debkeepr. Pounds, shillings, and pence values can be decimalized to any of the three units and returned to their original form when desired. For the purposes of plotting, the most useful workflow is to transform the lsd list column to decimalized pounds.\ndebkeepr has a set of functions meant to deal with data frames that mimic the form of a journal used for double-entry bookkeeping such as dafforne_transactions. The 177 transactions and 46 accounts in dafforne_transactions are more than enough to lose track of which accounts were most significant to the bookkeeper’s trade. Here, I will use deb_account_summary() to calculate the total credit or amount each account sent, total debit or amount each account received, and the current value of the account at the closing of the books. While deb_account_summary() gives a nice overview of the entire set of account books, it is also possible to focus on a single account with deb_account().\n# Summary of all accounts in Dafforne's example journal\ndeb_account_summary(df = dafforne_transactions)\n#&gt; # A tibble: 46 x 4\n#&gt;    account_id credit       debit        current     \n#&gt;         &lt;int&gt; &lt;S3: lsd&gt;    &lt;S3: lsd&gt;    &lt;S3: lsd&gt;   \n#&gt;  1          1 1956, 10, 11 2903, 13, 0  -947, -2, -1\n#&gt;  2          2 2006, 3, 9   150, 0, 0    1856, 3, 9  \n#&gt;  3          3 570, 0, 0    570, 0, 0    0, 0, 0     \n#&gt;  4          4 75, 0, 8     75, 0, 8     0, 0, 0     \n#&gt;  5          5 813, 3, 0    813, 3, 0    0, 0, 0     \n#&gt;  6          6 568, 1, 11   869, 2, 7    -301, 0, -8 \n#&gt;  7          7 2958, 18, 10 2958, 18, 10 0, 0, 0     \n#&gt;  8          8 1580, 10, 0  1580, 10, 0  0, 0, 0     \n#&gt;  9          9 1744, 1, 4   1744, 1, 4   0, 0, 0     \n#&gt; 10         10 606, 2, 6    606, 2, 6    0, 0, 0     \n#&gt; # ... with 36 more rows\n\n# Summary of the cash account in Dafforne's example journal\ndeb_account(df = dafforne_transactions, account_id = 1)\n#&gt; # A tibble: 3 x 2\n#&gt;   relation lsd         \n#&gt;   &lt;chr&gt;    &lt;S3: lsd&gt;   \n#&gt; 1 credit   1956, 10, 11\n#&gt; 2 debit    2903, 13, 0 \n#&gt; 3 current  -947, -2, -1\nTo plot the summary of the accounts in the journal and ledger the lsd list columns have to be converted to decimalized pounds, which is done with deb_lsd_l() and dplyr::mutate_if(). From this information, we can create a line range plot in which the upper limit is represented by the total credit, the lower limit by the total debit, and the current value by a point. Blue points show accounts that have been balanced by the end of the books and are therefore closed, while black points represent the values for accounts that remain open.\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Prepare account summary for plotting\n(dafforne_summary &lt;- dafforne_transactions %&gt;% \n  deb_account_summary() %&gt;% \n  mutate_if(deb_is_lsd, deb_lsd_l) %&gt;% \n  mutate(debit = -debit))\n#&gt; # A tibble: 46 x 4\n#&gt;    account_id credit   debit current\n#&gt;         &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1          1 1957.  -2904.    -947.\n#&gt;  2          2 2006.   -150     1856.\n#&gt;  3          3  570    -570        0 \n#&gt;  4          4   75.0   -75.0      0 \n#&gt;  5          5  813.   -813.       0 \n#&gt;  6          6  568.   -869.    -301.\n#&gt;  7          7 2959.  -2959.       0 \n#&gt;  8          8 1580.  -1580.       0 \n#&gt;  9          9 1744.  -1744.       0 \n#&gt; 10         10  606.   -606.       0 \n#&gt; # ... with 36 more rows\n\n# Plot summary of accounts\nggplot(data = dafforne_summary) + \n  geom_linerange(aes(x = account_id, ymin = debit, ymax = credit)) + \n  geom_point(aes(x = account_id, y = current,\n                 color = if_else(current == 0, \"Closed\", \"Open\"))) +\n  scale_color_manual(values = c(Open = \"black\", Closed = \"blue\")) + \n  scale_y_continuous(labels = scales::dollar_format(prefix = \"£\")) + \n  labs(x = \"Accounts\",\n       y = \"Pounds sterling\",\n       color = \"Status\",\n       title = \"Summary of the accounts in Dafforne's ledger\") + \n  theme_light()"
  },
  {
    "objectID": "post/debkeepr-intro/index.html#debkeepr-and-reproducibility",
    "href": "post/debkeepr-intro/index.html#debkeepr-and-reproducibility",
    "title": "Introducing debkeepr",
    "section": "debkeepr and reproducibility",
    "text": "debkeepr and reproducibility\nThe above plot only scratches the surface of the possibilities for exploring and analyzing account books that use pounds, shillings, and pence values with debkeepr. Yet, what I think is the most significant about debkeepr is not any one plot, time saving feature, or type of analysis, but rather the possibility of using the practices of reproducible research in the context of historical economic data that takes the form of pounds, shillings, and pence values. Individual calculations are easier to track and validate when written in code rather than scrawled out on a scratch piece of paper or entered into a calculator. Even more importantly, debkeepr provides a workflow for entering data from account books into a data base, tidying the data by transforming separate pounds, shillings, and pence variables into an lsd list column, and then exploring, analyzing, and visualizing the data in ways that integrate with the tidyverse and more general practices of data analysis in R. It is my hope that debkeepr can help bring to light crucial and interesting social interactions that are buried in economic manuscripts, making these stories accessible to a wider audience."
  },
  {
    "objectID": "post/debkeepr-intro/index.html#footnotes",
    "href": "post/debkeepr-intro/index.html#footnotes",
    "title": "Introducing debkeepr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information about the development of the system of pounds, shillings, and pence and medieval monetary systems more generally see Peter Spufford, Money and its Use in Medieval Europe (Cambridge: Cambridge University Press, 1988).↩︎\nFor a more in depth discussion of lsd objects and a step-by-step overview of how to create and work with them see Getting Started with debkeepr.↩︎\nThis scratch piece of paper comes from the archive of Daniël van der Meulen en Hester de la Faille, zijn vrouw, 1550-1648 at Erfgoed Leiden en Omstreken. You can access the original here.↩︎\nThis means that the default value of c(20, 12) does not need to be altered for the bases argument, though it is included in some of the below examples for transparency.↩︎"
  },
  {
    "objectID": "post/one-year-reflections/index.html",
    "href": "post/one-year-reflections/index.html",
    "title": "One Year Anniversary",
    "section": "",
    "text": "It was a year ago today that I launched this website with a post introducing myself and the goals for the blog. When I launched the site, I was at the beginning of teaching myself about digital humanities and how to code in R. Building this Hugo site was part of my learning process. I learned about the command line, Git, and GitHub, not to mention a bit of HTML and CSS to get the website up and going. I also wanted to share my progress and provide information for others who might want to go down a similar path. Over the past year I have gone from a coding newbie to actively working on two digital humanities projects and closing in on the launch of my first R package that makes it easier to work with historical non-decimal currencies.\nAfter writing a couple of posts on my thoughts about digital humanities, I have concentrated on writing posts about using R to analyze and visualize historical data. I have always tried to adopt the perspective of a newcomer to code and to R, since I was in that position not all that long ago. The rsats community has done an admirable job in trying to make learning R as approachable as possible, but there is no way around the fact that learning to code is a daunting task. My posts have mainly focused on using R for GIS and network analysis since these are the two topics most pertinent to the digital humanities projects I have been working on. Writing posts on these topics proved to be the best way for me to learn not only how to create maps and network graphs with R but also about the fields of GIS and network analysis in general.\nEven if no one read my posts they would have been useful endeavors for all that I learned through writing them. However, the most gratifying aspect of writing this blog has been hearing that the content has proven useful to others in their efforts to learn R. It has been particularly amazing and amusing to hear from people in the sciences and think about biologists and chemists learning to code by analyzing letters from a sixteenth-century merchant in the Low Countries, which is the data set that I have used in my posts.\nI never could have imagined that this website and my blog posts would be read by very many people, and I have been amazed by the steady stream of visitors that continue to read the website daily. The digital humanities and rstats communities have been amazingly open and welcoming, and I never could have gotten this far nor had this much fun without the kindness of the people in these communities. I have received very generous encouragement and increased visibility through tweeting out my posts from Maëlle Salmon, Mara Averick, Hadley Wickham, and the great R4DS community among many others. To these individuals and to the entire community that has come to read my blog I can only say thank you.\nIn a nice little coincidence this is the 11th post to the blog and pushes my first introductory post to the second page of blog entries. I am looking forward to the next year of continuing to learn more about digital humanities and R and writing about my experiences on this blog, pushing more and more posts to the second page and beyond."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Talks",
    "section": "",
    "text": "An abbreviated list of recent talks that I have given on my historical research, Digital Humanities, and R. For a full list of talks, see my CV.\n\n“Rough Estimation: Inheritance, Accounting, and Sibling Rivalry in an Early Modern Merchant Family,” Low Countries History Seminar, 15 October 2021\n\nVideo and Slides\n\n“Analyzing and Visualizing Double-Entry Bookkeeping: A Digital History Methodology,” Renaissance Society of America, April 2021\n\nSlides\n\n“‘How pleasant it is for brethren to dwell together in unity:’ Family and Exile in the Dutch Revolt,” 6–8 April 2021\n“Solving Niche Historical Problems with Digital Humanities,” Cal State Chico Digital Humanities Series, 24 February 2021\n“vctrs: Creating custom vector classes with the vctrs package,” RStudio Conference, 27–30 January 2020\n\nVideo and Slides\n\n“Learning and Using the tidyverse for Historical Research,” RStudio Conference, 15–18 January 2019\n\nVideo and Slides\n\n“Coding for the Humanities,” Workshop funded by the Mellon Scholars Program at Hope College, 16 November 2018\n\nSlides and code"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jesse Sadler",
    "section": "",
    "text": "I am a Digital Humanities Trainer and Project Consultant at the University Library at Virginia Tech. I am trained as a historian and received my Ph.D. in European History from UCLA. My historical research focuses on social and familial basis of politics, religion, and early modern capitalism. I have written on sibling relationships and inheritance in mercantile families from Antwerp during the Dutch Revolt.\nI am interested in making the tools of Data Science and Digital Humanities more widely accessible to help build open and reproducible research across the Humanities. I am particularly invested in the use of R, but I also dabble with Python and have interest in continuing to expand my digital skills. I am a Carpentries instructor and maintainer of the R for Social Scientists lesson."
  },
  {
    "objectID": "project/debkeepr.html",
    "href": "project/debkeepr.html",
    "title": "debkeepr",
    "section": "",
    "text": "debkeepr integrates non-decimal currencies that use tripartite and tetrapartite systems into the methodologies of Digital Humanities and the practices of reproducible research. The package makes it possible for historical non-decimal currencies, such as the tripartite system of pounds, shillings, and pence, to behave like decimalized numeric values through the implementation of the deb_lsd, deb_tetra, and deb_decimal vector types. These types are based on the infrastructure provided by the vctrs package. debkkeepr simplifies the process of performing arithmetic calculations with non-decimal currencies — such as adding £3 13s. 4d. sterling to £8 15s. 9d. sterling — and also provides a basis for analyzing account books with thousands of transactions recorded in non-decimal currencies. The name of the debkeepr package derives from this latter capability of analyzing historical account books that often used double-entry bookkeeping."
  },
  {
    "objectID": "project/debkeepr.html#installation",
    "href": "project/debkeepr.html#installation",
    "title": "debkeepr",
    "section": "Installation",
    "text": "Installation\nYou can install debkeepr from GitHub with remotes:\n\n# install.packages(\"remotes\")\nremotes::install_github(\"jessesadler/debkeepr\")\n\nPlease open an issue if you have any questions, comments, or requests."
  },
  {
    "objectID": "project/debkeepr.html#historical-background",
    "href": "project/debkeepr.html#historical-background",
    "title": "debkeepr",
    "section": "Historical Background",
    "text": "Historical Background\nThe debkeepr package uses the nomenclature of l, s, and d to represent pounds, shillings, and pence units in non-decimal currencies. The abbreviations derive from the Latin terms libra, solidus, and denarius. The libra was a Roman measurement of weight, while the solidus and denarius were both Roman coins. The denarius was a silver coin from the era of the Republic, in contrast to the golden solidus that was issued in the Late Empire. As the production of silver coins overtook that of gold by the 8th century, a solidus came to represent 12 silver denarii coins, and 240 denarii were — for a time — made from one libra or pound of silver. The custom of counting coins in dozens (solidi) and scores of dozens (librae) spread throughout the Carolingian Empire and became ingrained in much of Europe. However, a variety of currencies or monies of account used other bases for the solidus and denarius units. Some currencies and other value systems, such as those for weights, added a fourth unit. debkeepr provides a consistent manner for dealing with any set of bases within tripartite or tetrapartite systems through the bases attribute of deb_lsd, deb_tetra, and deb_decimal vectors."
  },
  {
    "objectID": "project/debkeepr.html#resources",
    "href": "project/debkeepr.html#resources",
    "title": "debkeepr",
    "section": "Resources",
    "text": "Resources\n\n\nGetting Started with debkeepr vignette: An introduction to the deb_lsd, deb_tetra, and deb_decimal types and their use as vectors and as columns in data frames.\n\nTransactions in Richard Dafforne’s Journal vignette: Examples of financial and arithmetic calculations dealing with various currencies taken from the practice journal in Richard Dafforne’s Merchant’s Mirrour (1660), a 17th-century textbook for learning accounting practices.\n\nAnalysis of Richard Dafforne’s Journal and Ledger vignette: An analysis of the practice journal and ledger in Dafforne’s Merchant’s Mirrour using the dafforne_transactions and dafforne_accounts data provided in debkeepr.\n\nA PDF copy of Dafforne’s practice journal can be consulted to further investigate the practices of early modern double-entry bookkeeping."
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Courses I have taught",
    "section": "",
    "text": "Digital History for Public History\n\n\n\n\n\n\n\n\n\n\n\nAncient History\n\n\n\n\n\n\n\n\n\n\n\nRevolution in Modern Europe\n\n\n\n\n\n\n\n\n\n\n\nThe Italian Renaissance\n\n\n\n\n\n\n\n\n\n\n\nModern World History\n\n\n\n\n\n\n\n\n\n\n\nEurope and the World 1200–1648\n\n\n\n\n\n\n\n\n\n\n\nEuropean Empires, Exploration, and Exchange since 1500\n\n\n\n\n\n\n\n\n\n\n\nCultural and Intellectual History of Modern Europe, Eighteenth Century\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Western Civilization: Circa A.D. 843 to circa 1715\n\n\n\n\n\n\n\n\n\n\n\nVirtue and Commerce: Republicanism and the Development of the Global Economy\n\n\n\n\n\n\n\n\n\n\n\nFreemasonry, Civil Society, and Democracy in 18th-Century Europe and America\n\n\n\n\n\n\n\nNo matching items"
  }
]